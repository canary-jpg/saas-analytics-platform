[0m15:05:10.018930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107eaa8a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c31520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c312e0>]}


============================== 15:05:10.021669 | 7199bb48-94c9-4f9f-b45f-3f98070f9984 ==============================
[0m15:05:10.021669 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:10.021985 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'static_parser': 'True', 'introspect': 'True', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'log_format': 'default', 'use_colors': 'True', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'target_path': 'None', 'version_check': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt debug'}
[0m15:05:10.030122 [info ] [MainThread]: dbt version: 1.11.4
[0m15:05:10.030436 [info ] [MainThread]: python version: 3.12.12
[0m15:05:10.030914 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:05:10.031202 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:05:10.085151 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:05:10.085441 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:05:10.085615 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:05:10.086781 [info ] [MainThread]: adapter type: duckdb
[0m15:05:10.086960 [info ] [MainThread]: adapter version: 1.10.0
[0m15:05:10.088732 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `data-paths` config has been renamed to `seed-paths`. Please update your
`dbt_project.yml` configuration to reflect this change.
[0m15:05:10.088934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '7199bb48-94c9-4f9f-b45f-3f98070f9984', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d7cc20>]}
[0m15:05:10.118647 [info ] [MainThread]: Configuration:
[0m15:05:10.118945 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:05:10.119102 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m15:05:10.119233 [info ] [MainThread]: Required dependencies:
[0m15:05:10.119453 [debug] [MainThread]: Executing "git --help"
[0m15:05:10.140509 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:05:10.141097 [debug] [MainThread]: STDERR: "b''"
[0m15:05:10.141295 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:05:10.141469 [info ] [MainThread]: Connection:
[0m15:05:10.141640 [info ] [MainThread]:   database: analytics
[0m15:05:10.141770 [info ] [MainThread]:   schema: main
[0m15:05:10.141898 [info ] [MainThread]:   path: Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:05:10.142022 [info ] [MainThread]:   config_options: None
[0m15:05:10.142141 [info ] [MainThread]:   extensions: None
[0m15:05:10.142255 [info ] [MainThread]:   settings: {}
[0m15:05:10.142373 [info ] [MainThread]:   external_root: .
[0m15:05:10.142486 [info ] [MainThread]:   use_credential_provider: None
[0m15:05:10.142598 [info ] [MainThread]:   attach: None
[0m15:05:10.142716 [info ] [MainThread]:   filesystems: None
[0m15:05:10.142826 [info ] [MainThread]:   remote: None
[0m15:05:10.142935 [info ] [MainThread]:   plugins: None
[0m15:05:10.143044 [info ] [MainThread]:   disable_transactions: False
[0m15:05:10.143398 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:10.366959 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:05:10.390493 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:05:10.390729 [debug] [MainThread]: On debug: select 1 as id
[0m15:05:10.390881 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:10.394680 [debug] [MainThread]: DuckDB adapter: Error running SQL: select 1 as id
[0m15:05:10.394849 [debug] [MainThread]: DuckDB adapter: Rolling back transaction.
[0m15:05:10.395055 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m15:05:10.395252 [info ] [MainThread]: [31m2 checks failed:[0m
[0m15:05:10.395385 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml


[0m15:05:10.395522 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  IO Error: Cannot open file "/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb": No such file or directory

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m15:05:10.395804 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ConfigDataPathDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:05:10.522773 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.5422592, "process_in_blocks": "0", "process_kernel_time": 0.157381, "process_mem_max_rss": "130727936", "process_out_blocks": "0", "process_user_time": 0.821021}
[0m15:05:10.523204 [debug] [MainThread]: Command `dbt debug` failed at 15:05:10.523137 after 0.54 seconds
[0m15:05:10.523412 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:05:10.523606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c313a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dfd1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108da5b20>]}
[0m15:05:10.523807 [debug] [MainThread]: Flushing usage events
[0m15:05:10.660077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:09:14.997574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dd6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109b5580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109b5370>]}


============================== 15:09:15.001893 | 53188ec5-48da-491f-8822-fa0cadb9d08a ==============================
[0m15:09:15.001893 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:09:15.002210 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'invocation_command': 'dbt debug', 'warn_error': 'None', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'fail_fast': 'False', 'target_path': 'None', 'printer_width': '80', 'cache_selected_only': 'False', 'empty': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'partial_parse': 'True', 'debug': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:09:15.010973 [info ] [MainThread]: dbt version: 1.11.4
[0m15:09:15.011387 [info ] [MainThread]: python version: 3.12.12
[0m15:09:15.011574 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:09:15.011718 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:09:15.121653 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:09:15.121960 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:09:15.122119 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:09:15.123283 [info ] [MainThread]: adapter type: duckdb
[0m15:09:15.123476 [info ] [MainThread]: adapter version: 1.10.0
[0m15:09:15.154099 [info ] [MainThread]: Configuration:
[0m15:09:15.154389 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:09:15.154537 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m15:09:15.154667 [info ] [MainThread]: Required dependencies:
[0m15:09:15.154879 [debug] [MainThread]: Executing "git --help"
[0m15:09:15.169182 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:09:15.169793 [debug] [MainThread]: STDERR: "b''"
[0m15:09:15.169996 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:09:15.170161 [info ] [MainThread]: Connection:
[0m15:09:15.170326 [info ] [MainThread]:   database: analytics
[0m15:09:15.170454 [info ] [MainThread]:   schema: main
[0m15:09:15.170577 [info ] [MainThread]:   path: /Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:09:15.170698 [info ] [MainThread]:   config_options: None
[0m15:09:15.170813 [info ] [MainThread]:   extensions: None
[0m15:09:15.170964 [info ] [MainThread]:   settings: {}
[0m15:09:15.171129 [info ] [MainThread]:   external_root: .
[0m15:09:15.171260 [info ] [MainThread]:   use_credential_provider: None
[0m15:09:15.171386 [info ] [MainThread]:   attach: None
[0m15:09:15.171503 [info ] [MainThread]:   filesystems: None
[0m15:09:15.171620 [info ] [MainThread]:   remote: None
[0m15:09:15.171735 [info ] [MainThread]:   plugins: None
[0m15:09:15.171850 [info ] [MainThread]:   disable_transactions: False
[0m15:09:15.172187 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:09:15.220982 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:09:15.245676 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:09:15.245945 [debug] [MainThread]: On debug: select 1 as id
[0m15:09:15.246092 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:09:15.260874 [debug] [MainThread]: SQL status: OK in 0.015 seconds
[0m15:09:15.261494 [debug] [MainThread]: On debug: Close
[0m15:09:15.261710 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:09:15.261883 [info ] [MainThread]: [31m1 check failed:[0m
[0m15:09:15.262015 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml


[0m15:09:15.263457 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.30779934, "process_in_blocks": "0", "process_kernel_time": 0.184507, "process_mem_max_rss": "132579328", "process_out_blocks": "0", "process_user_time": 0.796654}
[0m15:09:15.263733 [debug] [MainThread]: Command `dbt debug` failed at 15:09:15.263679 after 0.31 seconds
[0m15:09:15.263917 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:09:15.264098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d80bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dd6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110771cd0>]}
[0m15:09:15.264315 [debug] [MainThread]: Flushing usage events
[0m15:09:15.377537 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:09:33.872265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049e1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cd9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cd93d0>]}


============================== 15:09:33.875381 | ebe0baec-2ec5-402a-a245-201d76e95d57 ==============================
[0m15:09:33.875381 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:09:33.875709 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt debug', 'log_cache_events': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'empty': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'write_json': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'no_print': 'None', 'target_path': 'None', 'quiet': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False'}
[0m15:09:33.883528 [info ] [MainThread]: dbt version: 1.11.4
[0m15:09:33.883792 [info ] [MainThread]: python version: 3.12.12
[0m15:09:33.883945 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:09:33.884295 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:09:33.996431 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:09:33.996700 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:09:33.996843 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:09:33.997972 [info ] [MainThread]: adapter type: duckdb
[0m15:09:33.998153 [info ] [MainThread]: adapter version: 1.10.0
[0m15:09:34.044590 [info ] [MainThread]: Configuration:
[0m15:09:34.044868 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:09:34.045046 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:09:34.045182 [info ] [MainThread]: Required dependencies:
[0m15:09:34.045388 [debug] [MainThread]: Executing "git --help"
[0m15:09:34.063455 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:09:34.064020 [debug] [MainThread]: STDERR: "b''"
[0m15:09:34.064208 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:09:34.064374 [info ] [MainThread]: Connection:
[0m15:09:34.064537 [info ] [MainThread]:   database: analytics
[0m15:09:34.064666 [info ] [MainThread]:   schema: main
[0m15:09:34.064797 [info ] [MainThread]:   path: /Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:09:34.064918 [info ] [MainThread]:   config_options: None
[0m15:09:34.065037 [info ] [MainThread]:   extensions: None
[0m15:09:34.065155 [info ] [MainThread]:   settings: {}
[0m15:09:34.065268 [info ] [MainThread]:   external_root: .
[0m15:09:34.065381 [info ] [MainThread]:   use_credential_provider: None
[0m15:09:34.065493 [info ] [MainThread]:   attach: None
[0m15:09:34.065601 [info ] [MainThread]:   filesystems: None
[0m15:09:34.065711 [info ] [MainThread]:   remote: None
[0m15:09:34.065823 [info ] [MainThread]:   plugins: None
[0m15:09:34.065931 [info ] [MainThread]:   disable_transactions: False
[0m15:09:34.066247 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:09:34.113549 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:09:34.135492 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:09:34.135730 [debug] [MainThread]: On debug: select 1 as id
[0m15:09:34.135873 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:09:34.149897 [debug] [MainThread]: SQL status: OK in 0.014 seconds
[0m15:09:34.150523 [debug] [MainThread]: On debug: Close
[0m15:09:34.150747 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:09:34.150913 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:09:34.152318 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.31489116, "process_in_blocks": "0", "process_kernel_time": 0.181845, "process_mem_max_rss": "133693440", "process_out_blocks": "0", "process_user_time": 0.812649}
[0m15:09:34.152596 [debug] [MainThread]: Command `dbt debug` succeeded at 15:09:34.152545 after 0.32 seconds
[0m15:09:34.152763 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:09:34.152952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ea9040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e78f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a72a80>]}
[0m15:09:34.153163 [debug] [MainThread]: Flushing usage events
[0m15:09:34.249012 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:52.273317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10306abd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10574b500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b5760>]}


============================== 14:31:52.278246 | bc244f8d-0078-408a-965c-b548e369443e ==============================
[0m14:31:52.278246 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:31:52.278602 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'introspect': 'True', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'cache_selected_only': 'False', 'use_colors': 'True', 'write_json': 'True', 'printer_width': '80', 'empty': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_format': 'default', 'log_cache_events': 'False', 'fail_fast': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select staging', 'partial_parse': 'True'}
[0m14:31:52.764542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10644a030>]}
[0m14:31:52.792985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10646af00>]}
[0m14:31:52.794426 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:31:52.857093 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:31:52.857605 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:31:52.857807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e1400>]}
[0m14:31:53.224063 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.saas_analytics
[0m14:31:53.226483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e0a70>]}
[0m14:31:53.248032 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:31:53.249265 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:31:53.260527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b9580>]}
[0m14:31:53.260789 [info ] [MainThread]: Found 472 macros
[0m14:31:53.260954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106752e70>]}
[0m14:31:53.261208 [warn ] [MainThread]: The selection criterion 'staging' does not match any enabled nodes
[0m14:31:53.261735 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:31:53.261986 [debug] [MainThread]: Command end result
[0m14:31:53.271796 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:31:53.272951 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:31:53.274596 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:31:53.294610 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0616667, "process_in_blocks": "0", "process_kernel_time": 0.204515, "process_mem_max_rss": "130809856", "process_out_blocks": "0", "process_user_time": 1.276116}
[0m14:31:53.294932 [debug] [MainThread]: Command `dbt run` succeeded at 14:31:53.294877 after 1.06 seconds
[0m14:31:53.295166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060bec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107154260>]}
[0m14:31:53.295379 [debug] [MainThread]: Flushing usage events
[0m14:31:53.410041 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:11.669683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084d08f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be5760>]}


============================== 14:32:11.673279 | 7555be95-b896-47cf-be73-46ba0867ee81 ==============================
[0m14:32:11.673279 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:32:11.673625 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'static_parser': 'True', 'version_check': 'True', 'use_colors': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'fail_fast': 'False', 'no_print': 'None', 'empty': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'introspect': 'True', 'invocation_command': 'dbt test --select staging', 'write_json': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None'}
[0m14:32:11.835673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c4ddc0>]}
[0m14:32:11.863737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108508590>]}
[0m14:32:11.865715 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:32:11.916899 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:32:11.950541 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:32:11.950773 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:32:11.950908 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:32:11.951211 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.saas_analytics
[0m14:32:11.953851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ead730>]}
[0m14:32:11.973816 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:32:11.975156 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:32:11.990889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10925cb00>]}
[0m14:32:11.991164 [info ] [MainThread]: Found 472 macros
[0m14:32:11.991331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10916f3e0>]}
[0m14:32:11.991602 [warn ] [MainThread]: The selection criterion 'staging' does not match any enabled nodes
[0m14:32:11.992212 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:32:11.993098 [debug] [MainThread]: Command end result
[0m14:32:12.002825 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:32:12.003890 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:32:12.005541 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:32:12.006973 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.37557387, "process_in_blocks": "0", "process_kernel_time": 0.166873, "process_mem_max_rss": "132005888", "process_out_blocks": "0", "process_user_time": 0.878941}
[0m14:32:12.007226 [debug] [MainThread]: Command `dbt test` succeeded at 14:32:12.007179 after 0.38 seconds
[0m14:32:12.007416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108780920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092109e0>]}
[0m14:32:12.007595 [debug] [MainThread]: Flushing usage events
[0m14:32:12.108704 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:56.124482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10386cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb98e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb9670>]}


============================== 14:34:56.130856 | e3f22cac-a5a3-4d16-a8c4-77285b7b380e ==============================
[0m14:34:56.130856 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:56.131197 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'partial_parse': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select staging', 'log_format': 'default', 'use_colors': 'True', 'warn_error': 'None', 'static_parser': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'debug': 'False', 'write_json': 'True'}
[0m14:34:56.292576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e3f22cac-a5a3-4d16-a8c4-77285b7b380e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f225d0>]}
[0m14:34:56.321948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e3f22cac-a5a3-4d16-a8c4-77285b7b380e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104878cb0>]}
[0m14:34:56.323371 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:56.374828 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:56.399595 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/sources.yml - Runtime Error
    Syntax error near line 7
    ------------------------------
    4  |   - name: raw
    5  |     description: >
    6  |     Raw data loaded directly into DuckDB. These tables are the unmodified
    7  |     source of truth and should only be referenced via staging models.
    8  |   schema: main
    9  | 
    10 |   tables:
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 6, column 5
    could not find expected ':'
      in "<unicode string>", line 7, column 5
[0m14:34:56.401117 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3159924, "process_in_blocks": "0", "process_kernel_time": 0.168035, "process_mem_max_rss": "130351104", "process_out_blocks": "0", "process_user_time": 0.844647}
[0m14:34:56.401376 [debug] [MainThread]: Command `dbt run` failed at 14:34:56.401325 after 0.32 seconds
[0m14:34:56.401579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb9760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105185700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051623c0>]}
[0m14:34:56.401756 [debug] [MainThread]: Flushing usage events
[0m14:34:56.532681 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:32.956142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a5a600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105809a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058097f0>]}


============================== 14:35:32.960204 | 428b17c0-3778-49fc-ae26-cb90c34287aa ==============================
[0m14:35:32.960204 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:35:32.960579 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'write_json': 'True', 'invocation_command': 'dbt run --select staging', 'log_cache_events': 'False', 'no_print': 'None', 'fail_fast': 'False', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'printer_width': '80', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'introspect': 'True'}
[0m14:35:33.120081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105913aa0>]}
[0m14:35:33.149350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058a77a0>]}
[0m14:35:33.150903 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:35:33.205797 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:35:33.243913 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 5 files added, 0 files changed.
[0m14:35:33.244209 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_subscriptions.sql
[0m14:35:33.244373 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_users.sql
[0m14:35:33.244564 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/sources.yml
[0m14:35:33.244752 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/staging.yml
[0m14:35:33.244898 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_events.sql
[0m14:35:33.457309 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:35:33.457638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d1e20>]}
[0m14:35:33.557991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f93a40>]}
[0m14:35:33.599715 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:33.601312 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:33.613985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa1d30>]}
[0m14:35:33.614272 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:35:33.614440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f8f380>]}
[0m14:35:33.615377 [info ] [MainThread]: 
[0m14:35:33.615730 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:35:33.615911 [info ] [MainThread]: 
[0m14:35:33.616161 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:35:33.618329 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:35:33.658412 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:35:33.658652 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:35:33.658796 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:33.676669 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:35:33.677457 [debug] [ThreadPool]: On list_analytics: Close
[0m14:35:33.677832 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:35:33.678083 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:35:33.681307 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.681479 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:35:33.681611 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:35:33.682396 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:35:33.682996 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.683136 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:35:33.683357 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.683482 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.683605 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:35:33.684062 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.684431 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:35:33.684557 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.684670 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:35:33.684862 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.684991 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:35:33.685539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:35:33.688198 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:33.688369 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:35:33.688553 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:35:33.688809 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.688942 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:33.689079 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:35:33.698901 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:35:33.699593 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:35:33.700640 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:35:33.700791 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:35:33.701241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f69190>]}
[0m14:35:33.701451 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.701578 [debug] [MainThread]: On master: BEGIN
[0m14:35:33.701689 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:35:33.701925 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.702051 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.702171 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.702287 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.702484 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.702606 [debug] [MainThread]: On master: Close
[0m14:35:33.704041 [debug] [Thread-1 (]: Began running node model.saas_analytics.stg_events
[0m14:35:33.704237 [debug] [Thread-2 (]: Began running node model.saas_analytics.stg_subscriptions
[0m14:35:33.704617 [debug] [Thread-3 (]: Began running node model.saas_analytics.stg_users
[0m14:35:33.704473 [info ] [Thread-1 (]: 1 of 3 START sql view model main.stg_events .................................... [RUN]
[0m14:35:33.704865 [info ] [Thread-2 (]: 2 of 3 START sql view model main.stg_subscriptions ............................. [RUN]
[0m14:35:33.705082 [info ] [Thread-3 (]: 3 of 3 START sql view model main.stg_users ..................................... [RUN]
[0m14:35:33.705327 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.stg_events)
[0m14:35:33.705585 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_subscriptions'
[0m14:35:33.705823 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_users'
[0m14:35:33.706004 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.stg_events
[0m14:35:33.706177 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.stg_subscriptions
[0m14:35:33.706329 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.stg_users
[0m14:35:33.710228 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.stg_events"
[0m14:35:33.712003 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.stg_subscriptions"
[0m14:35:33.714590 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.stg_users"
[0m14:35:33.715427 [debug] [Thread-1 (]: Began executing node model.saas_analytics.stg_events
[0m14:35:33.715628 [debug] [Thread-2 (]: Began executing node model.saas_analytics.stg_subscriptions
[0m14:35:33.715780 [debug] [Thread-3 (]: Began executing node model.saas_analytics.stg_users
[0m14:35:33.732455 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.stg_subscriptions"
[0m14:35:33.733792 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.stg_events"
[0m14:35:33.735391 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.stg_users"
[0m14:35:33.736106 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.736279 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: BEGIN
[0m14:35:33.736427 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:33.736766 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.737105 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:35:33.737308 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: BEGIN
[0m14:35:33.737495 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: BEGIN
[0m14:35:33.737657 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.737815 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:35:33.737966 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:35:33.738111 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.738453 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

  
  create view "analytics"."main"."stg_events__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_events"
),

renamed as (
    select
        event_id,
        user_id,
        event_timestamp as event_at,
        lower(trim(event_name)) as event_name,
        lower(trim(device_type)) as device_type,
        lower(trim(plan_type)) as plan_type,
        experiment_variant,
        event_properties
    from source 
)

select * from renamed
  );

[0m14:35:33.739016 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739178 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739351 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.739534 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:35:33.739689 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739853 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

  
  create view "analytics"."main"."stg_subscriptions__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_subscriptions"
),

renamed as (
    select
        subscription_id,
        user_id,
        start_date as subscription_started_at,
        end_date as subscription_ended_at,
        lower(trim(plan)) as plan,
        lower(trim(status)) as subscription_status,
        monthly_revenue as monthly_revenue_usd,
        end_date is null as is_active,
        end_date is not null as is_churned
    from source
)

select * from renamed
  );

[0m14:35:33.740040 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquistion_channel)) as acquistion_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:35:33.743220 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.743629 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events__dbt_tmp" rename to "stg_events"
[0m14:35:33.744097 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.745662 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.745850 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions__dbt_tmp" rename to "stg_subscriptions"
[0m14:35:33.746019 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.751096 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:35:33.751340 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.752165 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquistion_channel)) as acquistion_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:35:33.752392 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:35:33.752562 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:35:33.752713 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m14:35:33.753579 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:35:33.753852 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: ROLLBACK
[0m14:35:33.754048 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.754463 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.755326 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:35:33.758281 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.758599 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

      drop view if exists "analytics"."main"."stg_events__dbt_backup" cascade
    
[0m14:35:33.758902 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:35:33.760173 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.760380 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

      drop view if exists "analytics"."main"."stg_subscriptions__dbt_backup" cascade
    
[0m14:35:33.760555 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.761910 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: Close
[0m14:35:33.763030 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.763759 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: Close
[0m14:35:33.764536 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106472db0>]}
[0m14:35:33.764716 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10647c920>]}
[0m14:35:33.765099 [info ] [Thread-1 (]: 1 of 3 OK created sql view model main.stg_events ............................... [[32mOK[0m in 0.06s]
[0m14:35:33.765648 [debug] [Thread-1 (]: Finished running node model.saas_analytics.stg_events
[0m14:35:33.765359 [info ] [Thread-2 (]: 2 of 3 OK created sql view model main.stg_subscriptions ........................ [[32mOK[0m in 0.06s]
[0m14:35:33.766060 [debug] [Thread-2 (]: Finished running node model.saas_analytics.stg_subscriptions
[0m14:35:33.769347 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.stg_users'
[0m14:35:33.769548 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: Close
[0m14:35:33.770929 [debug] [Thread-3 (]: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m14:35:33.771443 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106335f10>]}
[0m14:35:33.772110 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model main.stg_users ............................ [[31mERROR[0m in 0.07s]
[0m14:35:33.772467 [debug] [Thread-3 (]: Finished running node model.saas_analytics.stg_users
[0m14:35:33.772713 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.stg_users' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined.
[0m14:35:33.773779 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.773963 [debug] [MainThread]: On master: BEGIN
[0m14:35:33.774101 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:35:33.775150 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m14:35:33.775526 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.775874 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.776184 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.776504 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.776656 [debug] [MainThread]: On master: Close
[0m14:35:33.776860 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:33.776983 [debug] [MainThread]: Connection 'model.saas_analytics.stg_events' was properly closed.
[0m14:35:33.777099 [debug] [MainThread]: Connection 'model.saas_analytics.stg_subscriptions' was properly closed.
[0m14:35:33.777205 [debug] [MainThread]: Connection 'model.saas_analytics.stg_users' was properly closed.
[0m14:35:33.777376 [info ] [MainThread]: 
[0m14:35:33.777536 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m14:35:33.778101 [debug] [MainThread]: Command end result
[0m14:35:33.795149 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:33.796263 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:33.799561 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:35:33.799798 [info ] [MainThread]: 
[0m14:35:33.799984 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:35:33.800132 [info ] [MainThread]: 
[0m14:35:33.800308 [error] [MainThread]: [31mFailure in model stg_users (models/staging/stg_users.sql)[0m
[0m14:35:33.800591 [error] [MainThread]:   Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m14:35:33.800720 [info ] [MainThread]: 
[0m14:35:33.800875 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/stg_users.sql
[0m14:35:33.801011 [info ] [MainThread]: 
[0m14:35:33.801158 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:35:33.801417 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:35:33.802867 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8866466, "process_in_blocks": "0", "process_kernel_time": 0.191036, "process_mem_max_rss": "151142400", "process_out_blocks": "0", "process_user_time": 1.314644}
[0m14:35:33.803140 [debug] [MainThread]: Command `dbt run` failed at 14:35:33.803092 after 0.89 seconds
[0m14:35:33.803345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058098b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105913380>]}
[0m14:35:33.803573 [debug] [MainThread]: Flushing usage events
[0m14:35:33.908272 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:22.178238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a4bf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779da60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779d820>]}


============================== 14:36:22.183847 | d8c7331f-53e0-4b4f-92d8-3d8162b04be5 ==============================
[0m14:36:22.183847 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:36:22.184193 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select staging', 'target_path': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'introspect': 'True', 'debug': 'False', 'write_json': 'True', 'static_parser': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'printer_width': '80', 'partial_parse': 'True'}
[0m14:36:22.349409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10798e1e0>]}
[0m14:36:22.378869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107659cd0>]}
[0m14:36:22.380327 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:36:22.431207 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:36:22.495110 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:36:22.495558 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_events.sql
[0m14:36:22.495783 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_subscriptions.sql
[0m14:36:22.495939 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_users.sql
[0m14:36:22.654332 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:36:22.654663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fa52b0>]}
[0m14:36:22.715227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ffb8f0>]}
[0m14:36:22.787719 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:22.789639 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:22.802822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107276cc0>]}
[0m14:36:22.803112 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:36:22.803284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd6240>]}
[0m14:36:22.804262 [info ] [MainThread]: 
[0m14:36:22.804465 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:36:22.804615 [info ] [MainThread]: 
[0m14:36:22.804861 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:36:22.807020 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:36:22.836010 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:36:22.836255 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:36:22.836407 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:22.853946 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:36:22.854900 [debug] [ThreadPool]: On list_analytics: Close
[0m14:36:22.855244 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:36:22.855491 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:36:22.858824 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.859007 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:36:22.859144 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:36:22.859806 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:36:22.860442 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.860587 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:36:22.860815 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.860944 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.861075 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:36:22.861477 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.861864 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:36:22.861992 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.862103 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:36:22.862300 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.862426 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:36:22.863001 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:36:22.865781 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:22.871566 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:36:22.871822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:36:22.872209 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.872395 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:22.872547 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:36:22.883496 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:36:22.884249 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:36:22.885212 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:36:22.885377 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:36:22.886146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fd3140>]}
[0m14:36:22.886370 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.886498 [debug] [MainThread]: On master: BEGIN
[0m14:36:22.886616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:36:22.886889 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.887046 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.887184 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.887304 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.887522 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.887648 [debug] [MainThread]: On master: Close
[0m14:36:22.889063 [debug] [Thread-1 (]: Began running node model.saas_analytics.stg_events
[0m14:36:22.889254 [debug] [Thread-2 (]: Began running node model.saas_analytics.stg_subscriptions
[0m14:36:22.889437 [debug] [Thread-3 (]: Began running node model.saas_analytics.stg_users
[0m14:36:22.889675 [info ] [Thread-1 (]: 1 of 3 START sql view model main.stg_events .................................... [RUN]
[0m14:36:22.889947 [info ] [Thread-2 (]: 2 of 3 START sql view model main.stg_subscriptions ............................. [RUN]
[0m14:36:22.890170 [info ] [Thread-3 (]: 3 of 3 START sql view model main.stg_users ..................................... [RUN]
[0m14:36:22.890385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.stg_events)
[0m14:36:22.890611 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_subscriptions'
[0m14:36:22.890812 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_users'
[0m14:36:22.890973 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.stg_events
[0m14:36:22.891126 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.stg_subscriptions
[0m14:36:22.891273 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.stg_users
[0m14:36:22.894881 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.stg_events"
[0m14:36:22.896238 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.stg_subscriptions"
[0m14:36:22.897748 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.stg_users"
[0m14:36:22.898652 [debug] [Thread-3 (]: Began executing node model.saas_analytics.stg_users
[0m14:36:22.898893 [debug] [Thread-2 (]: Began executing node model.saas_analytics.stg_subscriptions
[0m14:36:22.914522 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.stg_users"
[0m14:36:22.916209 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.stg_subscriptions"
[0m14:36:22.916387 [debug] [Thread-1 (]: Began executing node model.saas_analytics.stg_events
[0m14:36:22.917984 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.stg_events"
[0m14:36:22.918621 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.918821 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.919045 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: BEGIN
[0m14:36:22.919241 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.919406 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: BEGIN
[0m14:36:22.919570 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:36:22.919718 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: BEGIN
[0m14:36:22.919863 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:36:22.920119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:22.920366 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.920598 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.920766 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.920923 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.921064 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.921230 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquisition_channel)) as acquisition_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:36:22.921386 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.921554 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

  
  create view "analytics"."main"."stg_subscriptions__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_subscriptions"
),

renamed as (
    select
        subscription_id,
        user_id,
        start_date as subscription_started_at,
        end_date as subscription_ended_at,
        lower(trim(plan)) as plan,
        lower(trim(status)) as subscription_status,
        monthly_revenue as monthly_revenue_usd,
        end_date is null as is_active,
        end_date is not null as is_churned
    from source
)

select * from renamed
  );

[0m14:36:22.921982 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

  
  create view "analytics"."main"."stg_events__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_events"
),

renamed as (
    select
        event_id,
        user_id,
        event_timestamp as event_at,
        lower(trim(event_name)) as event_name,
        lower(trim(device_type)) as device_type,
        lower(trim(plan_type)) as plan_type,
        experiment_variant,
        event_properties
    from source 
)

select * from renamed
  );

[0m14:36:22.922315 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.925467 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.925661 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */
alter view "analytics"."main"."stg_users__dbt_tmp" rename to "stg_users"
[0m14:36:22.926028 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:36:22.926189 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:36:22.927575 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.927757 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.929114 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.929295 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions" rename to "stg_subscriptions__dbt_backup"
[0m14:36:22.933831 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: COMMIT
[0m14:36:22.934026 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events" rename to "stg_events__dbt_backup"
[0m14:36:22.934267 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.934474 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:36:22.934639 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: COMMIT
[0m14:36:22.934790 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:36:22.936130 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.937530 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.937704 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions__dbt_tmp" rename to "stg_subscriptions"
[0m14:36:22.937864 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.938020 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events__dbt_tmp" rename to "stg_events"
[0m14:36:22.941216 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.941460 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m14:36:22.941620 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

      drop view if exists "analytics"."main"."stg_users__dbt_backup" cascade
    
[0m14:36:22.943196 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:36:22.943352 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.943740 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.944410 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:36:22.944577 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:36:22.944724 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.944876 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.945953 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: Close
[0m14:36:22.946133 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:36:22.946279 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.948149 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.948319 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

      drop view if exists "analytics"."main"."stg_subscriptions__dbt_backup" cascade
    
[0m14:36:22.948533 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.949802 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.949970 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

      drop view if exists "analytics"."main"."stg_events__dbt_backup" cascade
    
[0m14:36:22.950131 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.950814 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: Close
[0m14:36:22.951119 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e46f60>]}
[0m14:36:22.951296 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.951435 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108333140>]}
[0m14:36:22.951722 [info ] [Thread-3 (]: 3 of 3 OK created sql view model main.stg_users ................................ [[32mOK[0m in 0.06s]
[0m14:36:22.952348 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: Close
[0m14:36:22.952841 [debug] [Thread-3 (]: Finished running node model.saas_analytics.stg_users
[0m14:36:22.952624 [info ] [Thread-2 (]: 2 of 3 OK created sql view model main.stg_subscriptions ........................ [[32mOK[0m in 0.06s]
[0m14:36:22.953171 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108240560>]}
[0m14:36:22.953388 [debug] [Thread-2 (]: Finished running node model.saas_analytics.stg_subscriptions
[0m14:36:22.953660 [info ] [Thread-1 (]: 1 of 3 OK created sql view model main.stg_events ............................... [[32mOK[0m in 0.06s]
[0m14:36:22.953983 [debug] [Thread-1 (]: Finished running node model.saas_analytics.stg_events
[0m14:36:22.954543 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.954679 [debug] [MainThread]: On master: BEGIN
[0m14:36:22.954794 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:36:22.955044 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.955174 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.955295 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.955402 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.955593 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.955713 [debug] [MainThread]: On master: Close
[0m14:36:22.955868 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:22.955977 [debug] [MainThread]: Connection 'model.saas_analytics.stg_events' was properly closed.
[0m14:36:22.956079 [debug] [MainThread]: Connection 'model.saas_analytics.stg_subscriptions' was properly closed.
[0m14:36:22.956180 [debug] [MainThread]: Connection 'model.saas_analytics.stg_users' was properly closed.
[0m14:36:22.956319 [info ] [MainThread]: 
[0m14:36:22.956456 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m14:36:22.956840 [debug] [MainThread]: Command end result
[0m14:36:22.969148 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:22.970043 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:22.973210 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:36:22.973408 [info ] [MainThread]: 
[0m14:36:22.973781 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:36:22.973936 [info ] [MainThread]: 
[0m14:36:22.974097 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:36:22.974359 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:36:22.975851 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.8371948, "process_in_blocks": "0", "process_kernel_time": 0.197084, "process_mem_max_rss": "148176896", "process_out_blocks": "0", "process_user_time": 1.281671}
[0m14:36:22.976083 [debug] [MainThread]: Command `dbt run` succeeded at 14:36:22.976042 after 0.84 seconds
[0m14:36:22.976268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108087fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a3dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080dfec0>]}
[0m14:36:22.976444 [debug] [MainThread]: Flushing usage events
[0m14:36:23.100067 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:39.961749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10450ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048d1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048d1880>]}


============================== 14:36:39.964912 | d0756bc3-e391-464d-b037-6ff575cce075 ==============================
[0m14:36:39.964912 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:36:39.965237 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'empty': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'introspect': 'True', 'invocation_command': 'dbt test --select staging', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'write_json': 'True', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'indirect_selection': 'eager', 'no_print': 'None'}
[0m14:36:40.113281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a6e390>]}
[0m14:36:40.141423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104950350>]}
[0m14:36:40.142940 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:36:40.193902 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:36:40.256810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:36:40.257054 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:36:40.257190 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:36:40.277983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b71220>]}
[0m14:36:40.317898 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:40.319241 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:40.336816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105014950>]}
[0m14:36:40.337126 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:36:40.337313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd6de0>]}
[0m14:36:40.338471 [info ] [MainThread]: 
[0m14:36:40.338649 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:36:40.338789 [info ] [MainThread]: 
[0m14:36:40.339048 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:36:40.341266 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:36:40.369427 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:40.369669 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:36:40.369819 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:40.380138 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:36:40.380983 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:40.381661 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:36:40.395607 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m14:36:40.396495 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:36:40.397484 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:36:40.397655 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:36:40.398331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f46d20>]}
[0m14:36:40.398569 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.398704 [debug] [MainThread]: On master: BEGIN
[0m14:36:40.398825 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:36:40.399096 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.399227 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.399353 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.399475 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.399677 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.399803 [debug] [MainThread]: On master: Close
[0m14:36:40.401300 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.401492 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.401841 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.401696 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired  [RUN]
[0m14:36:40.402055 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.402222 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:36:40.402396 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:36:40.402608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866)
[0m14:36:40.402778 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:36:40.403026 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:36:40.403764 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:36:40.403968 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.404214 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:36:40.404412 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.404575 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.413774 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.414041 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.418378 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.420297 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.422179 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.422627 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.428740 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.430965 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.432165 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.432360 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.432605 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.433740 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.434832 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.435144 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.435338 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: BEGIN
[0m14:36:40.435512 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.435708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.435882 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:36:40.436055 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.436223 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.436478 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:36:40.436639 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:36:40.436803 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.436949 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:36:40.437190 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:36:40.437356 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.437510 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.437643 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:36:40.437867 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','cancelled','expired'
)



  
  
      
    ) dbt_internal_test
[0m14:36:40.438036 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.438179 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.438479 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.438646 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.438799 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.439025 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.439203 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.439388 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.440537 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.443646 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:36:40.443925 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.444104 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.444541 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:36:40.444684 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.445405 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:36:40.445569 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:36:40.446203 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: ROLLBACK
[0m14:36:40.446878 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:36:40.447302 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:36:40.447592 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:36:40.448070 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866'
[0m14:36:40.448481 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:36:40.448645 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:36:40.448894 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.449531 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: Close
[0m14:36:40.449700 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:36:40.450122 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.449964 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.05s]
[0m14:36:40.450443 [error] [Thread-1 (]: 1 of 17 FAIL 1 accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired  [[31mFAIL 1[0m in 0.05s]
[0m14:36:40.450688 [error] [Thread-2 (]: 2 of 17 FAIL 8000 not_null_stg_events_event_at ................................. [[31mFAIL 8000[0m in 0.05s]
[0m14:36:40.450876 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:36:40.451115 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.451334 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.451541 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.451713 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:36:40.451890 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.452066 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.452243 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.452404 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.452557 [info ] [Thread-4 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:36:40.452753 [info ] [Thread-1 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:36:40.452958 [info ] [Thread-2 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:36:40.456090 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.456511 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:36:40.456794 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:36:40.456985 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:36:40.457234 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.457406 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.457575 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.459736 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.461592 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.463499 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.463683 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.465003 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.465355 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.465532 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.465696 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.466803 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.467026 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.468099 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.469256 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.469462 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:36:40.469694 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.469880 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.470188 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:36:40.470339 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.470516 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.470776 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.470965 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.471122 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:36:40.471280 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.471427 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.471597 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:36:40.471755 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.471906 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.472123 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.472456 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.472762 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.473145 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.473365 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.473535 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.473713 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.473886 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.474052 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.474245 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.474401 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.475277 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:36:40.475567 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.476267 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:36:40.476716 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:36:40.476920 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.477568 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:36:40.477995 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:36:40.478170 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:36:40.478845 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:36:40.479264 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:36:40.479428 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:36:40.479877 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:36:40.480296 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:36:40.480156 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:36:40.480606 [info ] [Thread-4 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.02s]
[0m14:36:40.480780 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:36:40.481212 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.481016 [info ] [Thread-2 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.02s]
[0m14:36:40.481464 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.481847 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.481694 [info ] [Thread-1 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.02s]
[0m14:36:40.482126 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.482297 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.482655 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.482467 [info ] [Thread-3 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:36:40.482861 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.483161 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.483020 [info ] [Thread-4 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:36:40.483389 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:36:40.483546 [info ] [Thread-2 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:36:40.483718 [info ] [Thread-1 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:36:40.483907 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:36:40.484070 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.484226 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:36:40.484395 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:36:40.484550 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.486749 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.486987 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.487167 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.489520 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.492542 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.526371 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.526750 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.527986 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.528215 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.529294 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.529542 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.530654 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.530876 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.531911 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.532149 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.532322 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:36:40.532511 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.532688 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.532855 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.533025 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:36:40.533300 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.533461 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:36:40.533622 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.533763 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.533908 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:36:40.534045 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.534187 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.534378 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.534570 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.534739 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.534901 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.535085 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.535300 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.535455 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.535625 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.535784 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.535947 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.536149 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.536338 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.537288 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:36:40.537557 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.538009 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:36:40.538169 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.538313 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.538505 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:36:40.539164 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:36:40.539845 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:36:40.540488 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:36:40.541201 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:36:40.540801 [info ] [Thread-3 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.06s]
[0m14:36:40.541680 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:36:40.542083 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:36:40.542230 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:36:40.542462 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.542624 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:36:40.542781 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:36:40.543181 [debug] [Thread-3 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.543027 [info ] [Thread-1 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.06s]
[0m14:36:40.543490 [info ] [Thread-2 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.06s]
[0m14:36:40.543691 [info ] [Thread-4 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.06s]
[0m14:36:40.544109 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.543878 [info ] [Thread-3 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:36:40.544362 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.544567 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.544732 [debug] [Thread-1 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.544916 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:36:40.545083 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.545250 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.545410 [info ] [Thread-1 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:36:40.545595 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.545770 [info ] [Thread-2 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:36:40.545995 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:36:40.546232 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:36:40.550291 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.550519 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:36:40.550697 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:36:40.550874 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.551076 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.551269 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.553618 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.557223 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.559207 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.559464 [debug] [Thread-3 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.560764 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.561204 [debug] [Thread-1 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.561385 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.561561 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.561747 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.563756 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.564815 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.565889 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.566070 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:36:40.566362 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.566813 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.567007 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.567178 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.567344 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:36:40.567506 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:36:40.567672 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.567817 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.567960 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.568109 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.568251 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:36:40.568421 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.568731 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.568920 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569086 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569372 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.569537 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.569687 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569844 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.570039 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.570225 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.570689 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.572507 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.573645 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:36:40.574284 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:36:40.574489 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.574670 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.574836 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:36:40.575004 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.575427 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:36:40.576238 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:36:40.576966 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:36:40.577673 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:36:40.577939 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.578435 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:36:40.578886 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:36:40.582263 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:36:40.582510 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.582709 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:36:40.582878 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:36:40.583032 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:36:40.583196 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:36:40.583536 [info ] [Thread-1 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.04s]
[0m14:36:40.583832 [info ] [Thread-3 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.04s]
[0m14:36:40.584248 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:36:40.584084 [info ] [Thread-2 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:36:40.584520 [debug] [Thread-1 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.584736 [debug] [Thread-3 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.584888 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.585086 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.587398 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.587990 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.589383 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.589997 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.590163 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:36:40.590306 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.590666 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:36:40.590817 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.590973 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.592546 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.593369 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:36:40.593853 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:36:40.594028 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:36:40.594308 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:36:40.594558 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.595256 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.595411 [debug] [MainThread]: On master: BEGIN
[0m14:36:40.595530 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:36:40.595799 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.595930 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.596059 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.596177 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.596369 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.596494 [debug] [MainThread]: On master: Close
[0m14:36:40.596654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:40.596776 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:36:40.596882 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:36:40.596986 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:36:40.597086 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:36:40.597215 [info ] [MainThread]: 
[0m14:36:40.597352 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m14:36:40.598390 [debug] [MainThread]: Command end result
[0m14:36:40.612312 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:40.613423 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:40.616863 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:36:40.617040 [info ] [MainThread]: 
[0m14:36:40.617220 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m14:36:40.617364 [info ] [MainThread]: 
[0m14:36:40.617540 [error] [MainThread]: [31mFailure in test accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired (models/staging/staging.yml)[0m
[0m14:36:40.617706 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:36:40.617826 [info ] [MainThread]: 
[0m14:36:40.617977 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/accepted_values_stg_subscripti_43218a3e992f3fc96ed40658d91620bb.sql
[0m14:36:40.618102 [info ] [MainThread]: 
[0m14:36:40.618252 [error] [MainThread]: [31mFailure in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:36:40.618397 [error] [MainThread]:   Got 8000 results, configured to fail if != 0
[0m14:36:40.618511 [info ] [MainThread]: 
[0m14:36:40.618650 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:36:40.618771 [info ] [MainThread]: 
[0m14:36:40.618910 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=17
[0m14:36:40.620338 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.6966848, "process_in_blocks": "0", "process_kernel_time": 0.193666, "process_mem_max_rss": "158597120", "process_out_blocks": "0", "process_user_time": 1.2042}
[0m14:36:40.620564 [debug] [MainThread]: Command `dbt test` failed at 14:36:40.620524 after 0.70 seconds
[0m14:36:40.620754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10450ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b71940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b5a510>]}
[0m14:36:40.620924 [debug] [MainThread]: Flushing usage events
[0m14:36:40.718193 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:47:15.528980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106670c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5640>]}


============================== 14:47:15.534951 | e26f83cc-4287-41b3-9d5a-ff4aa184806d ==============================
[0m14:47:15.534951 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:47:15.535287 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'empty': 'None', 'fail_fast': 'False', 'target_path': 'None', 'warn_error': 'None', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'write_json': 'True', 'introspect': 'True', 'version_check': 'True', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'invocation_command': 'dbt test --select staging', 'static_parser': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:47:15.703626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e26f83cc-4287-41b3-9d5a-ff4aa184806d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10764a360>]}
[0m14:47:15.731817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e26f83cc-4287-41b3-9d5a-ff4aa184806d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072894c0>]}
[0m14:47:15.733106 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:47:15.786055 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:47:15.838290 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/staging.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |         description: Date the event was fired.
    47 |         tests:
    48 |           - not_null
    49 |             config:
    50 |               severity: warn
    51 | 
    52 |       - name: event_name
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 49, column 19
[0m14:47:15.839879 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.3486674, "process_in_blocks": "0", "process_kernel_time": 0.16323, "process_mem_max_rss": "131710976", "process_out_blocks": "0", "process_user_time": 0.853429}
[0m14:47:15.840157 [debug] [MainThread]: Command `dbt test` failed at 14:47:15.840104 after 0.35 seconds
[0m14:47:15.840357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077574a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ec0ad0>]}
[0m14:47:15.840535 [debug] [MainThread]: Flushing usage events
[0m14:47:15.983865 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:41.991172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106956180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb58e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb56a0>]}


============================== 14:50:41.997498 | 4229c09d-8046-4dae-b851-167cf9ff406d ==============================
[0m14:50:41.997498 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:50:41.997914 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt test --select staging', 'fail_fast': 'False', 'warn_error': 'None', 'no_print': 'None', 'static_parser': 'True', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'debug': 'False', 'cache_selected_only': 'False', 'printer_width': '80', 'use_colors': 'True', 'quiet': 'False'}
[0m14:50:42.159731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4229c09d-8046-4dae-b851-167cf9ff406d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c8dee0>]}
[0m14:50:42.188386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4229c09d-8046-4dae-b851-167cf9ff406d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11906d2e0>]}
[0m14:50:42.189792 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:50:42.240964 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:50:42.293475 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/staging.yml - Runtime Error
    Syntax error near line 45
    ------------------------------
    42 |               to: ref('stg_users')
    43 |               field: user_id
    44 | 
    45 |      - name: event_at
    46 |         description: Date the event was fired.
    47 |         tests:
    48 |           - not_null:
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 26, column 5
    did not find expected key
      in "<unicode string>", line 45, column 6
[0m14:50:42.294993 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.34547922, "process_in_blocks": "0", "process_kernel_time": 0.164853, "process_mem_max_rss": "132661248", "process_out_blocks": "0", "process_user_time": 0.859124}
[0m14:50:42.295259 [debug] [MainThread]: Command `dbt test` failed at 14:50:42.295204 after 0.35 seconds
[0m14:50:42.295456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190536e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d1e060>]}
[0m14:50:42.295644 [debug] [MainThread]: Flushing usage events
[0m14:50:42.431035 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:51:50.295375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043e6600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29730>]}


============================== 14:51:50.301963 | 56017025-6a9a-4d27-a371-92e6ffd68c47 ==============================
[0m14:51:50.301963 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:51:50.302292 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'printer_width': '80', 'fail_fast': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'write_json': 'True', 'target_path': 'None', 'quiet': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'invocation_command': 'dbt test --select staging', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'introspect': 'True', 'partial_parse': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True'}
[0m14:51:50.465699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10701a480>]}
[0m14:51:50.495468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065dd4c0>]}
[0m14:51:50.496930 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:51:50.549149 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:51:50.615571 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:51:50.616121 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:51:50.780249 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:51:50.780605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107469430>]}
[0m14:51:50.792209 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/staging.yml:
  	test definition dictionary must have exactly one key, got [('not null', None), ('config', {'severity': 'warn'})] instead (2 keys)
  	@: UnparsedModelUpdate(original_file_path='mode...ne)
[0m14:51:50.792598 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:51:50.797487 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.5418513, "process_in_blocks": "0", "process_kernel_time": 0.170472, "process_mem_max_rss": "136544256", "process_out_blocks": "0", "process_user_time": 1.0355}
[0m14:51:50.797826 [debug] [MainThread]: Command `dbt test` failed at 14:51:50.797764 after 0.54 seconds
[0m14:51:50.798140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107657a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107630c80>]}
[0m14:51:50.798365 [debug] [MainThread]: Flushing usage events
[0m14:51:50.903508 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:24.126563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dd1070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d790>]}


============================== 14:53:24.129776 | 5bb91f57-642f-4b57-bb86-820bee96a63f ==============================
[0m14:53:24.129776 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:53:24.130148 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test --select staging', 'use_experimental_parser': 'False', 'log_format': 'default', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'debug': 'False', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'introspect': 'True'}
[0m14:53:24.289339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b35ee0>]}
[0m14:53:24.318334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040d7c20>]}
[0m14:53:24.319678 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:53:24.370926 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:53:24.435501 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:53:24.436027 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:53:24.606536 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on
'stg_subscriptions' in package 'saas_analytics' (models/staging/staging.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m14:53:24.606860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10728cc80>]}
[0m14:53:24.652197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107355820>]}
[0m14:53:24.728337 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:24.729784 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:24.748584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107319c40>]}
[0m14:53:24.748907 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:53:24.749120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071ea210>]}
[0m14:53:24.750311 [info ] [MainThread]: 
[0m14:53:24.750510 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:53:24.750652 [info ] [MainThread]: 
[0m14:53:24.750891 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:53:24.753605 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:53:24.782595 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:24.782840 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:53:24.782993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:24.795574 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:53:24.795831 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:24.795987 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:53:24.812562 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:53:24.813616 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:53:24.814615 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:53:24.814811 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:53:24.815561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d5b50>]}
[0m14:53:24.815814 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.816367 [debug] [MainThread]: On master: BEGIN
[0m14:53:24.816497 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:24.816815 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.816949 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.817077 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.817195 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.817397 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.817527 [debug] [MainThread]: On master: Close
[0m14:53:24.820501 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.820728 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.820901 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.821060 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.821301 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__cancelled  [RUN]
[0m14:53:24.821541 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:53:24.821798 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:53:24.821987 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:53:24.822207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931)
[0m14:53:24.822429 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:24.822642 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:24.822838 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:24.822994 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.823147 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.823291 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.823433 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.827664 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.832294 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.834239 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.835949 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.836661 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.844894 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.845163 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.845366 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.845545 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.846697 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.847759 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.848868 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.849110 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.849371 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: BEGIN
[0m14:53:24.849562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.849906 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.850090 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.850301 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.850517 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.850665 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:53:24.850814 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:53:24.850960 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:53:24.851112 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.851256 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:53:24.851398 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:53:24.851538 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:53:24.851705 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','cancelled'
)



  
  
      
    ) dbt_internal_test
[0m14:53:24.852058 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852342 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852490 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852646 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.852800 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.852959 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.853125 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.853306 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.853477 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.854878 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.857267 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:53:24.857550 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:24.858019 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:24.858226 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m14:53:24.858407 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:53:24.858563 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:53:24.859238 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:53:24.859964 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:53:24.860865 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: ROLLBACK
[0m14:53:24.860263 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:53:24.861359 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:24.861765 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:24.862175 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931'
[0m14:53:24.862407 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.862579 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:53:24.862745 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:53:24.862895 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: Close
[0m14:53:24.863074 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.863359 [warn ] [Thread-2 (]: 2 of 17 WARN 8000 not_null_stg_events_event_at ................................. [[33mWARN 8000[0m in 0.04s]
[0m14:53:24.863583 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.04s]
[0m14:53:24.863889 [error] [Thread-1 (]: 1 of 17 FAIL 1 accepted_values_stg_subscriptions_subscription_status__active__cancelled  [[31mFAIL 1[0m in 0.04s]
[0m14:53:24.864079 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:53:24.864337 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.864564 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.864785 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.864959 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:53:24.865129 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.865309 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.865485 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.865679 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.865857 [info ] [Thread-2 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:53:24.866043 [info ] [Thread-4 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:53:24.869790 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.866186 [info ] [Thread-1 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:53:24.870133 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:53:24.870350 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:53:24.870576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:53:24.870761 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.870936 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.871116 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.873081 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.873293 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.875169 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.876909 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.878073 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.878624 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.879757 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.879958 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.881036 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.881247 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.882360 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.882612 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.882808 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:53:24.882996 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.883180 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.883358 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.883509 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:53:24.883674 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.883930 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:53:24.884085 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.884252 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.884398 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:53:24.884552 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.884789 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.884975 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.885129 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.885368 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.885615 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.885784 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.885948 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.886181 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.886360 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.886546 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.886739 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.886958 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.887144 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.888467 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:53:24.889007 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:53:24.889224 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889461 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889650 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889828 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:53:24.890686 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:53:24.891462 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:53:24.892176 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:53:24.892513 [info ] [Thread-2 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.02s]
[0m14:53:24.893060 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:53:24.893516 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:53:24.893756 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.894177 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:53:24.894369 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:53:24.894549 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:53:24.894749 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.894936 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:53:24.895205 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.895419 [info ] [Thread-1 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.02s]
[0m14:53:24.895623 [info ] [Thread-2 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:53:24.896193 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.895910 [info ] [Thread-4 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.03s]
[0m14:53:24.896526 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.896725 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:53:24.896915 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.897146 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.897315 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.897485 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.897649 [info ] [Thread-3 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:53:24.897876 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.898081 [info ] [Thread-1 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:53:24.900395 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.900630 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:53:24.900821 [info ] [Thread-4 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:53:24.901023 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:53:24.901239 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.901422 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:53:24.901581 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.903513 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.903736 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.905594 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.905801 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.909572 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.910936 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.911312 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.911511 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.912645 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.913751 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.913962 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.914153 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.914363 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:53:24.915517 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.915687 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.916060 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.916234 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:53:24.916420 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.916598 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.916744 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.916908 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:53:24.917068 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.917242 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.917450 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.917620 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.917785 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.917922 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:53:24.918203 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.918355 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.918493 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.918665 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.918834 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.919031 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.919227 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.919386 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.919585 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.920308 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:53:24.920472 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.920690 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.921163 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:53:24.921802 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:53:24.922006 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.922168 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:53:24.922644 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:53:24.922841 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.923470 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:53:24.923761 [info ] [Thread-2 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.03s]
[0m14:53:24.923955 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:53:24.924600 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:53:24.924842 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.925239 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:53:24.925493 [info ] [Thread-1 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.02s]
[0m14:53:24.925946 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:53:24.926120 [debug] [Thread-2 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.926305 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:53:24.926529 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.926682 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:53:24.926854 [info ] [Thread-2 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:53:24.927264 [debug] [Thread-1 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.927117 [info ] [Thread-3 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.03s]
[0m14:53:24.927707 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:53:24.927561 [info ] [Thread-4 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.928114 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.927917 [info ] [Thread-1 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:53:24.928312 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.928520 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.928680 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.928852 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:53:24.931511 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.931741 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.931910 [info ] [Thread-3 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:53:24.932122 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.932354 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:53:24.932588 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:53:24.934900 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.935127 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:53:24.935298 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.935531 [debug] [Thread-2 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.935677 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.938490 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.939816 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.940058 [debug] [Thread-1 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.942389 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.943803 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.944124 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.944342 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.946450 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.946656 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:53:24.946831 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.947044 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.947213 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.948263 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.948447 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:53:24.948837 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.949001 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.949234 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.949391 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:53:24.949634 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.949809 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.949950 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.950095 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.950274 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.950437 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:53:24.950636 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.950881 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.951037 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.951220 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.951394 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.951699 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.951870 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.952095 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.952267 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.954094 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.954401 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:53:24.955749 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:53:24.956546 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:53:24.956785 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:53:24.956975 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m14:53:24.957413 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:53:24.957804 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:53:24.958037 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:53:24.958694 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:53:24.958857 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:53:24.959465 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:53:24.959784 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:53:24.960266 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:53:24.960522 [info ] [Thread-1 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.03s]
[0m14:53:24.960987 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:53:24.961210 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.961375 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:53:24.961604 [debug] [Thread-1 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.961755 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:53:24.961934 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.962198 [info ] [Thread-3 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.962452 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:53:24.962974 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.962726 [info ] [Thread-2 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.03s]
[0m14:53:24.963222 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:53:24.963507 [debug] [Thread-2 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.963676 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.966177 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.966909 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.968199 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.968728 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.968888 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:53:24.969031 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.969392 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:53:24.969533 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.969679 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.971196 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.972031 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:53:24.972557 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:53:24.972785 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:53:24.973165 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:53:24.973468 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.974186 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.974356 [debug] [MainThread]: On master: BEGIN
[0m14:53:24.974486 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:53:24.974906 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.975103 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.975304 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.975431 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.975672 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.975804 [debug] [MainThread]: On master: Close
[0m14:53:24.975986 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:24.976102 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:53:24.976208 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:53:24.976311 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:53:24.976410 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:53:24.976548 [info ] [MainThread]: 
[0m14:53:24.976693 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m14:53:24.977761 [debug] [MainThread]: Command end result
[0m14:53:24.990606 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:24.991914 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:24.995508 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:53:24.995707 [info ] [MainThread]: 
[0m14:53:24.995880 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 1 warning:[0m
[0m14:53:24.996026 [info ] [MainThread]: 
[0m14:53:24.996209 [error] [MainThread]: [31mFailure in test accepted_values_stg_subscriptions_subscription_status__active__cancelled (models/staging/staging.yml)[0m
[0m14:53:24.996375 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:53:24.996494 [info ] [MainThread]: 
[0m14:53:24.996646 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/accepted_values_stg_subscripti_ed26b36fb4559cc7cdc0f9175b0b5654.sql
[0m14:53:24.996774 [info ] [MainThread]: 
[0m14:53:24.996924 [warn ] [MainThread]: [33mWarning in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:53:24.997076 [warn ] [MainThread]: Got 8000 results, configured to warn if != 0
[0m14:53:24.997196 [info ] [MainThread]: 
[0m14:53:24.997338 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:53:24.997462 [info ] [MainThread]: 
[0m14:53:24.997605 [info ] [MainThread]: Done. PASS=15 WARN=1 ERROR=1 SKIP=0 NO-OP=0 TOTAL=17
[0m14:53:24.997861 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:53:24.999304 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.90952873, "process_in_blocks": "0", "process_kernel_time": 0.205225, "process_mem_max_rss": "161267712", "process_out_blocks": "0", "process_user_time": 1.387335}
[0m14:53:24.999570 [debug] [MainThread]: Command `dbt test` failed at 14:53:24.999522 after 0.91 seconds
[0m14:53:24.999775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d9a0>]}
[0m14:53:24.999958 [debug] [MainThread]: Flushing usage events
[0m14:53:25.128046 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:43.580935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104915280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104003500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d700>]}


============================== 14:53:43.584032 | 2cde45ef-7bdf-4f12-98b5-6db7ff4d983e ==============================
[0m14:53:43.584032 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:53:43.584374 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'debug': 'False', 'invocation_command': 'dbt test --select staging', 'introspect': 'True', 'target_path': 'None', 'use_colors': 'True', 'empty': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'printer_width': '80', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None'}
[0m14:53:43.752542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102347e60>]}
[0m14:53:43.782425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dcdee0>]}
[0m14:53:43.784101 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:53:43.839910 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:53:43.907442 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:53:43.907989 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:53:44.077718 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on
'stg_subscriptions' in package 'saas_analytics' (models/staging/staging.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m14:53:44.078044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105531010>]}
[0m14:53:44.116305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c53da0>]}
[0m14:53:44.190277 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:44.192132 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:44.210314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054cd430>]}
[0m14:53:44.210621 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:53:44.210800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b27e0>]}
[0m14:53:44.212168 [info ] [MainThread]: 
[0m14:53:44.212396 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:53:44.212545 [info ] [MainThread]: 
[0m14:53:44.212788 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:53:44.215021 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:53:44.241676 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:44.241911 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:53:44.242052 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:44.253901 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m14:53:44.254114 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:44.254263 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:53:44.271170 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:53:44.272242 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:53:44.273238 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:53:44.273409 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:53:44.274273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a01a0>]}
[0m14:53:44.274521 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.274650 [debug] [MainThread]: On master: BEGIN
[0m14:53:44.274771 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:44.275032 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.275164 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.275289 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.275403 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.275587 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.275713 [debug] [MainThread]: On master: Close
[0m14:53:44.277257 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.277444 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.277759 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.277915 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.277625 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__canceled  [RUN]
[0m14:53:44.278106 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:53:44.278298 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:53:44.278488 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:53:44.278694 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140)
[0m14:53:44.278910 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:44.279100 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:44.279284 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:44.279438 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.279582 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.279724 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.279861 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.284192 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.289120 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.294836 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.296652 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.297194 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.303431 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.305650 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.305879 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.307088 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.307284 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.308441 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.309568 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.309954 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.310178 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.310386 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: BEGIN
[0m14:53:44.310564 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:53:44.310738 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.310953 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.311113 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.311256 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:53:44.311412 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:53:44.311556 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:53:44.311911 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:53:44.312080 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:53:44.312248 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312389 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312645 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.312812 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312963 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.313105 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.313273 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','canceled'
)



  
  
      
    ) dbt_internal_test
[0m14:53:44.313432 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.313583 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.313731 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.313974 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.314212 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.315457 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.317710 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:53:44.317947 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.318110 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.318534 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:44.318678 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m14:53:44.319372 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:53:44.319981 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:53:44.320133 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:53:44.320633 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:44.321285 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: ROLLBACK
[0m14:53:44.321941 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:44.322112 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:53:44.321562 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:53:44.322632 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140'
[0m14:53:44.322789 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:53:44.323239 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.323037 [warn ] [Thread-2 (]: 2 of 17 WARN 8000 not_null_stg_events_event_at ................................. [[33mWARN 8000[0m in 0.04s]
[0m14:53:44.323442 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: Close
[0m14:53:44.323818 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.323668 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.04s]
[0m14:53:44.324097 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.324330 [info ] [Thread-1 (]: 1 of 17 PASS accepted_values_stg_subscriptions_subscription_status__active__canceled  [[32mPASS[0m in 0.05s]
[0m14:53:44.324679 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.324461 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:53:44.324878 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.325104 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.325262 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.325437 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:53:44.325739 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.325590 [info ] [Thread-2 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:53:44.326076 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.325944 [info ] [Thread-4 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:53:44.326268 [info ] [Thread-1 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:53:44.326439 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:53:44.328580 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.328823 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:53:44.329045 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:53:44.329218 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.329422 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.329593 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.331681 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.333599 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.333834 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.335790 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.337963 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.338280 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.338477 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.339611 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.340914 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.341179 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.341495 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.342777 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.342995 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:53:44.343255 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.343446 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.343770 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.343951 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.344100 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:53:44.344263 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.344433 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:53:44.344591 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:53:44.344741 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.344890 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.345034 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.345177 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.345404 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.345615 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.345817 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.346056 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.346205 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.346365 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.346537 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.346715 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.346880 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.347035 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.347289 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.348501 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:53:44.348991 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:53:44.349145 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:53:44.349314 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349478 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349639 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349898 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.350649 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:53:44.351306 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:53:44.351886 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:53:44.352132 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.352597 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:53:44.353006 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:53:44.353416 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:53:44.353590 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.353779 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:53:44.353944 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:53:44.354098 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:53:44.354269 [info ] [Thread-3 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:53:44.354567 [info ] [Thread-4 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.03s]
[0m14:53:44.354781 [info ] [Thread-1 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.03s]
[0m14:53:44.355277 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:53:44.355115 [info ] [Thread-2 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.03s]
[0m14:53:44.355543 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.355771 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.355938 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.356154 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.356325 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.356523 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.359071 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.359353 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.359531 [info ] [Thread-4 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:53:44.359716 [info ] [Thread-1 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:53:44.359991 [info ] [Thread-2 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:53:44.360245 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:53:44.360479 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:53:44.360638 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.360797 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:53:44.360949 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.361093 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.362275 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.362429 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.364195 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.365887 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.367844 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.368223 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.368397 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:53:44.368566 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.368906 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.370225 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.370420 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.370583 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.370736 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.370918 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.373054 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.374505 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.374762 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.374995 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.375385 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:53:44.375590 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.375858 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.376044 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.376200 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.376346 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:53:44.376494 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.376645 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:53:44.376796 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.376934 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.377115 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.377831 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:53:44.378008 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.378614 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:53:44.378769 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.378995 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.379165 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:53:44.379352 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.379517 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.379668 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.380082 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.379940 [info ] [Thread-3 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.02s]
[0m14:53:44.380300 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.380964 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:53:44.381257 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.381565 [debug] [Thread-3 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.381994 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:53:44.382200 [info ] [Thread-3 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:53:44.382384 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.382528 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.382671 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:53:44.382849 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:53:44.383478 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:53:44.384195 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:53:44.384567 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.384427 [info ] [Thread-2 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.385045 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:53:44.385472 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:53:44.388030 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.388295 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.388457 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:53:44.388604 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:53:44.388836 [debug] [Thread-2 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.389224 [info ] [Thread-1 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.03s]
[0m14:53:44.389475 [info ] [Thread-4 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.03s]
[0m14:53:44.389688 [info ] [Thread-2 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:53:44.389963 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.390343 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.390569 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:53:44.390787 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.390950 [debug] [Thread-3 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.391131 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.391315 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.391478 [info ] [Thread-1 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:53:44.392795 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.392963 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:53:44.395506 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.395753 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:53:44.396001 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:53:44.396225 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.396404 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.399359 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.401121 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.401322 [debug] [Thread-2 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.401527 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.402751 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.402926 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:53:44.403162 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.403434 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.403604 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.404732 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.405857 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.406052 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.406226 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m14:53:44.406475 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:53:44.406650 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.406928 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.407125 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.407365 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.407545 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.407929 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:53:44.408088 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.408240 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:53:44.408413 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.408565 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.408717 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.408962 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.409183 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.409424 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.409601 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.409784 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.409954 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.410185 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.412123 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.413310 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:53:44.413558 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.414060 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:53:44.414254 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.414414 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m14:53:44.414607 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:53:44.415347 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:53:44.416044 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:53:44.416671 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:53:44.416939 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:53:44.417435 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:53:44.417885 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:53:44.418286 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:53:44.418516 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.418684 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:53:44.418848 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:53:44.418999 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:53:44.419174 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.419459 [info ] [Thread-2 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.03s]
[0m14:53:44.419656 [info ] [Thread-1 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.419949 [info ] [Thread-3 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.04s]
[0m14:53:44.420130 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:53:44.420365 [debug] [Thread-2 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.420580 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.420785 [debug] [Thread-3 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.420953 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:53:44.421214 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.423519 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.424028 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.426267 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.426668 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.426826 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:53:44.426964 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.427356 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:53:44.427528 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.427681 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.429288 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.430197 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:53:44.430742 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:53:44.430935 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:53:44.431249 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:53:44.431514 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.432187 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.432358 [debug] [MainThread]: On master: BEGIN
[0m14:53:44.432487 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:53:44.432813 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.432949 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.433076 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.433197 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.433400 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.433529 [debug] [MainThread]: On master: Close
[0m14:53:44.433696 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:44.433815 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:53:44.433923 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:53:44.434033 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:53:44.434137 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:53:44.434285 [info ] [MainThread]: 
[0m14:53:44.434425 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:53:44.435459 [debug] [MainThread]: Command end result
[0m14:53:44.448409 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:44.449581 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:44.453093 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:53:44.453285 [info ] [MainThread]: 
[0m14:53:44.453473 [info ] [MainThread]: [33mCompleted with 1 warning:[0m
[0m14:53:44.453620 [info ] [MainThread]: 
[0m14:53:44.453798 [warn ] [MainThread]: [33mWarning in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:53:44.453961 [warn ] [MainThread]: Got 8000 results, configured to warn if != 0
[0m14:53:44.454086 [info ] [MainThread]: 
[0m14:53:44.454235 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:53:44.454369 [info ] [MainThread]: 
[0m14:53:44.454514 [info ] [MainThread]: Done. PASS=16 WARN=1 ERROR=0 SKIP=0 NO-OP=0 TOTAL=17
[0m14:53:44.454783 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:53:44.456274 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.9109618, "process_in_blocks": "0", "process_kernel_time": 0.205519, "process_mem_max_rss": "162791424", "process_out_blocks": "0", "process_user_time": 1.385866}
[0m14:53:44.456550 [debug] [MainThread]: Command `dbt test` succeeded at 14:53:44.456501 after 0.91 seconds
[0m14:53:44.456755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048dfe60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d7f0>]}
[0m14:53:44.456942 [debug] [MainThread]: Flushing usage events
[0m14:53:44.557202 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:48:55.068522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106810a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a9640>]}


============================== 15:48:55.076073 | 3f942e24-110a-4e97-b1ae-cda05f7fc5e9 ==============================
[0m15:48:55.076073 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:48:55.076415 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'static_parser': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'partial_parse': 'True', 'empty': 'False', 'log_cache_events': 'False', 'target_path': 'None', 'introspect': 'True', 'printer_width': '80', 'no_print': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'use_colors': 'True', 'warn_error': 'None', 'quiet': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select marts', 'write_json': 'True'}
[0m15:48:55.237961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093d5550>]}
[0m15:48:55.266494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c68aa0>]}
[0m15:48:55.267926 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:48:55.320730 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:48:55.389240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m15:48:55.389583 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/dim_users.sql
[0m15:48:55.389746 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_events.sql
[0m15:48:55.389954 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/marts.yml
[0m15:48:55.599257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099afbf0>]}
[0m15:48:55.673790 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:48:55.675805 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:48:55.689512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b48e30>]}
[0m15:48:55.689791 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:48:55.689961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bbcb90>]}
[0m15:48:55.690923 [info ] [MainThread]: 
[0m15:48:55.691089 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:48:55.691221 [info ] [MainThread]: 
[0m15:48:55.691449 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:48:55.693523 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:48:55.723101 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:48:55.723344 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:48:55.724251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:48:55.741120 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:48:55.742089 [debug] [ThreadPool]: On list_analytics: Close
[0m15:48:55.742451 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:48:55.742688 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:48:55.746618 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.746790 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:48:55.746924 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:48:55.747793 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:48:55.748419 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.748556 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:48:55.748775 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.748902 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.749022 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:48:55.749480 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.749850 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:48:55.749979 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.750094 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:48:55.750287 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.750412 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:48:55.750992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:48:55.753651 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:48:55.753870 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:48:55.754021 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:48:55.754460 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.754597 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:48:55.754733 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:48:55.765396 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:48:55.766210 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:48:55.767210 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:48:55.767352 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:48:55.767972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ae9130>]}
[0m15:48:55.768238 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.768372 [debug] [MainThread]: On master: BEGIN
[0m15:48:55.768496 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:48:55.768744 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.768869 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.768991 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.769109 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.769290 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.769412 [debug] [MainThread]: On master: Close
[0m15:48:55.770865 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:48:55.771088 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:48:55.771342 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:48:55.771590 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:48:55.771832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:48:55.772051 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:48:55.772218 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:48:55.772371 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:48:55.776377 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:48:55.778027 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:48:55.778694 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:48:55.778875 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:48:55.795042 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:48:55.796655 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:48:55.797221 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:48:55.797389 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:48:55.797536 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:48:55.797843 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:48:55.798112 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:48:55.798285 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:48:55.798429 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:48:55.798602 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:48:55.798883 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:48:55.802426 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:48:55.802660 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:48:55.802819 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:48:55.803038 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:48:55.803194 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:48:55.803463 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
        u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:48:55.804014 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
        u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:48:55.804249 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:48:55.805066 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:48:55.809541 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:48:55.809720 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:48:55.810458 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:48:55.810652 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:48:55.812431 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:48:55.813848 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^
[0m15:48:55.814035 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ecf4a0>]}
[0m15:48:55.814246 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ef5f10>]}
[0m15:48:55.814565 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:48:55.815101 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:48:55.814884 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:48:55.815388 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^.
[0m15:48:55.815736 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:48:55.816273 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^.
[0m15:48:55.816899 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.817042 [debug] [MainThread]: On master: BEGIN
[0m15:48:55.817163 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:48:55.817497 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.817629 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.817753 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.817868 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.818076 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.818235 [debug] [MainThread]: On master: Close
[0m15:48:55.818526 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:48:55.818684 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:48:55.818814 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:48:55.818991 [info ] [MainThread]: 
[0m15:48:55.819146 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:48:55.819522 [debug] [MainThread]: Command end result
[0m15:48:55.839413 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:48:55.840520 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:48:55.844101 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:48:55.844281 [info ] [MainThread]: 
[0m15:48:55.844463 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:48:55.844600 [info ] [MainThread]: 
[0m15:48:55.844773 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:48:55.844930 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:48:55.845056 [info ] [MainThread]: 
[0m15:48:55.845198 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:48:55.845325 [info ] [MainThread]: 
[0m15:48:55.845472 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:48:55.845617 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^
[0m15:48:55.845730 [info ] [MainThread]: 
[0m15:48:55.845869 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:48:55.845985 [info ] [MainThread]: 
[0m15:48:55.846123 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:48:55.847171 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.817876, "process_in_blocks": "0", "process_kernel_time": 0.205791, "process_mem_max_rss": "149913600", "process_out_blocks": "0", "process_user_time": 1.270329}
[0m15:48:55.847386 [debug] [MainThread]: Command `dbt run` failed at 15:48:55.847344 after 0.82 seconds
[0m15:48:55.847562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106810a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10854cbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109165cd0>]}
[0m15:48:55.847734 [debug] [MainThread]: Flushing usage events
[0m15:48:55.981522 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:49:47.157057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ce2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b57c0>]}


============================== 15:49:47.163631 | d0b003f2-98bd-43ba-9a7e-61f863ea5223 ==============================
[0m15:49:47.163631 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:49:47.163974 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'empty': 'False', 'printer_width': '80', 'invocation_command': 'dbt run --select marts', 'warn_error': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False'}
[0m15:49:47.318618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090e4e90>]}
[0m15:49:47.346908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109557830>]}
[0m15:49:47.348280 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:49:47.399718 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:49:47.464483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:49:47.464906 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/dim_users.sql
[0m15:49:47.663716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1adf70>]}
[0m15:49:47.736966 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:49:47.738370 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:49:47.750321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbcafc0>]}
[0m15:49:47.750602 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:49:47.750779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d162840>]}
[0m15:49:47.751760 [info ] [MainThread]: 
[0m15:49:47.751938 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:49:47.752071 [info ] [MainThread]: 
[0m15:49:47.752300 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:49:47.754384 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:49:47.781139 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:49:47.781389 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:49:47.781537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:49:47.796252 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:49:47.797017 [debug] [ThreadPool]: On list_analytics: Close
[0m15:49:47.797366 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:49:47.797581 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:49:47.800831 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.801007 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:49:47.801139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:49:47.801824 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:49:47.802503 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.802655 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:49:47.802892 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.803024 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.803149 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:49:47.803548 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.803936 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:49:47.804070 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.804184 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:49:47.804379 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.804507 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:49:47.805130 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:49:47.807690 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:49:47.808634 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:49:47.808815 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:49:47.809104 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.809240 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:49:47.809385 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:49:47.819103 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:49:47.819861 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:49:47.820793 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:49:47.820956 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:49:47.821582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc22ed0>]}
[0m15:49:47.821794 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.821918 [debug] [MainThread]: On master: BEGIN
[0m15:49:47.822034 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:49:47.822280 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.822407 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.822524 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.822638 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.822841 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.822962 [debug] [MainThread]: On master: Close
[0m15:49:47.824206 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:49:47.824393 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:49:47.824631 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:49:47.824873 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:49:47.825094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:49:47.825313 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:49:47.825475 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:49:47.825627 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:49:47.829585 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:49:47.831320 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:49:47.831834 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:49:47.844615 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:49:47.847868 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:49:47.849480 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:49:47.849993 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:49:47.850187 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:49:47.850346 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:49:47.850504 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:49:47.850656 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:49:47.850801 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:49:47.851202 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:49:47.851423 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:49:47.851573 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:49:47.851730 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:49:47.851948 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:49:47.852264 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:49:47.854772 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:49:47.855238 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:49:47.855438 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:49:47.856777 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:49:47.857043 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:49:47.857251 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:49:47.861436 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:49:47.861630 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:49:47.862322 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:49:47.862992 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:49:47.864157 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:49:47.865468 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:49:47.865841 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10899e870>]}
[0m15:49:47.866005 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4f6480>]}
[0m15:49:47.866295 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:49:47.866801 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:49:47.866533 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:49:47.867073 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:49:47.867306 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:49:47.867828 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^.
[0m15:49:47.868406 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.868554 [debug] [MainThread]: On master: BEGIN
[0m15:49:47.868681 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:49:47.868997 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.869128 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.869249 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.869365 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.869572 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.869697 [debug] [MainThread]: On master: Close
[0m15:49:47.869855 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:49:47.869973 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:49:47.870081 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:49:47.870223 [info ] [MainThread]: 
[0m15:49:47.870355 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m15:49:47.870702 [debug] [MainThread]: Command end result
[0m15:49:47.884824 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:49:47.885880 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:49:47.888592 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:49:47.888753 [info ] [MainThread]: 
[0m15:49:47.888930 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:49:47.889072 [info ] [MainThread]: 
[0m15:49:47.889243 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:49:47.889407 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:49:47.889532 [info ] [MainThread]: 
[0m15:49:47.889678 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:49:47.889808 [info ] [MainThread]: 
[0m15:49:47.889953 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:49:47.890098 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:49:47.890218 [info ] [MainThread]: 
[0m15:49:47.890357 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:49:47.890472 [info ] [MainThread]: 
[0m15:49:47.890605 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:49:47.891982 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.77687764, "process_in_blocks": "0", "process_kernel_time": 0.182145, "process_mem_max_rss": "150044672", "process_out_blocks": "0", "process_user_time": 1.240899}
[0m15:49:47.892185 [debug] [MainThread]: Command `dbt run` failed at 15:49:47.892149 after 0.78 seconds
[0m15:49:47.892357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089d05c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089d14f0>]}
[0m15:49:47.892521 [debug] [MainThread]: Flushing usage events
[0m15:49:47.991649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:50:05.112324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103f77d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5520>]}


============================== 15:50:05.116283 | 2a9d5b37-6906-4a77-ba46-59cb7c9a9107 ==============================
[0m15:50:05.116283 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:50:05.116621 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'quiet': 'False', 'debug': 'False', 'log_format': 'default', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'write_json': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'no_print': 'None', 'warn_error': 'None', 'invocation_command': 'dbt run --select marts', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'printer_width': '80', 'partial_parse': 'True'}
[0m15:50:05.290337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f49880>]}
[0m15:50:05.318625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111370830>]}
[0m15:50:05.319968 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:50:05.379575 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:50:05.447209 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:50:05.447656 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/fct_events.sql
[0m15:50:05.640187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fc6db0>]}
[0m15:50:05.720040 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:50:05.721941 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:50:05.737235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c060f0>]}
[0m15:50:05.737521 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:50:05.737699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128d2ea0>]}
[0m15:50:05.738619 [info ] [MainThread]: 
[0m15:50:05.738801 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:50:05.738937 [info ] [MainThread]: 
[0m15:50:05.739162 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:50:05.741266 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:50:05.770275 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:50:05.770508 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:50:05.770658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:05.788097 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:50:05.788872 [debug] [ThreadPool]: On list_analytics: Close
[0m15:50:05.789228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:50:05.789485 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:50:05.792676 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.792845 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:50:05.792977 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:50:05.793632 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:50:05.794245 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.794376 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:50:05.794597 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.794717 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.794908 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:50:05.795219 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.795598 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:50:05.795726 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.795837 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:50:05.796034 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.796160 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:50:05.796734 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:50:05.799273 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:50:05.799422 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:50:05.799595 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:50:05.799837 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.799966 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:50:05.800097 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:50:05.810819 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:50:05.811561 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:50:05.812544 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:50:05.812707 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:50:05.813536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128d27e0>]}
[0m15:50:05.813763 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.813894 [debug] [MainThread]: On master: BEGIN
[0m15:50:05.814012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:50:05.814260 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.814387 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.814510 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.814627 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.814820 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.814941 [debug] [MainThread]: On master: Close
[0m15:50:05.816294 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:50:05.816470 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:50:05.816702 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:50:05.817111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:50:05.816904 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:50:05.817313 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:50:05.817520 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:50:05.822525 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:50:05.822776 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:50:05.824726 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:50:05.825343 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:50:05.825556 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:50:05.842563 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:50:05.843355 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:50:05.843989 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:50:05.844170 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:50:05.844316 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:50:05.844609 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:50:05.844776 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:50:05.844916 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:50:05.845169 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:50:05.845315 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:50:05.845551 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:50:05.846092 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:50:05.846247 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:50:05.846428 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:50:05.849929 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:50:05.850286 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:50:05.850533 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:50:05.850698 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:50:05.850926 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:50:05.851123 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:50:05.856411 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:50:05.857122 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:50:05.857290 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:50:05.857468 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:50:05.859171 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:50:05.860120 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^
[0m15:50:05.861216 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112803500>]}
[0m15:50:05.861378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b274d0>]}
[0m15:50:05.861689 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:50:05.862191 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:50:05.861928 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:50:05.862466 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^.
[0m15:50:05.862695 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:50:05.863207 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:50:05.863793 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.863938 [debug] [MainThread]: On master: BEGIN
[0m15:50:05.864057 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:50:05.864341 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.864469 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.864592 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.864708 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.864895 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.865019 [debug] [MainThread]: On master: Close
[0m15:50:05.865178 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:50:05.865287 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:50:05.865389 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:50:05.865529 [info ] [MainThread]: 
[0m15:50:05.865664 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:50:05.866006 [debug] [MainThread]: Command end result
[0m15:50:05.880415 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:50:05.881595 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:50:05.884364 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:50:05.884533 [info ] [MainThread]: 
[0m15:50:05.884712 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:50:05.884859 [info ] [MainThread]: 
[0m15:50:05.885045 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:50:05.885209 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^
[0m15:50:05.885335 [info ] [MainThread]: 
[0m15:50:05.885483 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:50:05.885607 [info ] [MainThread]: 
[0m15:50:05.885751 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:50:05.885903 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:50:05.886025 [info ] [MainThread]: 
[0m15:50:05.886163 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:50:05.886280 [info ] [MainThread]: 
[0m15:50:05.886420 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:50:05.887777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8143798, "process_in_blocks": "0", "process_kernel_time": 0.198754, "process_mem_max_rss": "150077440", "process_out_blocks": "0", "process_user_time": 1.238241}
[0m15:50:05.888016 [debug] [MainThread]: Command `dbt run` failed at 15:50:05.887973 after 0.81 seconds
[0m15:50:05.888203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ee08f0>]}
[0m15:50:05.888373 [debug] [MainThread]: Flushing usage events
[0m15:50:05.985365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:51:52.959202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a0ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058759d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105875760>]}


============================== 15:51:52.966284 | b5001dfd-e186-44be-8218-e9a93fa37978 ==============================
[0m15:51:52.966284 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:51:52.966608 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'write_json': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'version_check': 'True', 'no_print': 'None', 'quiet': 'False', 'debug': 'False', 'log_format': 'default', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select marts', 'target_path': 'None', 'use_colors': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'indirect_selection': 'eager'}
[0m15:51:53.133528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105678b30>]}
[0m15:51:53.162450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105233e30>]}
[0m15:51:53.163867 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:51:53.215460 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:51:53.282808 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:53.283215 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/fct_events.sql
[0m15:51:53.476855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616f8f0>]}
[0m15:51:53.552299 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:51:53.553536 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:51:53.566788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106068800>]}
[0m15:51:53.567078 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:51:53.567262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d63140>]}
[0m15:51:53.568198 [info ] [MainThread]: 
[0m15:51:53.568457 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:51:53.568635 [info ] [MainThread]: 
[0m15:51:53.568885 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:51:53.571033 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:51:53.600525 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:51:53.600764 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:51:53.600994 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:53.618958 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:51:53.619947 [debug] [ThreadPool]: On list_analytics: Close
[0m15:51:53.620295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:51:53.620515 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:51:53.623725 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.623894 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:51:53.624024 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:51:53.624763 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:51:53.625379 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.625515 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:51:53.625852 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.625976 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.626100 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:51:53.626377 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.626834 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:51:53.626993 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.627124 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:51:53.627341 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.627478 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:51:53.628086 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:51:53.630731 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:51:53.630897 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:51:53.631081 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:51:53.631327 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.631459 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:51:53.631590 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:51:53.642325 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:51:53.643048 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:51:53.644026 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:51:53.644185 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:51:53.645035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e8fef0>]}
[0m15:51:53.645265 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.645391 [debug] [MainThread]: On master: BEGIN
[0m15:51:53.645505 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:51:53.645751 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.645875 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.645994 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.646109 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.646292 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.646417 [debug] [MainThread]: On master: Close
[0m15:51:53.647722 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:51:53.647913 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:51:53.648146 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:51:53.648549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:51:53.648340 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:51:53.648751 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:51:53.648958 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:51:53.653750 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:51:53.653957 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:51:53.655835 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:51:53.656543 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:51:53.656759 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:51:53.672267 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:51:53.673820 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:51:53.674538 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.674706 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:51:53.674853 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:51:53.675200 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:51:53.675384 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:51:53.675536 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:51:53.675785 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.675968 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.676177 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquisition_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:51:53.676551 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.676694 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:51:53.676923 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:51:53.682012 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:51:53.682310 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:51:53.682545 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:51:53.687723 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:51:53.687970 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:51:53.689347 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:51:53.690429 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063eaba0>]}
[0m15:51:53.690781 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:51:53.691117 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:51:53.691368 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:51:53.774349 [debug] [Thread-2 (]: SQL status: OK in 0.098 seconds
[0m15:51:53.778205 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.778562 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events__dbt_tmp" rename to "fct_events"
[0m15:51:53.779462 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.787641 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:51:53.787861 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.788023 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:51:53.795183 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:51:53.798014 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.798196 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

      drop table if exists "analytics"."main"."fct_events__dbt_backup" cascade
    
[0m15:51:53.798865 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.799972 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:51:53.800257 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102f1ee40>]}
[0m15:51:53.800551 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.fct_events .............................. [[32mOK[0m in 0.15s]
[0m15:51:53.800801 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:51:53.801440 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.801607 [debug] [MainThread]: On master: BEGIN
[0m15:51:53.801737 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:51:53.802000 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.802130 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.802255 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.802373 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.802572 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.802697 [debug] [MainThread]: On master: Close
[0m15:51:53.802859 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:51:53.802987 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:51:53.803099 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:51:53.803255 [info ] [MainThread]: 
[0m15:51:53.803395 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:51:53.803745 [debug] [MainThread]: Command end result
[0m15:51:53.819054 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:51:53.820188 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:51:53.823331 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:51:53.823535 [info ] [MainThread]: 
[0m15:51:53.823723 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:51:53.823875 [info ] [MainThread]: 
[0m15:51:53.824059 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:51:53.824235 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:51:53.824368 [info ] [MainThread]: 
[0m15:51:53.824519 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:51:53.824655 [info ] [MainThread]: 
[0m15:51:53.824803 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:51:53.826339 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9080825, "process_in_blocks": "0", "process_kernel_time": 0.203356, "process_mem_max_rss": "174784512", "process_out_blocks": "0", "process_user_time": 1.442706}
[0m15:51:53.826589 [debug] [MainThread]: Command `dbt run` failed at 15:51:53.826546 after 0.91 seconds
[0m15:51:53.826782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053eb140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105731e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b56cf0>]}
[0m15:51:53.826958 [debug] [MainThread]: Flushing usage events
[0m15:51:53.945795 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:53:00.973497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107327500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5910>]}


============================== 15:53:00.981338 | 93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e ==============================
[0m15:53:00.981338 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:53:00.981667 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select marts', 'indirect_selection': 'eager', 'use_colors': 'True', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'partial_parse': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'empty': 'False', 'version_check': 'True', 'introspect': 'True', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'debug': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'send_anonymous_usage_stats': 'True'}
[0m15:53:01.148225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e52000>]}
[0m15:53:01.176852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077b0b60>]}
[0m15:53:01.178165 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:53:01.230092 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:53:01.297035 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:53:01.297448 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/dim_users.sql
[0m15:53:01.495509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11286e870>]}
[0m15:53:01.571767 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:53:01.573535 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:53:01.589769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a95b0>]}
[0m15:53:01.590069 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:53:01.590245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bafa70>]}
[0m15:53:01.591151 [info ] [MainThread]: 
[0m15:53:01.591316 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:53:01.591448 [info ] [MainThread]: 
[0m15:53:01.591675 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:53:01.593774 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:53:01.622185 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:53:01.622430 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:53:01.622575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:01.639161 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:53:01.640132 [debug] [ThreadPool]: On list_analytics: Close
[0m15:53:01.640481 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:53:01.640725 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:53:01.643958 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.644125 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:53:01.644259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:53:01.645045 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:53:01.645637 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.645772 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:53:01.645986 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.646111 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.646234 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:53:01.646694 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.647055 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:53:01.647184 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.647296 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:53:01.647486 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.647611 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:53:01.648339 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:53:01.651049 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:53:01.651957 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:53:01.652103 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:53:01.652423 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.652556 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:53:01.652689 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:53:01.666459 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:53:01.667774 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:53:01.668814 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:53:01.668965 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:53:01.669677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e3b110>]}
[0m15:53:01.669933 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.670070 [debug] [MainThread]: On master: BEGIN
[0m15:53:01.670190 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:53:01.670445 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.670575 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.670699 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.670811 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.671000 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.671126 [debug] [MainThread]: On master: Close
[0m15:53:01.672596 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:53:01.672773 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:53:01.673013 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:53:01.673429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:53:01.673211 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:53:01.673637 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:53:01.673846 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:53:01.678047 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:53:01.678246 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:53:01.679932 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:53:01.680639 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:53:01.693430 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:53:01.696892 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:53:01.698607 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:53:01.699263 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.699438 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:53:01.699619 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.699797 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:53:01.699988 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:53:01.700297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:53:01.700526 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.700688 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.700842 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.701040 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquisition_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:53:01.701232 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.701636 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (

    select * from "analytics"."main"."stg_users"

),

events as (

    select * from "analytics"."main"."stg_events"

),

subscriptions as (

    select * from "analytics"."main"."stg_subscriptions"

),

-- Aggregate event activity per user
user_events as (

    select
        events.user_id,
        min(events.event_at) as first_event_at,
        max(events.event_at) as last_event_at,
        count(*) as total_events,
        count(distinct events.event_name) as unique_event_types

    from events
    where events.event_at is not null  -- exclude events with null timestamps
    group by events.user_id

),

-- Get current subscription (most recent active or most recent ended)
user_current_subscription as (

    select
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed

    from (
        select
            *,
            row_number() over (
                partition by user_id 
                order by 
                    is_active desc,  -- active subscriptions first
                    subscription_started_at desc  -- then most recent
            ) as rn
        from subscriptions
    )
    where rn = 1

),

-- Calculate lifetime subscription metrics
user_subscription_metrics as (

    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns

    from subscriptions
    group by user_id

),

-- Bring it all together
final as (

    select
        -- user identity
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,

        -- event activity
        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        -- current subscription
        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        -- lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,

        -- derived: days since signup
        datediff('day', u.signed_up_at, current_date) as days_since_signup,

        -- derived: recency (days since last event)
        case
            when e.last_event_at is null then null
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event

    from users u
    left join user_events e
        on u.user_id = e.user_id
    left join user_current_subscription cs
        on u.user_id = cs.user_id
    left join user_subscription_metrics sm
        on u.user_id = sm.user_id

)

select * from final
    );
  
  
[0m15:53:01.725036 [debug] [Thread-1 (]: SQL status: OK in 0.023 seconds
[0m15:53:01.728827 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.729060 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */
alter table "analytics"."main"."dim_users__dbt_tmp" rename to "dim_users"
[0m15:53:01.734296 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:53:01.741855 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: COMMIT
[0m15:53:01.742228 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.742412 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: COMMIT
[0m15:53:01.746510 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:53:01.751648 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.751947 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

      drop table if exists "analytics"."main"."dim_users__dbt_backup" cascade
    
[0m15:53:01.752748 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.754046 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:53:01.755196 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112baf5c0>]}
[0m15:53:01.755566 [info ] [Thread-1 (]: 1 of 2 OK created sql table model main.dim_users ............................... [[32mOK[0m in 0.08s]
[0m15:53:01.755911 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:53:01.811255 [debug] [Thread-2 (]: SQL status: OK in 0.107 seconds
[0m15:53:01.815248 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.815501 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_events'
  
[0m15:53:01.816062 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.816710 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.816873 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_events'
  
[0m15:53:01.818052 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.820646 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.820867 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events" rename to "fct_events__dbt_backup"
[0m15:53:01.821300 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.822885 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.823071 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events__dbt_tmp" rename to "fct_events"
[0m15:53:01.823437 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.825439 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:53:01.825656 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.825822 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:53:01.838610 [debug] [Thread-2 (]: SQL status: OK in 0.013 seconds
[0m15:53:01.840503 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.840770 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

      drop table if exists "analytics"."main"."fct_events__dbt_backup" cascade
    
[0m15:53:01.842653 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:53:01.843603 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:53:01.843937 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ef67b0>]}
[0m15:53:01.844291 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.fct_events .............................. [[32mOK[0m in 0.17s]
[0m15:53:01.844622 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:53:01.845385 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.845564 [debug] [MainThread]: On master: BEGIN
[0m15:53:01.845699 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:53:01.846055 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.846193 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.846324 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.846445 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.846660 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.846791 [debug] [MainThread]: On master: Close
[0m15:53:01.846975 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:53:01.847094 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:53:01.847207 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:53:01.847384 [info ] [MainThread]: 
[0m15:53:01.847569 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m15:53:01.847966 [debug] [MainThread]: Command end result
[0m15:53:01.864131 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:53:01.865412 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:53:01.868395 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:53:01.868565 [info ] [MainThread]: 
[0m15:53:01.868760 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:53:01.868974 [info ] [MainThread]: 
[0m15:53:01.869147 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m15:53:01.870540 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9370321, "process_in_blocks": "0", "process_kernel_time": 0.212514, "process_mem_max_rss": "200802304", "process_out_blocks": "0", "process_user_time": 1.467614}
[0m15:53:01.870761 [debug] [MainThread]: Command `dbt run` succeeded at 15:53:01.870720 after 0.94 seconds
[0m15:53:01.870942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107327500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107696180>]}
[0m15:53:01.871109 [debug] [MainThread]: Flushing usage events
[0m15:53:01.969783 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:17:12.151069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10430b530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050618b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105061640>]}


============================== 14:17:12.163623 | 1a8631c7-db73-4902-9451-ec73ef810e99 ==============================
[0m14:17:12.163623 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:17:12.163961 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select experiments', 'introspect': 'True', 'quiet': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'use_colors': 'True', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'log_format': 'default', 'write_json': 'True'}
[0m14:17:12.332657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a8631c7-db73-4902-9451-ec73ef810e99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105060e90>]}
[0m14:17:12.362573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a8631c7-db73-4902-9451-ec73ef810e99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10518c470>]}
[0m14:17:12.363938 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:17:12.417182 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:17:12.475148 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: marts/experiments.yml - Runtime Error
    Syntax error near line 44
    ------------------------------
    41 |       - name: assigned_at_signup
    42 |         description: True if the user was assigned to the experiment on their signup day.
    43 | 
    44 |     - name: fct_experiment_conversions
    45 |       description: >
    46 |         One row per user-conversion_event pair, showing when (and if) a user converted
    47 |         on a key metric after being assigned to an experiment variant. Only includes
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 4, column 5
    did not find expected key
      in "<unicode string>", line 44, column 5
[0m14:17:12.481796 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3722872, "process_in_blocks": "0", "process_kernel_time": 0.197851, "process_mem_max_rss": "132775936", "process_out_blocks": "0", "process_user_time": 0.896562}
[0m14:17:12.482142 [debug] [MainThread]: Command `dbt run` failed at 14:17:12.482066 after 0.37 seconds
[0m14:17:12.482398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105061700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052519a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052ebd40>]}
[0m14:17:12.482616 [debug] [MainThread]: Flushing usage events
[0m14:17:12.650255 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:17:41.234370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cbb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331850>]}


============================== 14:17:41.238478 | 5a23bb44-6884-462a-8a23-651fdfc4240a ==============================
[0m14:17:41.238478 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:17:41.238848 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'indirect_selection': 'eager', 'log_format': 'default', 'invocation_command': 'dbt run --select experiments', 'fail_fast': 'False', 'empty': 'False', 'static_parser': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'warn_error': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'target_path': 'None', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:17:41.408643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a23bb44-6884-462a-8a23-651fdfc4240a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092ca2d0>]}
[0m14:17:41.437829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a23bb44-6884-462a-8a23-651fdfc4240a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c73bc0>]}
[0m14:17:41.439281 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:17:41.501940 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:17:41.573858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:17:41.574250 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_assignments.sql
[0m14:17:41.574423 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:17:41.574640 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:17:41.574776 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:17:41.768484 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.fct_experiment_conversions' (models/marts/fct_experiment_conversions.sql) depends on a node named 'fct_experiments_assignments' which was not found
[0m14:17:41.770196 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5788519, "process_in_blocks": "0", "process_kernel_time": 0.172149, "process_mem_max_rss": "137019392", "process_out_blocks": "0", "process_user_time": 1.060322}
[0m14:17:41.770471 [debug] [MainThread]: Command `dbt run` failed at 14:17:41.770421 after 0.58 seconds
[0m14:17:41.770683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b56f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a0c560>]}
[0m14:17:41.770867 [debug] [MainThread]: Flushing usage events
[0m14:17:41.882810 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:23.465706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108249eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1910>]}


============================== 14:19:23.476739 | 11a35a8c-2080-4508-af78-0ee2a677d63c ==============================
[0m14:19:23.476739 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:19:23.477148 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None', 'indirect_selection': 'eager', 'empty': 'False', 'invocation_command': 'dbt run --select experiments', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'printer_width': '80', 'introspect': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'no_print': 'None'}
[0m14:19:23.643549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11a35a8c-2080-4508-af78-0ee2a677d63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10903e3f0>]}
[0m14:19:23.672144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11a35a8c-2080-4508-af78-0ee2a677d63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7b020>]}
[0m14:19:23.673451 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:19:23.724922 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:19:23.795157 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:19:23.795586 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:19:23.795824 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:19:23.795967 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:19:23.796101 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:19:23.978886 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.rpt_experiment_results' (models/marts/rpt_experiment_results.sql) depends on a node named 'fct_experiment_assignments' which was not found
[0m14:19:23.980760 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5573125, "process_in_blocks": "0", "process_kernel_time": 0.174344, "process_mem_max_rss": "138018816", "process_out_blocks": "0", "process_user_time": 1.039068}
[0m14:19:23.981015 [debug] [MainThread]: Command `dbt run` failed at 14:19:23.980966 after 0.56 seconds
[0m14:19:23.981214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10926af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a3c20>]}
[0m14:19:23.981393 [debug] [MainThread]: Flushing usage events
[0m14:19:24.114566 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:56.849075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caaff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053698e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105369670>]}


============================== 14:19:56.852623 | 282d0e2e-cd3e-44f0-9b96-91259616d698 ==============================
[0m14:19:56.852623 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:19:56.852942 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'introspect': 'True', 'version_check': 'True', 'target_path': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select experiments', 'profiles_dir': '/Users/hazeldonaldson/.dbt'}
[0m14:19:56.997600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105394fe0>]}
[0m14:19:57.026589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d24e90>]}
[0m14:19:57.028741 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:19:57.080456 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:19:57.146821 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:19:57.147223 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:19:57.147393 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:19:57.147534 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:19:57.147663 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:19:57.394372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc4cb0>]}
[0m14:19:57.441615 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:19:57.443840 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:19:57.460470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b4acf0>]}
[0m14:19:57.460780 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:19:57.460960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595c440>]}
[0m14:19:57.461363 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:19:57.462056 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:19:57.462318 [debug] [MainThread]: Command end result
[0m14:19:57.483119 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:19:57.484265 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:19:57.486600 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:19:57.488050 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.6735688, "process_in_blocks": "0", "process_kernel_time": 0.156827, "process_mem_max_rss": "139689984", "process_out_blocks": "0", "process_user_time": 1.169049}
[0m14:19:57.488302 [debug] [MainThread]: Command `dbt run` succeeded at 14:19:57.488255 after 0.67 seconds
[0m14:19:57.488489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105406fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104610140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105740050>]}
[0m14:19:57.488666 [debug] [MainThread]: Flushing usage events
[0m14:19:57.611441 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:20:15.294776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bf9b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc9760>]}


============================== 14:20:15.298703 | 4afa1aa9-613a-4d5f-a699-f3a280e364e1 ==============================
[0m14:20:15.298703 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:20:15.299068 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'printer_width': '80', 'version_check': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt test --select experiments', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m14:20:15.461019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea8350>]}
[0m14:20:15.492113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bfa4b0>]}
[0m14:20:15.494439 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:20:15.551756 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:20:15.617242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:15.617487 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:20:15.617621 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:15.639731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10908f9b0>]}
[0m14:20:15.682207 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:15.683552 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:15.701349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109533410>]}
[0m14:20:15.701631 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:20:15.701812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e31a60>]}
[0m14:20:15.702219 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:20:15.702894 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:20:15.703791 [debug] [MainThread]: Command end result
[0m14:20:15.723113 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:15.724128 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:15.725780 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:20:15.727214 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.46840212, "process_in_blocks": "0", "process_kernel_time": 0.149678, "process_mem_max_rss": "134905856", "process_out_blocks": "0", "process_user_time": 0.965844}
[0m14:20:15.727464 [debug] [MainThread]: Command `dbt test` succeeded at 14:20:15.727418 after 0.47 seconds
[0m14:20:15.727652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc9a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095332f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109068800>]}
[0m14:20:15.727816 [debug] [MainThread]: Flushing usage events
[0m14:20:15.829038 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:20:54.359005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e92d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b59d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b5760>]}


============================== 14:20:54.370046 | 4a73ed28-a999-40d6-9d22-dcaaddd42f3a ==============================
[0m14:20:54.370046 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:20:54.370433 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'no_print': 'None', 'use_colors': 'True', 'quiet': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'invocation_command': 'dbt run --select experiments', 'introspect': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:20:54.532257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d57a40>]}
[0m14:20:54.560498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105863260>]}
[0m14:20:54.561836 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:20:54.614418 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:20:54.683152 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:54.683398 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:20:54.683539 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:54.705652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10721cb00>]}
[0m14:20:54.751620 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:54.753381 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:54.768013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107679b50>]}
[0m14:20:54.768303 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:20:54.768474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078f4f20>]}
[0m14:20:54.768895 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:20:54.769587 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:20:54.770480 [debug] [MainThread]: Command end result
[0m14:20:54.789199 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:54.790212 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:54.791928 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:20:54.793406 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.47304237, "process_in_blocks": "0", "process_kernel_time": 0.180919, "process_mem_max_rss": "134479872", "process_out_blocks": "0", "process_user_time": 0.961316}
[0m14:20:54.793669 [debug] [MainThread]: Command `dbt run` succeeded at 14:20:54.793622 after 0.47 seconds
[0m14:20:54.793855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105856c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f46cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107149d00>]}
[0m14:20:54.794023 [debug] [MainThread]: Flushing usage events
[0m14:20:54.898285 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:23:03.578912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10722fd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187b5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187b5700>]}


============================== 14:23:03.590383 | 13e0e240-facd-40a0-be1a-2e9b48ea2347 ==============================
[0m14:23:03.590383 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:23:03.590716 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'fail_fast': 'False', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'version_check': 'True', 'printer_width': '80', 'target_path': 'None', 'no_print': 'None', 'invocation_command': 'dbt run --select marts.experiment', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'use_colors': 'True'}
[0m14:23:03.755437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cfba70>]}
[0m14:23:03.783930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11878b830>]}
[0m14:23:03.785272 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:23:03.837432 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:23:03.907829 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 4 files added, 0 files changed.
[0m14:23:03.908189 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/fct_experiment_conversions.sql
[0m14:23:03.908422 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/experiments.yml
[0m14:23:03.908575 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:23:03.908708 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:23:03.908929 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:23:03.909061 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:23:03.909184 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:23:04.154675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194ae2a0>]}
[0m14:23:04.199080 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:23:04.201260 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:23:04.216277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194c6540>]}
[0m14:23:04.216568 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:23:04.216747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11935ec00>]}
[0m14:23:04.217158 [warn ] [MainThread]: The selection criterion 'marts.experiment' does not match any enabled nodes
[0m14:23:04.217841 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:23:04.218095 [debug] [MainThread]: Command end result
[0m14:23:04.268178 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:23:04.272217 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:23:04.274383 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:23:04.275559 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7338924, "process_in_blocks": "0", "process_kernel_time": 0.181641, "process_mem_max_rss": "140509184", "process_out_blocks": "0", "process_user_time": 1.165126}
[0m14:23:04.275804 [debug] [MainThread]: Command `dbt run` succeeded at 14:23:04.275760 after 0.73 seconds
[0m14:23:04.275996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e59580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e5abd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106853230>]}
[0m14:23:04.276162 [debug] [MainThread]: Flushing usage events
[0m14:23:04.395226 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:48.067198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108881ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2b5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2b55e0>]}


============================== 14:24:48.079447 | d8bb8961-327c-4fa2-b7dc-e01ea31b6005 ==============================
[0m14:24:48.079447 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:24:48.079815 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'write_json': 'True', 'partial_parse': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'quiet': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'introspect': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'printer_width': '80'}
[0m14:24:48.248531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a391820>]}
[0m14:24:48.277299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd95b0>]}
[0m14:24:48.278669 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:24:48.330944 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:24:48.400911 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:24:48.401186 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:24:48.401327 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:24:48.424013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a31c740>]}
[0m14:24:48.472583 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:24:48.474405 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:24:48.489375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b512990>]}
[0m14:24:48.489700 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:24:48.489881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b57deb0>]}
[0m14:24:48.491641 [info ] [MainThread]: 
[0m14:24:48.491831 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:24:48.491966 [info ] [MainThread]: 
[0m14:24:48.492214 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:24:48.494523 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:24:48.537615 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:24:48.537875 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:24:48.538034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:48.556796 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:24:48.557628 [debug] [ThreadPool]: On list_analytics: Close
[0m14:24:48.557990 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:24:48.558251 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:24:48.561715 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.561903 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:24:48.562058 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:48.562833 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:24:48.563500 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.563641 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:24:48.563867 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.563993 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.564114 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:24:48.564351 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.564730 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:24:48.564865 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.564983 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:24:48.565206 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.565336 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:24:48.566618 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:24:48.569727 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:24:48.569917 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:24:48.570049 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:48.570411 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.570544 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:24:48.570677 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:24:48.580800 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:24:48.581681 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:24:48.582719 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:24:48.582873 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:24:48.583759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a392630>]}
[0m14:24:48.584046 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.584200 [debug] [MainThread]: On master: BEGIN
[0m14:24:48.584334 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:48.584657 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.584791 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.584918 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.585036 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.585236 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.585361 [debug] [MainThread]: On master: Close
[0m14:24:48.587045 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.587323 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:24:48.587550 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:24:48.587714 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.592435 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.593156 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.608863 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.609459 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.609637 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:24:48.609793 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:48.610169 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:24:48.610319 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.610498 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partiion by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:24:48.616441 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partiion by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:24:48.616760 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:24:48.617017 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: ROLLBACK
[0m14:24:48.621406 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_experiments_assignments'
[0m14:24:48.621609 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:24:48.622825 [debug] [Thread-1 (]: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^
[0m14:24:48.623868 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b01e030>]}
[0m14:24:48.624203 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_experiments_assignments ......... [[31mERROR[0m in 0.04s]
[0m14:24:48.624485 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.624738 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiments_assignments' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^.
[0m14:24:48.625399 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:24:48.625661 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.fct_experiment_conversions ........................... [[33mSKIP[0m]
[0m14:24:48.625896 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:24:48.626154 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:24:48.626361 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:24:48.626567 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:24:48.627077 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.627227 [debug] [MainThread]: On master: BEGIN
[0m14:24:48.627351 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:24:48.627900 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.628056 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.628187 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.628307 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.628529 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.628654 [debug] [MainThread]: On master: Close
[0m14:24:48.628829 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:48.628951 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:24:48.629110 [info ] [MainThread]: 
[0m14:24:48.629263 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m14:24:48.629708 [debug] [MainThread]: Command end result
[0m14:24:48.682815 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:24:48.683935 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:24:48.687308 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:24:48.687523 [info ] [MainThread]: 
[0m14:24:48.687704 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:24:48.687856 [info ] [MainThread]: 
[0m14:24:48.688038 [error] [MainThread]: [31mFailure in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)[0m
[0m14:24:48.688212 [error] [MainThread]:   Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^
[0m14:24:48.688344 [info ] [MainThread]: 
[0m14:24:48.688500 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiments_assignments.sql
[0m14:24:48.688629 [info ] [MainThread]: 
[0m14:24:48.688771 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:24:48.690199 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.66042906, "process_in_blocks": "0", "process_kernel_time": 0.195312, "process_mem_max_rss": "146915328", "process_out_blocks": "0", "process_user_time": 1.077618}
[0m14:24:48.690422 [debug] [MainThread]: Command `dbt run` failed at 14:24:48.690382 after 0.66 seconds
[0m14:24:48.690604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107330d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd95b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10820ec90>]}
[0m14:24:48.690773 [debug] [MainThread]: Flushing usage events
[0m14:24:48.823731 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:50.599010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5e5d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5730>]}


============================== 14:25:50.610824 | f69783b7-731d-47a0-a0ab-3e3860ea9ff3 ==============================
[0m14:25:50.610824 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:25:50.611233 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/experiments', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'log_format': 'default', 'target_path': 'None', 'log_cache_events': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'printer_width': '80', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'no_print': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'introspect': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:25:50.785792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca34200>]}
[0m14:25:50.814287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d1bb90>]}
[0m14:25:50.815568 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:25:50.867381 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:25:50.936899 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:50.937325 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:25:51.137716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6fb920>]}
[0m14:25:51.214739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:25:51.216501 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:25:51.231020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d323c50>]}
[0m14:25:51.231309 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:25:51.231481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d163740>]}
[0m14:25:51.233046 [info ] [MainThread]: 
[0m14:25:51.233223 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:25:51.233359 [info ] [MainThread]: 
[0m14:25:51.233584 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:25:51.235722 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:25:51.264760 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:25:51.264997 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:25:51.265336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:51.284407 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:25:51.285377 [debug] [ThreadPool]: On list_analytics: Close
[0m14:25:51.285722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:25:51.285962 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:25:51.289204 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.289499 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:25:51.289686 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:51.290406 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:25:51.291194 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.291329 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:25:51.291545 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.291670 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.291792 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:25:51.292030 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.292392 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:25:51.292517 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.292627 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:25:51.292813 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.292938 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:25:51.293622 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:25:51.296354 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:25:51.296509 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:25:51.296687 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:51.296942 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.297071 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:25:51.297201 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:25:51.307919 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:25:51.308648 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:25:51.309588 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:25:51.309733 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:25:51.310643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da9de20>]}
[0m14:25:51.310870 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.310998 [debug] [MainThread]: On master: BEGIN
[0m14:25:51.311111 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:51.311423 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.311591 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.311729 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.311848 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.312053 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.312180 [debug] [MainThread]: On master: Close
[0m14:25:51.313620 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.313882 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:25:51.314098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:25:51.314265 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.318172 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.319533 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.335191 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.336690 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.336874 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:25:51.337028 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:25:51.337393 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:25:51.337547 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.337733 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:25:51.342155 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:25:51.342399 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:25:51.342621 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: ROLLBACK
[0m14:25:51.348320 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_experiments_assignments'
[0m14:25:51.348521 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:25:51.349702 [debug] [Thread-1 (]: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^
[0m14:25:51.350634 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db2adb0>]}
[0m14:25:51.350948 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_experiments_assignments ......... [[31mERROR[0m in 0.04s]
[0m14:25:51.351219 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.351456 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiments_assignments' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^.
[0m14:25:51.352084 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:25:51.352346 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.fct_experiment_conversions ........................... [[33mSKIP[0m]
[0m14:25:51.352582 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:25:51.352844 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:25:51.353048 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:25:51.353255 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:25:51.353753 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.353901 [debug] [MainThread]: On master: BEGIN
[0m14:25:51.354027 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:25:51.354346 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.354479 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.354604 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.354731 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.354919 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.355044 [debug] [MainThread]: On master: Close
[0m14:25:51.355202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:51.355319 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:25:51.355470 [info ] [MainThread]: 
[0m14:25:51.355604 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m14:25:51.355918 [debug] [MainThread]: Command end result
[0m14:25:51.372573 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:25:51.373565 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:25:51.376247 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:25:51.376411 [info ] [MainThread]: 
[0m14:25:51.376578 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:25:51.376715 [info ] [MainThread]: 
[0m14:25:51.376897 [error] [MainThread]: [31mFailure in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)[0m
[0m14:25:51.377068 [error] [MainThread]:   Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^
[0m14:25:51.377260 [info ] [MainThread]: 
[0m14:25:51.377412 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiments_assignments.sql
[0m14:25:51.377542 [info ] [MainThread]: 
[0m14:25:51.377913 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:25:51.379436 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.82057434, "process_in_blocks": "0", "process_kernel_time": 0.194725, "process_mem_max_rss": "149553152", "process_out_blocks": "0", "process_user_time": 1.254209}
[0m14:25:51.379687 [debug] [MainThread]: Command `dbt run` failed at 14:25:51.379642 after 0.82 seconds
[0m14:25:51.379882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b57f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce7a900>]}
[0m14:25:51.380062 [debug] [MainThread]: Flushing usage events
[0m14:25:51.481053 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:18.861900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f2c950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb56d0>]}


============================== 14:26:18.868149 | 7b6b96ee-c6c1-49fe-a8e2-2759c38abf54 ==============================
[0m14:26:18.868149 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:26:18.868571 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'empty': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'introspect': 'True', 'target_path': 'None', 'no_print': 'None', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'version_check': 'True', 'printer_width': '80', 'debug': 'False', 'cache_selected_only': 'False'}
[0m14:26:19.028888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f543b0>]}
[0m14:26:19.057090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c77e60>]}
[0m14:26:19.058576 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:26:19.111510 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:26:19.177987 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:19.178451 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:26:19.374159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afb2a50>]}
[0m14:26:19.448456 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:19.449816 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:19.462287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aec79b0>]}
[0m14:26:19.462573 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:26:19.462749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10affb6e0>]}
[0m14:26:19.464385 [info ] [MainThread]: 
[0m14:26:19.464565 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:26:19.464760 [info ] [MainThread]: 
[0m14:26:19.465070 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:26:19.467258 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:26:19.496643 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:26:19.496892 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:26:19.497039 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:19.515049 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:26:19.516036 [debug] [ThreadPool]: On list_analytics: Close
[0m14:26:19.516389 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:26:19.516635 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:26:19.519913 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.520085 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:26:19.520224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:19.521107 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:26:19.521912 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.522071 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:26:19.522315 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.522453 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.522583 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:26:19.523097 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.523530 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:19.523669 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.523793 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:19.524003 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.524132 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:26:19.524821 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:26:19.527475 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:19.527635 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:26:19.527820 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:19.528069 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.528199 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:19.528388 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:26:19.545994 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:26:19.546759 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:26:19.547738 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:26:19.547897 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:26:19.548607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af45190>]}
[0m14:26:19.548879 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.549076 [debug] [MainThread]: On master: BEGIN
[0m14:26:19.549207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:19.549541 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.549679 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.550029 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.550318 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.550529 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.550660 [debug] [MainThread]: On master: Close
[0m14:26:19.555066 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.555331 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:26:19.555596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:26:19.555809 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.559939 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.560674 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.576119 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.576672 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.576846 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:26:19.577000 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:19.577364 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.577513 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.577690 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:26:19.604837 [debug] [Thread-1 (]: SQL status: OK in 0.027 seconds
[0m14:26:19.608146 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.608339 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:26:19.609219 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:19.616132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:19.616370 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.616534 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:19.618571 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:26:19.621980 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.622168 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:26:19.622731 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.623875 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:26:19.624780 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a2ecf0>]}
[0m14:26:19.625084 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:26:19.625347 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.625672 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.625920 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:26:19.626179 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:19.626341 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.627915 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.628307 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.629864 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.630217 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.630385 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:26:19.630535 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:26:19.630851 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.631004 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.631233 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:19.634700 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:19.635063 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m14:26:19.635398 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: ROLLBACK
[0m14:26:19.639640 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:19.639828 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:26:19.640981 [debug] [Thread-3 (]: Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^
[0m14:26:19.641227 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbaf590>]}
[0m14:26:19.641545 [error] [Thread-3 (]: 2 of 3 ERROR creating sql table model main.fct_experiment_conversions .......... [[31mERROR[0m in 0.02s]
[0m14:26:19.641816 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.642059 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiment_conversions' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^.
[0m14:26:19.642704 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:26:19.642938 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:26:19.643162 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:26:19.644893 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.645051 [debug] [MainThread]: On master: BEGIN
[0m14:26:19.645187 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:26:19.645563 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.645696 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.645823 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.645934 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.646126 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.646251 [debug] [MainThread]: On master: Close
[0m14:26:19.646422 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:19.646539 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:26:19.646648 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:26:19.646809 [info ] [MainThread]: 
[0m14:26:19.646967 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m14:26:19.647445 [debug] [MainThread]: Command end result
[0m14:26:19.664739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:19.665789 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:19.668409 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:26:19.668570 [info ] [MainThread]: 
[0m14:26:19.668744 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:26:19.668885 [info ] [MainThread]: 
[0m14:26:19.669062 [error] [MainThread]: [31mFailure in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)[0m
[0m14:26:19.669226 [error] [MainThread]:   Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^
[0m14:26:19.669352 [info ] [MainThread]: 
[0m14:26:19.669503 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiment_conversions.sql
[0m14:26:19.669631 [info ] [MainThread]: 
[0m14:26:19.669772 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 NO-OP=0 TOTAL=3
[0m14:26:19.670786 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.84552443, "process_in_blocks": "0", "process_kernel_time": 0.196724, "process_mem_max_rss": "180895744", "process_out_blocks": "0", "process_user_time": 1.292083}
[0m14:26:19.670981 [debug] [MainThread]: Command `dbt run` failed at 14:26:19.670945 after 0.85 seconds
[0m14:26:19.671151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b4c1a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10988a600>]}
[0m14:26:19.671316 [debug] [MainThread]: Flushing usage events
[0m14:26:19.784737 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:42.785364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bbe540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107432510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f8eb70>]}


============================== 14:26:42.793804 | afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19 ==============================
[0m14:26:42.793804 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:26:42.794141 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'debug': 'False', 'write_json': 'True', 'no_print': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'introspect': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default'}
[0m14:26:42.958946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779e150>]}
[0m14:26:42.987463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10769a4e0>]}
[0m14:26:42.988799 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:26:43.041233 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:26:43.111032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:43.111443 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiment_conversions.sql
[0m14:26:43.311564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f44650>]}
[0m14:26:43.389425 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:43.390722 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:43.404242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f627e0>]}
[0m14:26:43.404512 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:26:43.404688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e269f0>]}
[0m14:26:43.406293 [info ] [MainThread]: 
[0m14:26:43.406480 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:26:43.406622 [info ] [MainThread]: 
[0m14:26:43.406851 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:26:43.408990 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:26:43.439431 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:26:43.439660 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:26:43.439808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:43.457454 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:26:43.458443 [debug] [ThreadPool]: On list_analytics: Close
[0m14:26:43.458798 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:26:43.459049 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:26:43.462608 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.462784 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:26:43.462911 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:43.463741 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:26:43.464386 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.464512 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:26:43.464722 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.464843 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.464963 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:26:43.465203 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.465579 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:43.465711 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.465823 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:43.466014 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.466144 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:26:43.466803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:26:43.469397 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:43.469553 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:26:43.469728 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:43.469963 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.470089 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:43.470217 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:26:43.480951 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:26:43.481697 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:26:43.482651 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:26:43.482794 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:26:43.483535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e6030>]}
[0m14:26:43.483795 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.483944 [debug] [MainThread]: On master: BEGIN
[0m14:26:43.484072 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:43.484337 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.484470 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.484596 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.484715 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.484914 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.485038 [debug] [MainThread]: On master: Close
[0m14:26:43.486404 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.486638 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:26:43.486838 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:26:43.486999 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.490741 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.491526 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.507500 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.508350 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.508547 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:26:43.508913 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:43.509284 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.509439 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.509622 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:26:43.538402 [debug] [Thread-1 (]: SQL status: OK in 0.028 seconds
[0m14:26:43.541255 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.541451 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:26:43.541958 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.542740 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.543038 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:26:43.543451 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.546989 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.547383 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:26:43.548365 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.550614 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.550796 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:26:43.551128 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.558733 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:43.558976 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.559132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:43.561378 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:26:43.564208 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.564410 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:26:43.565085 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.566256 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:26:43.567210 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caedb0>]}
[0m14:26:43.567518 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.08s]
[0m14:26:43.567792 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.568128 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.568398 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:26:43.568692 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:43.568858 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.570602 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.571053 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.572606 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.573137 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.573306 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:26:43.573452 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:26:43.573758 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.573920 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.574121 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:43.584113 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m14:26:43.585834 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.586013 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:26:43.586385 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.587146 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:26:43.587307 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.588165 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:26:43.589743 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.590998 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.591174 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:26:43.591492 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.592136 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:26:43.592390 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e6b350>]}
[0m14:26:43.592676 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.02s]
[0m14:26:43.592925 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.593254 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:26:43.593544 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:26:43.593835 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:26:43.594015 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:26:43.596368 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.596844 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:26:43.598653 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.599071 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.599280 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:26:43.599423 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:26:43.599778 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.599929 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.600254 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vs.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:26:43.606047 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vs.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:26:43.606458 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m14:26:43.606691 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: ROLLBACK
[0m14:26:43.610987 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results'
[0m14:26:43.611192 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:26:43.612695 [debug] [Thread-2 (]: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^
[0m14:26:43.613456 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10825e4e0>]}
[0m14:26:43.613814 [error] [Thread-2 (]: 3 of 3 ERROR creating sql table model main.rpt_experiment_results .............. [[31mERROR[0m in 0.02s]
[0m14:26:43.614129 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:26:43.614365 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^.
[0m14:26:43.615301 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.618844 [debug] [MainThread]: On master: BEGIN
[0m14:26:43.619000 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:26:43.619393 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.619566 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.619708 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.619831 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.620037 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.620167 [debug] [MainThread]: On master: Close
[0m14:26:43.620349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:43.620502 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:26:43.620632 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:26:43.620746 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:26:43.620914 [info ] [MainThread]: 
[0m14:26:43.621062 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m14:26:43.621475 [debug] [MainThread]: Command end result
[0m14:26:43.637822 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:43.639005 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:43.642114 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:26:43.642267 [info ] [MainThread]: 
[0m14:26:43.642433 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:26:43.642574 [info ] [MainThread]: 
[0m14:26:43.642748 [error] [MainThread]: [31mFailure in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)[0m
[0m14:26:43.642916 [error] [MainThread]:   Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^
[0m14:26:43.643042 [info ] [MainThread]: 
[0m14:26:43.643190 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results.sql
[0m14:26:43.643321 [info ] [MainThread]: 
[0m14:26:43.643461 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:26:43.644831 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.89537007, "process_in_blocks": "0", "process_kernel_time": 0.212696, "process_mem_max_rss": "187138048", "process_out_blocks": "0", "process_user_time": 1.344202}
[0m14:26:43.645043 [debug] [MainThread]: Command `dbt run` failed at 14:26:43.645003 after 0.90 seconds
[0m14:26:43.645265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bbe540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107595e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068ad4f0>]}
[0m14:26:43.645473 [debug] [MainThread]: Flushing usage events
[0m14:26:43.754961 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:07.267017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10758fa40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109249a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092497c0>]}


============================== 14:27:07.272007 | c3c0f5e5-953b-42ea-a6d8-148267016d0c ==============================
[0m14:27:07.272007 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:27:07.272440 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'log_format': 'default', 'empty': 'False', 'write_json': 'True', 'target_path': 'None'}
[0m14:27:07.434465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092d0140>]}
[0m14:27:07.463764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108623740>]}
[0m14:27:07.465998 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:27:07.517376 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:27:07.584785 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:27:07.585221 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:27:07.778225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad8fe0>]}
[0m14:27:07.853520 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:27:07.854919 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:27:07.867944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096ff4d0>]}
[0m14:27:07.868233 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:27:07.868411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a7d430>]}
[0m14:27:07.870006 [info ] [MainThread]: 
[0m14:27:07.870187 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:27:07.870327 [info ] [MainThread]: 
[0m14:27:07.870554 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:27:07.872683 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:27:07.901060 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:27:07.901296 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:27:07.901442 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:07.919291 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:27:07.920265 [debug] [ThreadPool]: On list_analytics: Close
[0m14:27:07.920632 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:27:07.920852 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:27:07.924096 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.924267 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:27:07.924402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:27:07.925160 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:27:07.926090 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.926454 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:27:07.926775 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.926932 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.927078 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:27:07.927596 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.928034 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:27:07.928218 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.928338 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:27:07.928625 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.928953 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:27:07.929732 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:27:07.932504 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:27:07.932684 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:27:07.932922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:27:07.933340 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.933813 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:27:07.934225 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:27:07.949177 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:27:07.950362 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:27:07.951338 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:27:07.951507 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:27:07.952359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d77b00>]}
[0m14:27:07.952594 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:07.952731 [debug] [MainThread]: On master: BEGIN
[0m14:27:07.952848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:27:07.953133 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:07.953268 [debug] [MainThread]: On master: COMMIT
[0m14:27:07.953399 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:07.953517 [debug] [MainThread]: On master: COMMIT
[0m14:27:07.953721 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:07.953848 [debug] [MainThread]: On master: Close
[0m14:27:07.955166 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.955428 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:27:07.955646 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:27:07.955819 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.959658 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.960267 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.975776 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.976593 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.976779 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:27:07.976928 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:07.977284 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:07.977429 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.977605 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:27:07.994990 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m14:27:07.997962 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.998177 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:27:07.998690 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:07.999385 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.999560 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:27:08.000007 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.003103 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.003301 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:27:08.004163 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.006246 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.006423 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:27:08.006717 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.013882 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:27:08.014097 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.014308 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:27:08.016648 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:27:08.019588 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.019785 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:27:08.020651 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.021823 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:27:08.022771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068f6ba0>]}
[0m14:27:08.023080 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:27:08.023342 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:27:08.023680 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.023891 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:27:08.024197 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:27:08.024358 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.025974 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.026607 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.028238 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.028784 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.028968 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:27:08.029131 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:27:08.029572 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.029754 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.029967 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:27:08.038977 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m14:27:08.039675 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.039946 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:27:08.040424 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.041019 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.041187 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:27:08.041610 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.043228 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.043408 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:27:08.043718 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.045010 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.045170 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:27:08.045576 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.046416 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:27:08.046583 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.046725 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:27:08.047904 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.048970 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.049128 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:27:08.049512 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.050100 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:27:08.050353 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dd1400>]}
[0m14:27:08.050631 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m14:27:08.050877 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.051211 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:27:08.051454 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:27:08.051687 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:27:08.051848 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:27:08.053680 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.054112 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:27:08.055630 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.055998 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.056153 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:27:08.056293 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:27:08.056589 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.056774 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.057086 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:27:08.062721 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:27:08.063111 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m14:27:08.063367 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: ROLLBACK
[0m14:27:08.072189 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results'
[0m14:27:08.072431 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:27:08.073662 [debug] [Thread-2 (]: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^
[0m14:27:08.073916 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f94170>]}
[0m14:27:08.074219 [error] [Thread-2 (]: 3 of 3 ERROR creating sql table model main.rpt_experiment_results .............. [[31mERROR[0m in 0.02s]
[0m14:27:08.074485 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:27:08.074747 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^.
[0m14:27:08.075709 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:08.075878 [debug] [MainThread]: On master: BEGIN
[0m14:27:08.076007 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:27:08.076324 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:08.076465 [debug] [MainThread]: On master: COMMIT
[0m14:27:08.076714 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:08.076837 [debug] [MainThread]: On master: COMMIT
[0m14:27:08.077056 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:08.077401 [debug] [MainThread]: On master: Close
[0m14:27:08.077621 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:08.077757 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:27:08.077966 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:27:08.078083 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:27:08.078300 [info ] [MainThread]: 
[0m14:27:08.078774 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m14:27:08.079468 [debug] [MainThread]: Command end result
[0m14:27:08.101205 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:27:08.102313 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:27:08.105167 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:27:08.105330 [info ] [MainThread]: 
[0m14:27:08.105504 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:27:08.105645 [info ] [MainThread]: 
[0m14:27:08.105829 [error] [MainThread]: [31mFailure in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)[0m
[0m14:27:08.106275 [error] [MainThread]:   Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^
[0m14:27:08.106455 [info ] [MainThread]: 
[0m14:27:08.106642 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results.sql
[0m14:27:08.106791 [info ] [MainThread]: 
[0m14:27:08.106942 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:27:08.108416 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8814913, "process_in_blocks": "0", "process_kernel_time": 0.204851, "process_mem_max_rss": "186695680", "process_out_blocks": "0", "process_user_time": 1.333466}
[0m14:27:08.108670 [debug] [MainThread]: Command `dbt run` failed at 14:27:08.108623 after 0.88 seconds
[0m14:27:08.108934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092497f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084ec620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084ed550>]}
[0m14:27:08.109118 [debug] [MainThread]: Flushing usage events
[0m14:27:08.219622 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:53.564543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f79b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d670>]}


============================== 14:28:53.576937 | 99b2c78e-c422-4455-b4a5-298216317fe2 ==============================
[0m14:28:53.576937 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:28:53.577270 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'version_check': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_colors': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'log_format': 'default', 'no_print': 'None', 'static_parser': 'True', 'indirect_selection': 'eager'}
[0m14:28:53.752306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99b2c78e-c422-4455-b4a5-298216317fe2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ef41a0>]}
[0m14:28:53.780992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99b2c78e-c422-4455-b4a5-298216317fe2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065a0b60>]}
[0m14:28:53.782376 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:28:53.834388 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:28:53.903663 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:28:53.904078 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:28:54.059605 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.rpt_experiment_results' (models/marts/experiments/rpt_experiment_results.sql) depends on a node named 'fct_experiment_assignments' which was not found
[0m14:28:54.061361 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5351511, "process_in_blocks": "0", "process_kernel_time": 0.168743, "process_mem_max_rss": "137887744", "process_out_blocks": "0", "process_user_time": 1.016422}
[0m14:28:54.061626 [debug] [MainThread]: Command `dbt run` failed at 14:28:54.061575 after 0.54 seconds
[0m14:28:54.061829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a1d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107419f40>]}
[0m14:28:54.062007 [debug] [MainThread]: Flushing usage events
[0m14:28:54.191517 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:15.802034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104813f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed17f0>]}


============================== 14:29:15.806436 | e8805335-8dbb-41c4-9050-6a5026f22abd ==============================
[0m14:29:15.806436 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:29:15.806800 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'quiet': 'False', 'debug': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'version_check': 'True', 'empty': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'warn_error': 'None', 'log_cache_events': 'False'}
[0m14:29:15.975006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f3bef0>]}
[0m14:29:16.003438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049abe60>]}
[0m14:29:16.004961 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:29:16.057170 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:29:16.124834 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:29:16.125255 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:29:16.318272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10580d640>]}
[0m14:29:16.394353 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:16.395945 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:16.409725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10552ab10>]}
[0m14:29:16.410010 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:29:16.410180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c1880>]}
[0m14:29:16.411738 [info ] [MainThread]: 
[0m14:29:16.411924 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:29:16.412062 [info ] [MainThread]: 
[0m14:29:16.412290 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:29:16.414481 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:29:16.445496 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:29:16.445769 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:29:16.445925 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:16.466230 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m14:29:16.467267 [debug] [ThreadPool]: On list_analytics: Close
[0m14:29:16.467831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:29:16.468108 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:29:16.471874 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.472112 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:29:16.472252 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:29:16.473082 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:29:16.473945 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.474095 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:29:16.474328 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.474454 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.474576 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:29:16.474820 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.475240 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:29:16.475376 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.475491 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:29:16.475691 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.475820 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:29:16.476546 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:29:16.479711 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:29:16.479924 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:29:16.480139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:29:16.480446 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.480577 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:29:16.480710 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:29:16.497026 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:29:16.498003 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:29:16.499016 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:29:16.499169 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:29:16.500233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059ffb00>]}
[0m14:29:16.500474 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.500609 [debug] [MainThread]: On master: BEGIN
[0m14:29:16.500731 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:29:16.501054 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.501186 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.501309 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.501427 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.501626 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.501750 [debug] [MainThread]: On master: Close
[0m14:29:16.503260 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.503513 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:29:16.503725 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:29:16.503891 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.508859 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.509681 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.525170 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.526422 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.526614 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:29:16.526774 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:16.527197 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.527349 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.527528 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:29:16.547629 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m14:29:16.550417 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.550618 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:29:16.551114 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.551803 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.551970 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:29:16.552401 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.555921 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.556153 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:29:16.557239 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.559524 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.559704 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:29:16.560012 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.567298 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:29:16.567518 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.567674 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:29:16.569882 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:29:16.572625 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.572810 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:29:16.573604 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.574721 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:29:16.575653 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10414ad20>]}
[0m14:29:16.575951 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:29:16.576209 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.576575 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.576819 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:29:16.577074 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:29:16.577239 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.578818 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.579459 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.581033 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.581564 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.581731 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:29:16.581879 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:29:16.582311 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.582566 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.582773 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:29:16.592480 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m14:29:16.593164 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.593429 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:29:16.593901 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.594487 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.594646 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:29:16.595064 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.596722 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.596899 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:29:16.597213 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.598526 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.598679 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:29:16.598943 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.599689 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:29:16.599842 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.599980 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:29:16.601509 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.602603 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.602760 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:29:16.603107 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.603701 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:29:16.603970 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a67710>]}
[0m14:29:16.604247 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m14:29:16.604487 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.604801 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:29:16.605081 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:29:16.605366 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:29:16.605536 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:29:16.607405 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.608062 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:29:16.609569 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.610265 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.610419 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:29:16.610562 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:29:16.610861 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.611007 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.611292 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m14:29:16.625631 [debug] [Thread-2 (]: SQL status: OK in 0.014 seconds
[0m14:29:16.628092 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.628270 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m14:29:16.628627 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.629312 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:29:16.629464 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.629598 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:29:16.630108 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.631260 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.631428 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m14:29:16.631702 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.632308 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:29:16.632558 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b9b4d0>]}
[0m14:29:16.632824 [info ] [Thread-2 (]: 3 of 3 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.03s]
[0m14:29:16.633061 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:29:16.633618 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.633797 [debug] [MainThread]: On master: BEGIN
[0m14:29:16.633930 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:29:16.634179 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.634311 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.634434 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.634547 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.634731 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.634852 [debug] [MainThread]: On master: Close
[0m14:29:16.635011 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:29:16.635123 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:29:16.635224 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:29:16.635322 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:29:16.635466 [info ] [MainThread]: 
[0m14:29:16.635604 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:29:16.635984 [debug] [MainThread]: Command end result
[0m14:29:16.651655 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:16.652705 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:16.655592 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:29:16.655756 [info ] [MainThread]: 
[0m14:29:16.655932 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:16.656065 [info ] [MainThread]: 
[0m14:29:16.656214 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:29:16.657618 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.89273214, "process_in_blocks": "0", "process_kernel_time": 0.207586, "process_mem_max_rss": "193691648", "process_out_blocks": "0", "process_user_time": 1.340497}
[0m14:29:16.657831 [debug] [MainThread]: Command `dbt run` succeeded at 14:29:16.657793 after 0.89 seconds
[0m14:29:16.658000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104813f20>]}
[0m14:29:16.658164 [debug] [MainThread]: Flushing usage events
[0m14:29:16.763014 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:41.560270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044165d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db57c0>]}


============================== 14:29:41.570257 | adfbecbe-671f-486d-9c72-f90746c9618b ==============================
[0m14:29:41.570257 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:29:41.570658 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select path:models/marts/experiements', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'empty': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m14:29:41.776213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f1dc10>]}
[0m14:29:41.804099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107999e20>]}
[0m14:29:41.805376 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:29:41.856505 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:29:41.924356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:29:41.924597 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:29:41.924735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:29:41.946864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3bd40>]}
[0m14:29:41.990888 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:41.992443 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:42.010332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114967a40>]}
[0m14:29:42.010616 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:29:42.010783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123feff0>]}
[0m14:29:42.011860 [warn ] [MainThread]: The selection criterion 'path:models/marts/experiements' does not match any enabled nodes
[0m14:29:42.012567 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:29:42.013511 [debug] [MainThread]: Command end result
[0m14:29:42.034607 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:42.035635 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:42.037177 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:29:42.038612 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.5171716, "process_in_blocks": "0", "process_kernel_time": 0.173199, "process_mem_max_rss": "136593408", "process_out_blocks": "0", "process_user_time": 0.962304}
[0m14:29:42.038894 [debug] [MainThread]: Command `dbt test` succeeded at 14:29:42.038844 after 0.52 seconds
[0m14:29:42.039096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db50a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110640da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fe4050>]}
[0m14:29:42.039267 [debug] [MainThread]: Flushing usage events
[0m14:29:42.144577 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:30:13.223560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102bd6f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057118b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105711670>]}


============================== 14:30:13.228381 | 2268c34c-d5d5-4008-a35d-03c85f0838b1 ==============================
[0m14:30:13.228381 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:30:13.228713 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select path:models/marts/experiments', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'debug': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'target_path': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'empty': 'None', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'warn_error': 'None'}
[0m14:30:13.396998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bdd520>]}
[0m14:30:13.427317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e764b0>]}
[0m14:30:13.428987 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:30:13.491648 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:30:13.561231 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:30:13.561474 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:30:13.561614 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:30:13.583971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057787d0>]}
[0m14:30:13.626980 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:30:13.628300 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:30:13.647592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105df7da0>]}
[0m14:30:13.647896 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:30:13.648078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10599cd10>]}
[0m14:30:13.649865 [info ] [MainThread]: 
[0m14:30:13.650048 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:30:13.650192 [info ] [MainThread]: 
[0m14:30:13.650439 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:30:13.652733 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:30:13.682773 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:30:13.683004 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:30:13.683144 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:13.696224 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:30:13.696443 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:30:13.696593 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:30:13.713176 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:30:13.714394 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:30:13.715472 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:30:13.715622 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:30:13.716712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b0560>]}
[0m14:30:13.716943 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.717080 [debug] [MainThread]: On master: BEGIN
[0m14:30:13.717201 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:30:13.717465 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.717593 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.717717 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.717837 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.718036 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.718162 [debug] [MainThread]: On master: Close
[0m14:30:13.719799 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.720000 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.720348 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.720206 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:30:13.720557 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.720725 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:30:13.720908 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:30:13.721151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:30:13.721323 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:30:13.721544 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:30:13.721757 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:30:13.721929 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.722119 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:30:13.722271 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.722417 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.731985 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.732222 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.734711 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.738415 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.740258 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.740814 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.741030 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.741209 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.741379 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.749519 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.750711 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.751803 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.753502 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.754077 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.754288 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.754481 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.754642 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:30:13.754812 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.754979 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:30:13.755141 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:30:13.755292 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:30:13.755441 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:30:13.755582 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:30:13.755728 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.755977 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:30:13.756203 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756457 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756671 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756845 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.757004 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.757149 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.757302 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.757457 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.757613 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.757771 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.757951 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:30:13.758203 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:30:13.759429 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.759608 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.761841 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:30:13.762573 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:30:13.763052 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:30:13.763220 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:30:13.763398 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m14:30:13.763808 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:30:13.763963 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:30:13.764439 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:30:13.764289 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.04s]
[0m14:30:13.797207 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:30:13.797642 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.798320 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:30:13.798647 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.08s]
[0m14:30:13.799112 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:30:13.799338 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.799778 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:30:13.800011 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.800199 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:30:13.800365 [info ] [Thread-4 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:30:13.800540 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:30:13.800703 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.800966 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.08s]
[0m14:30:13.801182 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:30:13.801424 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.08s]
[0m14:30:13.801603 [info ] [Thread-3 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:30:13.801830 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.801983 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.802189 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.802353 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:30:13.802525 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.804587 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.804843 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.805011 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.805191 [info ] [Thread-2 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:30:13.805448 [info ] [Thread-1 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:30:13.807333 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.807539 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:30:13.807723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:30:13.807986 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.808142 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.808333 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.810146 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.811321 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.813305 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.813495 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.814974 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.815443 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.815635 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.815840 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:30:13.816104 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.817353 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.817584 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.817745 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:30:13.818811 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.819023 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:30:13.819484 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.819645 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:30:13.819907 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:30:13.820106 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:30:13.820333 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.820499 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:30:13.820662 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.820809 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.820971 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:30:13.821175 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.821355 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.821519 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.821656 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.821823 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.822070 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.822394 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.822584 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.822792 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:30:13.822950 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.823116 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.823338 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.823513 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.824281 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:30:13.824860 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:30:13.825451 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:30:13.826139 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:30:13.826330 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.826781 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:30:13.826954 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:30:13.827373 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:30:13.828030 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:30:13.828192 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:30:13.828380 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:30:13.828671 [info ] [Thread-4 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.03s]
[0m14:30:13.829165 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:30:13.829409 [info ] [Thread-3 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:30:13.829925 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.829699 [error] [Thread-2 (]: 7 of 12 FAIL 3300 not_null_fct_experiments_assignments_assigned_at ............. [[31mFAIL 3300[0m in 0.02s]
[0m14:30:13.830143 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:30:13.830375 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.830567 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.830806 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.831216 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.831061 [info ] [Thread-1 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:30:13.831447 [info ] [Thread-4 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:30:13.831654 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.831844 [info ] [Thread-3 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:30:13.832103 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.832294 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:30:13.832468 [info ] [Thread-2 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:30:13.832655 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:30:13.832836 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.833010 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.833195 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:30:13.833373 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.833553 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:30:13.836009 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.836284 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.839605 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.839878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:30:13.843052 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.843316 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.845270 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.845524 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.845708 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.845878 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.847046 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.848318 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.849972 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.850345 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.851802 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.852166 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.852393 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.852588 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.852767 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:30:13.852943 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.853095 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:30:13.853249 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:30:13.853403 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:30:13.853552 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:30:13.853694 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:30:13.853836 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:30:13.854088 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.854307 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.854574 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.854735 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.854886 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.855031 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.855175 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.855333 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.855507 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.855676 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.855861 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.856175 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:30:13.856378 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:30:13.856629 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.857193 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.858141 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:30:13.858900 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:30:13.859989 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:30:13.860187 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m14:30:13.862308 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:30:13.862756 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:30:13.863599 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:30:13.863802 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:30:13.864439 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:30:13.864603 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:30:13.865049 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:30:13.865785 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:30:13.865355 [info ] [Thread-3 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:30:13.866017 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:30:13.866403 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:30:13.866259 [info ] [Thread-4 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:30:13.866681 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.866925 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:30:13.867384 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.867141 [info ] [Thread-2 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:30:13.867714 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.867961 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.868592 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.868732 [debug] [MainThread]: On master: BEGIN
[0m14:30:13.868847 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:30:13.869144 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.869265 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.869381 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.869493 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.869686 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.869803 [debug] [MainThread]: On master: Close
[0m14:30:13.869951 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:30:13.870069 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:30:13.870178 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:30:13.870282 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:30:13.870386 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:30:13.870517 [info ] [MainThread]: 
[0m14:30:13.870650 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:30:13.871509 [debug] [MainThread]: Command end result
[0m14:30:13.893915 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:30:13.895103 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:30:13.898768 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:30:13.898958 [info ] [MainThread]: 
[0m14:30:13.899128 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:30:13.899263 [info ] [MainThread]: 
[0m14:30:13.899436 [error] [MainThread]: [31mFailure in test not_null_fct_experiments_assignments_assigned_at (models/marts/experiments/experiments.yml)[0m
[0m14:30:13.899596 [error] [MainThread]:   Got 3300 results, configured to fail if != 0
[0m14:30:13.899714 [info ] [MainThread]: 
[0m14:30:13.899863 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/experiments.yml/not_null_fct_experiments_assignments_assigned_at.sql
[0m14:30:13.899991 [info ] [MainThread]: 
[0m14:30:13.900129 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m14:30:13.901179 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.7169268, "process_in_blocks": "0", "process_kernel_time": 0.208098, "process_mem_max_rss": "153927680", "process_out_blocks": "0", "process_user_time": 1.154499}
[0m14:30:13.901386 [debug] [MainThread]: Command `dbt test` failed at 14:30:13.901348 after 0.72 seconds
[0m14:30:13.901564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105924050>]}
[0m14:30:13.901737 [debug] [MainThread]: Flushing usage events
[0m14:30:14.030136 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:08.790235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106683500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff99a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff9790>]}


============================== 14:34:08.803928 | c04275c5-4f96-4002-9102-daf8febb49b4 ==============================
[0m14:34:08.803928 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:08.804277 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'empty': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'target_path': 'None', 'debug': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'write_json': 'True', 'printer_width': '80', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'partial_parse': 'True'}
[0m14:34:08.978175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070d8350>]}
[0m14:34:09.006826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070960c0>]}
[0m14:34:09.008243 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:09.060129 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:09.130534 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:34:09.130957 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:34:09.332171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10793c650>]}
[0m14:34:09.411430 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:09.413744 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:09.435316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077a07a0>]}
[0m14:34:09.435635 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:34:09.435820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067d28d0>]}
[0m14:34:09.437766 [info ] [MainThread]: 
[0m14:34:09.437993 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:34:09.438138 [info ] [MainThread]: 
[0m14:34:09.438388 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:34:09.440840 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:34:09.473652 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:09.473883 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:34:09.474025 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:09.487401 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:34:09.487647 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:09.487795 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:34:09.504836 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:34:09.506170 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:34:09.507205 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:34:09.507533 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:34:09.508674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107958830>]}
[0m14:34:09.508929 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.509071 [debug] [MainThread]: On master: BEGIN
[0m14:34:09.509201 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:34:09.509533 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.509665 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.509794 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.509911 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.510107 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.510237 [debug] [MainThread]: On master: Close
[0m14:34:09.511869 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.512067 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.512409 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.512569 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.512270 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:34:09.512776 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:34:09.512917 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:34:09.513301 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:34:09.513096 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:34:09.513568 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:34:09.513769 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:34:09.513931 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.514116 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:34:09.514262 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.514407 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.520249 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.520539 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.522897 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.527681 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.529664 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.530761 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.537207 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.539640 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.540870 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.541141 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.541294 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.542447 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.543458 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.543744 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.543989 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.544176 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:34:09.544385 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:34:09.544584 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.544863 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.545025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:34:09.545188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.545338 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:34:09.545486 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:34:09.545848 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:34:09.546052 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:34:09.546222 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546370 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546651 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.546818 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546978 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.547120 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.547292 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:34:09.547463 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.547629 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:34:09.547785 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.548032 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.548269 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.549395 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.549556 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.551765 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:34:09.552496 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:34:09.552710 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m14:34:09.553150 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:34:09.553537 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:34:09.553725 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:34:09.553878 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:34:09.554502 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:34:09.554656 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:34:09.554983 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.04s]
[0m14:34:09.555681 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:34:09.556157 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:34:09.556410 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.04s]
[0m14:34:09.556681 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.557136 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:34:09.557306 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:34:09.557547 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.557733 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.557920 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:34:09.558316 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.558169 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.04s]
[0m14:34:09.558542 [info ] [Thread-3 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:34:09.558750 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.05s]
[0m14:34:09.559170 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.558934 [info ] [Thread-4 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:34:09.559384 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:34:09.559599 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.559764 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.559949 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:34:09.560106 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.560278 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.560636 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.560453 [info ] [Thread-1 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:34:09.563112 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.563409 [info ] [Thread-2 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:34:09.565504 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.565735 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:34:09.566018 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:34:09.566314 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.566502 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.568408 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.570325 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.570540 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.570704 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.572694 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.573755 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.574011 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.574170 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.575262 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.576318 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.576603 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.576800 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.576989 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:34:09.577165 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:34:09.577341 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.577494 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:34:09.577667 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:34:09.577826 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:34:09.577998 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.578329 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.578493 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:34:09.578654 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.578794 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.578978 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:34:09.579130 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.579275 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.579412 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.579622 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.579788 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.579933 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.580080 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.580321 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.580500 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.580718 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:09.580969 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.581157 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:34:09.581992 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:34:09.582935 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:34:09.583221 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.583385 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.583838 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:34:09.584311 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:34:09.585760 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:34:09.586005 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:34:09.586714 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:34:09.586872 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:34:09.587383 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:34:09.588136 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:34:09.587700 [info ] [Thread-3 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.03s]
[0m14:34:09.588615 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:34:09.588473 [info ] [Thread-4 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:34:09.588878 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:34:09.589182 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.589707 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.589494 [info ] [Thread-2 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:34:09.590190 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.590022 [error] [Thread-1 (]: 7 of 12 FAIL 3300 not_null_fct_experiments_assignments_assigned_at ............. [[31mFAIL 3300[0m in 0.02s]
[0m14:34:09.590453 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.590690 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.591062 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.590864 [info ] [Thread-3 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:34:09.591278 [info ] [Thread-4 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:34:09.591464 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.591656 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.591854 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:34:09.592033 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:34:09.592202 [info ] [Thread-2 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:34:09.592385 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:34:09.592561 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.592725 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.592893 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:34:09.593067 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:34:09.595340 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.597174 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.597361 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.597523 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.600549 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.602750 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.603200 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.603399 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.605641 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.606951 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.607134 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.607418 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.609588 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.610771 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.611092 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.611290 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:34:09.611466 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:34:09.611771 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.611986 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:34:09.612145 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:34:09.612329 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.612588 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.612754 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.612924 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:34:09.613097 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:34:09.613246 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.613409 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.613562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.613708 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:34:09.613863 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.614032 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.614330 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.614502 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.614729 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.614948 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.615120 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.615296 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:34:09.615460 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.615605 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.615798 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:34:09.616741 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:34:09.617413 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:34:09.618029 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:34:09.618455 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:34:09.618795 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:34:09.618988 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:34:09.619153 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:34:09.619307 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.619605 [info ] [Thread-4 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:34:09.620360 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:34:09.620615 [info ] [Thread-3 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:34:09.620898 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.621526 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:34:09.621982 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:34:09.622197 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.622680 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:34:09.622829 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:34:09.623024 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:34:09.623297 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:34:09.623743 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.623500 [info ] [Thread-2 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:34:09.624035 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.624605 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.624755 [debug] [MainThread]: On master: BEGIN
[0m14:34:09.624877 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:34:09.625221 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.625385 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.625522 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.625643 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.625852 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.625983 [debug] [MainThread]: On master: Close
[0m14:34:09.626156 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:09.626276 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:34:09.626392 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:34:09.626505 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:34:09.626614 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:34:09.626751 [info ] [MainThread]: 
[0m14:34:09.626889 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m14:34:09.627693 [debug] [MainThread]: Command end result
[0m14:34:09.644106 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:09.646098 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:09.652347 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:34:09.652570 [info ] [MainThread]: 
[0m14:34:09.652745 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:34:09.652885 [info ] [MainThread]: 
[0m14:34:09.653058 [error] [MainThread]: [31mFailure in test not_null_fct_experiments_assignments_assigned_at (models/marts/experiments/experiments.yml)[0m
[0m14:34:09.653217 [error] [MainThread]:   Got 3300 results, configured to fail if != 0
[0m14:34:09.653337 [info ] [MainThread]: 
[0m14:34:09.653484 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/experiments.yml/not_null_fct_experiments_assignments_assigned_at.sql
[0m14:34:09.653612 [info ] [MainThread]: 
[0m14:34:09.653751 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m14:34:09.655165 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.9032498, "process_in_blocks": "0", "process_kernel_time": 0.204495, "process_mem_max_rss": "156286976", "process_out_blocks": "0", "process_user_time": 1.33338}
[0m14:34:09.655382 [debug] [MainThread]: Command `dbt test` failed at 14:34:09.655343 after 0.90 seconds
[0m14:34:09.655562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106299580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062980e0>]}
[0m14:34:09.655727 [debug] [MainThread]: Flushing usage events
[0m14:34:10.185967 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:56.538549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e7530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d9d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d9d760>]}


============================== 14:34:56.550873 | 6c9e2de0-5409-4fd6-82c5-409b6c2789ff ==============================
[0m14:34:56.550873 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:56.551212 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'introspect': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'log_format': 'default', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'debug': 'False', 'partial_parse': 'True', 'write_json': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'quiet': 'False', 'printer_width': '80', 'empty': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:34:56.717130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ecc440>]}
[0m14:34:56.745633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106844e30>]}
[0m14:34:56.747075 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:56.799077 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:56.868850 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:56.869089 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:34:56.869224 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:56.891518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107028ef0>]}
[0m14:34:56.935683 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:56.937233 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:56.951630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073fdee0>]}
[0m14:34:56.951905 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:34:56.952077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a0290>]}
[0m14:34:56.953689 [info ] [MainThread]: 
[0m14:34:56.953869 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:34:56.954012 [info ] [MainThread]: 
[0m14:34:56.954261 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:34:56.956428 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:34:56.986453 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:34:56.986688 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:34:56.986835 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:57.006048 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:34:57.007053 [debug] [ThreadPool]: On list_analytics: Close
[0m14:34:57.007417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:34:57.007667 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:34:57.011059 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.011252 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:34:57.011386 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:34:57.012195 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:34:57.012807 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.012948 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:34:57.013163 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.013287 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.013409 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:34:57.013973 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.014461 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:34:57.014616 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.014748 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:34:57.014969 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.015108 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:34:57.015995 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:34:57.018841 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:57.019011 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:34:57.019138 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:34:57.019439 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.019571 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:57.019705 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:34:57.031720 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m14:34:57.032637 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:34:57.033626 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:34:57.033761 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:34:57.034900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e7e660>]}
[0m14:34:57.035126 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.035253 [debug] [MainThread]: On master: BEGIN
[0m14:34:57.035367 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:34:57.035610 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.035735 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.035855 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.035970 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.036155 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.036277 [debug] [MainThread]: On master: Close
[0m14:34:57.037738 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.037981 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:34:57.038177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:34:57.038334 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.043325 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.044609 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.060189 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.060898 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.061074 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:34:57.061236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:57.061626 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.061792 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.061973 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:34:57.080931 [debug] [Thread-1 (]: SQL status: OK in 0.019 seconds
[0m14:34:57.083860 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.084069 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:34:57.084558 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.085203 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.085381 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:34:57.085833 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.088968 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.089160 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:34:57.089992 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:57.091374 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.091543 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:34:57.091844 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.099051 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:34:57.099260 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.099425 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:34:57.101533 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:34:57.104079 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.104261 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:34:57.104995 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:57.106132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:34:57.107076 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104446f60>]}
[0m14:34:57.107395 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:34:57.107661 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.108010 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.108290 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:34:57.108645 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:34:57.108844 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.110469 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.111163 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.112721 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.113409 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.113598 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:34:57.113758 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:34:57.114147 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.114304 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.114510 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:34:57.129525 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m14:34:57.130220 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.130399 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:34:57.130870 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.131450 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.131614 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:34:57.132033 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.133737 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.133914 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:34:57.134230 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.136279 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.136445 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:34:57.136750 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.137478 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:34:57.137634 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.137770 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:34:57.140105 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:34:57.172906 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.173172 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:34:57.173699 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.174356 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:34:57.174625 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067256a0>]}
[0m14:34:57.174927 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.07s]
[0m14:34:57.175178 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.175496 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:34:57.175785 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:34:57.176073 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:34:57.176245 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:34:57.178142 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.178933 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:34:57.180486 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.181041 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.181232 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:34:57.181402 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:34:57.181774 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.181940 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.182251 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m14:34:57.193386 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m14:34:57.194064 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.194249 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m14:34:57.194714 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.195301 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.195518 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m14:34:57.195905 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.197431 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.197601 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m14:34:57.197900 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.199167 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.199349 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m14:34:57.199689 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.200474 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:34:57.200637 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.200780 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:34:57.201369 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.202512 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.202679 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m14:34:57.203082 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.203668 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:34:57.203922 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079145f0>]}
[0m14:34:57.204224 [info ] [Thread-2 (]: 3 of 3 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.03s]
[0m14:34:57.204467 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:34:57.205033 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.205186 [debug] [MainThread]: On master: BEGIN
[0m14:34:57.205310 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:34:57.205555 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.205688 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.205818 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.205936 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.206130 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.206258 [debug] [MainThread]: On master: Close
[0m14:34:57.206415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:57.206532 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:34:57.206645 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:34:57.206754 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:34:57.206909 [info ] [MainThread]: 
[0m14:34:57.207059 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m14:34:57.207458 [debug] [MainThread]: Command end result
[0m14:34:57.223856 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:57.224913 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:57.227879 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:34:57.228046 [info ] [MainThread]: 
[0m14:34:57.228224 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:34:57.228358 [info ] [MainThread]: 
[0m14:34:57.228505 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:34:57.229832 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7288726, "process_in_blocks": "0", "process_kernel_time": 0.219692, "process_mem_max_rss": "185958400", "process_out_blocks": "0", "process_user_time": 1.182765}
[0m14:34:57.230055 [debug] [MainThread]: Command `dbt run` succeeded at 14:34:57.230016 after 0.73 seconds
[0m14:34:57.230221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107563230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107561b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107563ec0>]}
[0m14:34:57.230382 [debug] [MainThread]: Flushing usage events
[0m14:34:57.336502 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:10.961546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087159a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ee5940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ee5700>]}


============================== 14:35:10.965577 | f96468e7-43aa-4036-9fed-3ac07b0327db ==============================
[0m14:35:10.965577 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:35:10.965941 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'fail_fast': 'False', 'version_check': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'no_print': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'partial_parse': 'True', 'quiet': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'log_format': 'default', 'warn_error': 'None', 'introspect': 'True', 'write_json': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True'}
[0m14:35:11.138862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108aca870>]}
[0m14:35:11.167019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090d7fe0>]}
[0m14:35:11.169175 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:35:11.220647 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:35:11.287810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:11.288061 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:35:11.288202 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:11.310526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ebbad0>]}
[0m14:35:11.354641 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:11.356101 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:11.373605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10918c800>]}
[0m14:35:11.373898 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:35:11.374079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091b3f80>]}
[0m14:35:11.375881 [info ] [MainThread]: 
[0m14:35:11.376072 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:35:11.376215 [info ] [MainThread]: 
[0m14:35:11.376462 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:35:11.378748 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:35:11.406479 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:11.406701 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:35:11.406843 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:11.420717 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m14:35:11.420961 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:11.421122 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:35:11.438346 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:35:11.439508 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:35:11.440525 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:35:11.440701 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:35:11.441820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109170d10>]}
[0m14:35:11.442062 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.442199 [debug] [MainThread]: On master: BEGIN
[0m14:35:11.442327 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:35:11.442628 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.442766 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.442897 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.443020 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.443234 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.443362 [debug] [MainThread]: On master: Close
[0m14:35:11.444920 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.445119 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.445457 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.445321 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:35:11.445661 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.445824 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:35:11.446005 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:35:11.446247 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:35:11.446471 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:35:11.446704 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:35:11.446903 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:35:11.447068 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.447262 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:35:11.447415 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.447567 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.460268 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.460519 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.463334 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.467604 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.470339 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.470709 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.480034 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.480449 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.480656 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.480842 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.482057 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.483301 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.485121 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.485343 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.485694 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:35:11.485902 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.486316 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.486518 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.486692 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.486875 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.487043 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:35:11.487204 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.487381 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:35:11.487559 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:35:11.487719 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:35:11.487900 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:35:11.488062 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:35:11.488205 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:35:11.488572 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.488781 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.488949 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.489109 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.489258 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.489438 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:35:11.489609 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.489779 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.490032 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.490979 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.491191 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.493215 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:35:11.493468 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:35:11.527279 [debug] [Thread-1 (]: SQL status: OK in 0.039 seconds
[0m14:35:11.527881 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:35:11.528353 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:35:11.529146 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:35:11.529840 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:35:11.530429 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:35:11.530861 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:35:11.531284 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:35:11.531686 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:35:11.532006 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.09s]
[0m14:35:11.532185 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:35:11.532345 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:35:11.532494 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:35:11.532717 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.533020 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.09s]
[0m14:35:11.533314 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.09s]
[0m14:35:11.533593 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.09s]
[0m14:35:11.533800 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.534026 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.534272 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.534480 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.534644 [info ] [Thread-3 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:35:11.534823 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.534983 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.535162 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.535337 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:35:11.535504 [info ] [Thread-1 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:35:11.535690 [info ] [Thread-4 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:35:11.535872 [info ] [Thread-2 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:35:11.536043 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.536216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:35:11.536382 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:35:11.536542 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:35:11.538871 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.539115 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.539319 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.539557 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.541637 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.543531 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.545316 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.545584 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.546813 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.547112 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.547292 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.547467 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.548597 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.549748 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.549968 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.551063 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.551311 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:35:11.551565 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:35:11.551904 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.552094 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.552268 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:35:11.552439 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.552613 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.552764 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:35:11.552914 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:35:11.553070 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.553219 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:35:11.553359 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.553583 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.553750 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:35:11.553902 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554232 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.554389 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554550 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.554715 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554862 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.555007 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.555208 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.555404 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.555629 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.555788 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.556506 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:35:11.557361 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:35:11.557828 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:35:11.558257 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:35:11.558419 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:35:11.558586 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:35:11.558745 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:35:11.558897 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:11.559834 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:35:11.559213 [info ] [Thread-3 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.02s]
[0m14:35:11.560131 [info ] [Thread-2 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:35:11.560859 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:35:11.561252 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:35:11.561474 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.561701 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.562123 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:35:11.562273 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:35:11.562448 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.562641 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.562806 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:35:11.563047 [info ] [Thread-4 (]: 7 of 12 PASS not_null_fct_experiments_assignments_assigned_at .................. [[32mPASS[0m in 0.03s]
[0m14:35:11.563181 [info ] [Thread-3 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:35:11.563362 [info ] [Thread-2 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:35:11.563846 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.563659 [info ] [Thread-1 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:35:11.564067 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:35:11.564241 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:35:11.564409 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.564630 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.564789 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.564937 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.565268 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.565093 [info ] [Thread-4 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:35:11.567761 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.570072 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.570313 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:35:11.570598 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:35:11.570948 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:35:11.571163 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.571331 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.571562 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.571695 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.575772 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.577639 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.578826 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.579943 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.580777 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.581094 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.582268 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.582444 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.582644 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.582852 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:35:11.584080 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.586261 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:35:11.586547 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:35:11.586844 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.587022 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:35:11.587354 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:35:11.587621 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.587780 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:35:11.587944 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588094 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588239 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:35:11.588463 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.588618 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.588771 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588912 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.589077 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.589246 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.589401 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.589766 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:35:11.590035 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590194 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590347 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590498 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.590774 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:35:11.591589 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:35:11.592643 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:35:11.593281 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:35:11.593444 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m14:35:11.593606 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:35:11.593997 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:35:11.594141 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:35:11.594873 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:35:11.595038 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:35:11.595666 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:35:11.595970 [info ] [Thread-3 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:35:11.596467 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:35:11.596744 [info ] [Thread-2 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:35:11.597215 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:35:11.597439 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.597610 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:35:11.597838 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.597991 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:35:11.598341 [info ] [Thread-4 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:35:11.598827 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.598623 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:35:11.599179 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.599890 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.600049 [debug] [MainThread]: On master: BEGIN
[0m14:35:11.600175 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:35:11.600488 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.600621 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.600746 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.600859 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.601050 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.601176 [debug] [MainThread]: On master: Close
[0m14:35:11.601340 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:11.601500 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:35:11.601628 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:35:11.601745 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:35:11.601861 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:35:11.602002 [info ] [MainThread]: 
[0m14:35:11.602140 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m14:35:11.602981 [debug] [MainThread]: Command end result
[0m14:35:11.619311 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:11.620484 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:11.624054 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:35:11.624231 [info ] [MainThread]: 
[0m14:35:11.624405 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:35:11.624546 [info ] [MainThread]: 
[0m14:35:11.624697 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m14:35:11.626125 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.69964886, "process_in_blocks": "0", "process_kernel_time": 0.180843, "process_mem_max_rss": "152420352", "process_out_blocks": "0", "process_user_time": 1.166883}
[0m14:35:11.626362 [debug] [MainThread]: Command `dbt test` succeeded at 14:35:11.626320 after 0.70 seconds
[0m14:35:11.626538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cefc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109015d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109015940>]}
[0m14:35:11.626703 [debug] [MainThread]: Flushing usage events
[0m14:35:11.717931 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:04:37.568259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048df740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d6a0>]}


============================== 15:04:37.582986 | 631f7fc4-58b8-4416-8a9b-231ddc47653f ==============================
[0m15:04:37.582986 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:04:37.583402 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'empty': 'False', 'introspect': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'partial_parse': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'write_json': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m15:04:37.755212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102647d10>]}
[0m15:04:37.783193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045bfef0>]}
[0m15:04:37.784477 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:04:37.835127 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:04:37.905436 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m15:04:37.905796 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:04:37.906085 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/experiments.yml
[0m15:04:38.103500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058a9df0>]}
[0m15:04:38.180318 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:04:38.182101 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:04:38.196785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055fcd40>]}
[0m15:04:38.197068 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:04:38.197244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105035c40>]}
[0m15:04:38.198874 [info ] [MainThread]: 
[0m15:04:38.199061 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:04:38.199200 [info ] [MainThread]: 
[0m15:04:38.199436 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:04:38.201654 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:04:38.231328 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:04:38.231570 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:04:38.231720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:04:38.250417 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:04:38.251396 [debug] [ThreadPool]: On list_analytics: Close
[0m15:04:38.251750 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:04:38.252004 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:04:38.255312 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.255601 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:04:38.255752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:04:38.256435 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:04:38.257053 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.257183 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:04:38.257395 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.257519 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.257642 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:04:38.257877 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.258250 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:04:38.258382 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.258498 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:04:38.258694 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.258821 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:04:38.259508 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:04:38.262879 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:04:38.263215 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:04:38.263462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:04:38.263852 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.263996 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:04:38.264135 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:04:38.276486 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:04:38.277349 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:04:38.278222 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:04:38.278379 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:04:38.279423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058fd280>]}
[0m15:04:38.279665 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.279805 [debug] [MainThread]: On master: BEGIN
[0m15:04:38.279926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:04:38.280243 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.280425 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.280577 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.280703 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.280921 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.281059 [debug] [MainThread]: On master: Close
[0m15:04:38.282479 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.282733 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:04:38.282950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:04:38.283116 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.286984 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.288396 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.304797 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.305918 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.306091 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:04:38.306240 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:04:38.306591 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.306736 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.306909 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:04:38.324274 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m15:04:38.327174 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.327387 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:04:38.327869 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.328490 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.328668 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:04:38.329090 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.332362 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.332558 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:04:38.334020 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.335429 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.335597 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:04:38.335877 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.343132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:04:38.343357 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.343518 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:04:38.345695 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.348639 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.348856 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:04:38.349750 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.351011 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:04:38.352010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c4aa0>]}
[0m15:04:38.352344 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:04:38.352622 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.352995 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.353226 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:04:38.353462 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:04:38.353619 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.355401 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.356515 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.358059 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.358981 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.359195 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:04:38.359367 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:04:38.359885 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.360072 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.360297 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:04:38.369731 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:04:38.370550 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.370738 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:04:38.371230 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.371805 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.371967 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:04:38.372357 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.373918 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.374088 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:04:38.374381 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.375663 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.375822 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:04:38.376093 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.376844 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:04:38.377078 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.377256 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:04:38.378988 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.380057 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.380218 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:04:38.380578 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.381156 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:04:38.381414 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c2b620>]}
[0m15:04:38.381695 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:04:38.381936 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.382280 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:04:38.382496 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.382701 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:04:38.382948 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:04:38.383197 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:04:38.383383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:04:38.383551 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:04:38.383703 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.386369 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.388094 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.388616 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.390493 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.390833 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:04:38.392445 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.392708 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.392949 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:04:38.393125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:04:38.393407 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.393638 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:04:38.393845 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:04:38.394021 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.394243 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.394541 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:04:38.395751 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.395899 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.396189 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:04:38.399826 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:04:38.400142 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:04:38.400386 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:04:38.405704 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:04:38.406009 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:04:38.407709 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:04:38.408174 [debug] [Thread-2 (]: SQL status: OK in 0.012 seconds
[0m15:04:38.408445 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c30260>]}
[0m15:04:38.409043 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.411404 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:04:38.411216 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:04:38.411836 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.412024 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.412263 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:04:38.412930 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.413441 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:04:38.413868 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.415500 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.415675 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:04:38.415995 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.417312 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.417498 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:04:38.417796 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.418834 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:04:38.419080 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.419254 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:04:38.419941 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.421146 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.421314 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:04:38.421738 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.422336 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:04:38.422599 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b769f0>]}
[0m15:04:38.422875 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:04:38.423115 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:04:38.423659 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.423807 [debug] [MainThread]: On master: BEGIN
[0m15:04:38.423929 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:04:38.424163 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.424291 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.424415 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.424532 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.424721 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.424845 [debug] [MainThread]: On master: Close
[0m15:04:38.425002 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:04:38.425117 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:04:38.425223 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:04:38.425330 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:04:38.425485 [info ] [MainThread]: 
[0m15:04:38.425634 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:04:38.426069 [debug] [MainThread]: Command end result
[0m15:04:38.442854 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:04:38.444042 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:04:38.447557 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:04:38.447747 [info ] [MainThread]: 
[0m15:04:38.447921 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:04:38.448063 [info ] [MainThread]: 
[0m15:04:38.448238 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:04:38.448410 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:04:38.448541 [info ] [MainThread]: 
[0m15:04:38.448690 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:04:38.448817 [info ] [MainThread]: 
[0m15:04:38.448954 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:04:38.450343 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9215141, "process_in_blocks": "0", "process_kernel_time": 0.223921, "process_mem_max_rss": "188858368", "process_out_blocks": "0", "process_user_time": 1.374879}
[0m15:04:38.450560 [debug] [MainThread]: Command `dbt run` failed at 15:04:38.450520 after 0.92 seconds
[0m15:04:38.450737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050c8e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c0f140>]}
[0m15:04:38.450899 [debug] [MainThread]: Flushing usage events
[0m15:04:38.571623 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:24.859384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023be690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e398b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e39640>]}


============================== 15:05:24.874126 | e0818f9d-4332-43f9-aecc-9272ccf4c6cd ==============================
[0m15:05:24.874126 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:24.874488 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None', 'log_format': 'default', 'printer_width': '80', 'version_check': 'True', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'empty': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:05:25.039576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f1ae10>]}
[0m15:05:25.068318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103246cc0>]}
[0m15:05:25.069781 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:25.122747 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:05:25.193190 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:25.193434 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:05:25.193572 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:25.215887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd7020>]}
[0m15:05:25.261015 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:25.264771 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:25.278622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055121e0>]}
[0m15:05:25.278896 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:05:25.279064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f1b650>]}
[0m15:05:25.280747 [info ] [MainThread]: 
[0m15:05:25.280926 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:25.281066 [info ] [MainThread]: 
[0m15:05:25.281309 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:05:25.283459 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:05:25.313604 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:05:25.313866 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:05:25.314196 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:25.333567 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:05:25.334553 [debug] [ThreadPool]: On list_analytics: Close
[0m15:05:25.334909 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:05:25.335134 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:05:25.339105 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.339318 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:05:25.339463 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:25.340294 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:25.340954 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.341107 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:05:25.341359 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.341796 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.341966 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:05:25.342370 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.343053 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:25.343209 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.343337 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:25.343552 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.343686 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:05:25.344473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:05:25.347238 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:25.347550 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:05:25.347693 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:25.347977 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.348119 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:25.348255 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:05:25.359657 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:05:25.360424 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:05:25.361385 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:05:25.361536 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:05:25.362477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10538d310>]}
[0m15:05:25.362728 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.362864 [debug] [MainThread]: On master: BEGIN
[0m15:05:25.362983 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:25.363291 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.363480 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.363628 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.363753 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.363962 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.364092 [debug] [MainThread]: On master: Close
[0m15:05:25.365497 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.365743 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:05:25.365950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:05:25.366112 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.369886 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.370851 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.386422 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.387341 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.387516 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:05:25.387666 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:25.388022 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.388175 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.388355 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:05:25.406129 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m15:05:25.409173 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.409426 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:25.410008 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.410707 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.410981 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:25.411536 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.415633 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.416005 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:05:25.417116 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.418710 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.418913 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:05:25.419354 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.426862 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:25.427117 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.427277 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:25.429713 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:05:25.432589 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.432788 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:05:25.433490 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.434707 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:05:25.435675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502a660>]}
[0m15:05:25.436003 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:05:25.436272 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.436629 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.436910 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:05:25.437197 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:05:25.437367 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.439167 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.440199 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.443030 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.475948 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.476234 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:05:25.476405 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:05:25.476783 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.476945 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.477184 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:05:25.487084 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m15:05:25.487845 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.488081 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:25.488682 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.489442 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.490118 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:25.490679 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.496352 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.496586 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:05:25.496998 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.498469 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.498645 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:05:25.498940 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.499729 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:25.499890 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.500033 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:25.501699 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:05:25.502952 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.503146 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:05:25.503591 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.504236 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:05:25.504501 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059e9be0>]}
[0m15:05:25.504795 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.07s]
[0m15:05:25.505052 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.505410 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:05:25.505643 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.506020 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:05:25.506303 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:05:25.506612 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:05:25.506847 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:05:25.507018 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:05:25.507182 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.509328 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.511147 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.512257 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:05:25.512479 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.514503 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.516202 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.517108 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.517306 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.517475 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:05:25.517647 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:05:25.517812 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:05:25.517964 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:25.518349 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.518503 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.518655 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.518953 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:05:25.519253 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.520527 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:05:25.524615 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:05:25.525005 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:05:25.525245 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:05:25.530632 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m15:05:25.531341 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.531656 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:05:25.532468 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:05:25.532756 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:05:25.533491 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.534322 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:05:25.535073 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.535266 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105601c40>]}
[0m15:05:25.535480 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:05:25.535815 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:05:25.536172 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.536412 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:05:25.536595 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.538580 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.538756 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:05:25.539093 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.541541 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.541730 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:05:25.542050 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.542949 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:05:25.543156 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.543320 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:05:25.543946 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.546084 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.546309 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:05:25.546895 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.547579 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:05:25.547851 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a2c4d0>]}
[0m15:05:25.548136 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:05:25.548387 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:05:25.548962 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.549123 [debug] [MainThread]: On master: BEGIN
[0m15:05:25.549251 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:05:25.549530 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.549659 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.549780 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.549896 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.550093 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.550217 [debug] [MainThread]: On master: Close
[0m15:05:25.550379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:05:25.550938 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:05:25.551101 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:05:25.551270 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:05:25.551455 [info ] [MainThread]: 
[0m15:05:25.551678 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:05:25.552146 [debug] [MainThread]: Command end result
[0m15:05:25.574110 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:25.575350 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:25.578816 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:05:25.578999 [info ] [MainThread]: 
[0m15:05:25.579177 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:05:25.579317 [info ] [MainThread]: 
[0m15:05:25.579492 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:05:25.579662 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:05:25.579791 [info ] [MainThread]: 
[0m15:05:25.579942 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:05:25.580071 [info ] [MainThread]: 
[0m15:05:25.580209 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:05:25.581585 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7585153, "process_in_blocks": "0", "process_kernel_time": 0.214373, "process_mem_max_rss": "194756608", "process_out_blocks": "0", "process_user_time": 1.22754}
[0m15:05:25.581790 [debug] [MainThread]: Command `dbt run` failed at 15:05:25.581751 after 0.76 seconds
[0m15:05:25.581967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d19a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d34a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d2120>]}
[0m15:05:25.582130 [debug] [MainThread]: Flushing usage events
[0m15:05:25.679547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:59.328048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e51910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068dbc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106941640>]}


============================== 15:05:59.334961 | 624fcad9-0504-4ace-b1e5-67339e4074e6 ==============================
[0m15:05:59.334961 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:59.335349 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'fail_fast': 'False', 'introspect': 'True', 'no_print': 'None', 'printer_width': '80', 'debug': 'False', 'version_check': 'True', 'cache_selected_only': 'False'}
[0m15:05:59.537947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069c0350>]}
[0m15:05:59.569464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106283e60>]}
[0m15:05:59.570838 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:59.625110 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:05:59.694162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:59.694412 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:05:59.694602 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:59.717623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106be8560>]}
[0m15:05:59.761066 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:59.762453 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:59.774981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f9c290>]}
[0m15:05:59.775254 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:05:59.775427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069aa0f0>]}
[0m15:05:59.777097 [info ] [MainThread]: 
[0m15:05:59.777268 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:59.777399 [info ] [MainThread]: 
[0m15:05:59.777633 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:05:59.779916 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:05:59.808995 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:05:59.809228 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:05:59.809372 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:59.826949 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:05:59.827955 [debug] [ThreadPool]: On list_analytics: Close
[0m15:05:59.828327 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:05:59.828556 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:05:59.832260 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.832442 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:05:59.832575 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:59.833446 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:59.834092 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.834232 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:05:59.834461 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.834591 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.834717 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:05:59.835201 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.835578 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:59.835713 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.835827 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:59.836019 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.836145 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:05:59.836838 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:05:59.839448 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:59.839600 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:05:59.839718 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:59.839947 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.840071 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:59.840198 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:05:59.850965 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:05:59.851737 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:05:59.852718 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:05:59.852880 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:05:59.853875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a1e930>]}
[0m15:05:59.854103 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:59.854240 [debug] [MainThread]: On master: BEGIN
[0m15:05:59.854362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:59.854616 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:59.854746 [debug] [MainThread]: On master: COMMIT
[0m15:05:59.854868 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:59.854986 [debug] [MainThread]: On master: COMMIT
[0m15:05:59.855183 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:59.855304 [debug] [MainThread]: On master: Close
[0m15:05:59.856581 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.856828 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:05:59.857029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:05:59.857193 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.860904 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.861375 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.876771 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.877325 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.877496 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:05:59.877645 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:59.878056 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.878283 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.878538 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:05:59.896053 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m15:05:59.898834 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.899032 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:59.899528 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.900132 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.900297 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:59.900710 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.903762 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.903946 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:05:59.904734 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.906152 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.906325 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:05:59.906617 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.913513 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:59.913700 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.913855 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:59.915576 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:05:59.918226 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.918423 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:05:59.919238 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.920404 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:05:59.921328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074262d0>]}
[0m15:05:59.921645 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.06s]
[0m15:05:59.921914 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.922306 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.922603 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:05:59.922893 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:05:59.923066 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.924791 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.925205 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.927599 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.960283 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.960579 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:05:59.960749 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:05:59.961137 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.961309 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.961518 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:05:59.970698 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:05:59.971470 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.971671 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:59.972144 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.972750 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.972919 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:59.973348 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.974978 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.975156 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:05:59.975465 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.976793 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.976965 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:05:59.977238 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.978031 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:59.978216 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.978373 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:59.979572 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.980826 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.981003 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:05:59.981410 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.982008 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:05:59.982266 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073879e0>]}
[0m15:05:59.982553 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.06s]
[0m15:05:59.982814 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.983161 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:05:59.983353 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.983596 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:05:59.983891 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:05:59.984184 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:05:59.984431 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:05:59.984611 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:05:59.984785 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.986901 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.988660 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.991574 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:05:59.991762 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.993353 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.994971 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.995492 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.995808 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.996013 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:05:59.996187 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:05:59.996404 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:05:59.996711 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:59.997199 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.997402 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.997573 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.997878 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:05:59.998399 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:00.000174 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:00.004475 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:00.004891 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:06:00.005143 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:06:00.008985 [debug] [Thread-2 (]: SQL status: OK in 0.010 seconds
[0m15:06:00.009649 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.009828 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:00.010427 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.011204 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:06:00.011911 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.012110 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:06:00.012301 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:00.013057 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:00.014714 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.014892 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:06:00.015726 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:06:00.015979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074d69c0>]}
[0m15:06:00.016153 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:00.016452 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:06:00.018861 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.019139 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:00.019321 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:06:00.019565 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:06:00.020150 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.020918 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:00.021077 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.021224 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:00.021787 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.022876 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.023049 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:06:00.023493 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.024082 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:06:00.024331 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107424e30>]}
[0m15:06:00.024617 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:06:00.024862 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:06:00.025415 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:00.025564 [debug] [MainThread]: On master: BEGIN
[0m15:06:00.025685 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:06:00.025941 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:00.026071 [debug] [MainThread]: On master: COMMIT
[0m15:06:00.026192 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:00.026308 [debug] [MainThread]: On master: COMMIT
[0m15:06:00.026495 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:00.026618 [debug] [MainThread]: On master: Close
[0m15:06:00.026774 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:06:00.026889 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:06:00.026995 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:06:00.027102 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:06:00.027258 [info ] [MainThread]: 
[0m15:06:00.027402 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m15:06:00.027858 [debug] [MainThread]: Command end result
[0m15:06:00.044197 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:00.045304 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:00.048427 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:06:00.048610 [info ] [MainThread]: 
[0m15:06:00.048781 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:06:00.048923 [info ] [MainThread]: 
[0m15:06:00.049100 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:06:00.049271 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:06:00.049402 [info ] [MainThread]: 
[0m15:06:00.049549 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:00.049678 [info ] [MainThread]: 
[0m15:06:00.049815 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:06:00.051216 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7602901, "process_in_blocks": "0", "process_kernel_time": 0.206532, "process_mem_max_rss": "188563456", "process_out_blocks": "0", "process_user_time": 1.207264}
[0m15:06:00.051451 [debug] [MainThread]: Command `dbt run` failed at 15:06:00.051411 after 0.76 seconds
[0m15:06:00.051634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046356d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074e62a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074e42c0>]}
[0m15:06:00.051796 [debug] [MainThread]: Flushing usage events
[0m15:06:00.167628 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:06:41.859050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6ea20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f65850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f655e0>]}


============================== 15:06:41.876552 | 89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4 ==============================
[0m15:06:41.876552 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:06:41.876883 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'empty': 'False', 'introspect': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'use_experimental_parser': 'False', 'target_path': 'None', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'fail_fast': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'no_print': 'None', 'cache_selected_only': 'False'}
[0m15:06:42.054714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fcc590>]}
[0m15:06:42.083315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071035c0>]}
[0m15:06:42.084761 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:06:42.137188 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:06:42.207679 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:06:42.208095 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:42.401054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075b1a30>]}
[0m15:06:42.480177 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:42.481735 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:42.495848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077849e0>]}
[0m15:06:42.496132 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:06:42.496315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078c9340>]}
[0m15:06:42.498005 [info ] [MainThread]: 
[0m15:06:42.498182 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:06:42.498321 [info ] [MainThread]: 
[0m15:06:42.498541 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:06:42.500745 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:06:42.529834 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:06:42.530075 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:06:42.530218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:06:42.547725 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:06:42.548668 [debug] [ThreadPool]: On list_analytics: Close
[0m15:06:42.549018 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:06:42.549270 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:06:42.552518 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.552687 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:06:42.552820 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:42.553544 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:06:42.554329 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.554594 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:06:42.554868 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.555006 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.555140 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:06:42.555523 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.555916 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:06:42.556052 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.556174 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:06:42.556380 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.556515 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:06:42.557211 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:06:42.559813 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:06:42.559974 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:06:42.560151 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:42.560388 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.560521 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:06:42.560653 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:06:42.570895 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:06:42.571822 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:06:42.572724 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:06:42.572883 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:06:42.573708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a23770>]}
[0m15:06:42.573935 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.574067 [debug] [MainThread]: On master: BEGIN
[0m15:06:42.574184 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:42.574424 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.574556 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.574679 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.574794 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.574979 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.575098 [debug] [MainThread]: On master: Close
[0m15:06:42.576470 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.576702 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:06:42.576897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:06:42.577056 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.580999 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.582029 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.597618 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.598669 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.598861 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:06:42.599017 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:42.599377 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.599537 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.599723 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:06:42.618388 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m15:06:42.621241 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.621439 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:06:42.621949 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.623361 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.623537 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:06:42.623962 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.627069 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.627254 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:06:42.628134 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.629638 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.629826 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:06:42.630141 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.637291 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:06:42.637514 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.637681 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:06:42.639992 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:06:42.643118 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.643354 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:06:42.644059 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.645262 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:06:42.646254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104612db0>]}
[0m15:06:42.646567 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:06:42.646835 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.647218 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.647482 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:06:42.647724 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:06:42.647888 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.649582 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.650416 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.651923 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.652530 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.652699 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:06:42.652900 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:06:42.653290 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.653448 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.653681 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:06:42.662871 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:06:42.663558 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.663735 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:06:42.664213 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.664803 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.665029 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:06:42.665447 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.666969 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.667139 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:06:42.667435 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.668732 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.668898 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:06:42.669168 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.669924 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:06:42.670200 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.670344 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:06:42.671939 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.673019 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.673170 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:06:42.673550 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.674208 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:06:42.674474 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd73e0>]}
[0m15:06:42.674760 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:06:42.675016 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.675354 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:06:42.675561 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.675795 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:06:42.676249 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:06:42.675997 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:06:42.676454 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:06:42.676634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:06:42.678547 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.678784 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.681622 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.682333 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.684250 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.684591 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:06:42.686128 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.686436 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.686611 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:06:42.686763 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:42.687090 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.687258 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:06:42.687414 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.687553 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:06:42.687700 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.688040 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:42.689236 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:06:42.689385 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.689660 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:06:42.694918 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:42.695387 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:06:42.695721 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:06:42.700962 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:06:42.701196 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:06:42.701380 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m15:06:42.701992 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.702172 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:42.703515 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^
[0m15:06:42.703802 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.703989 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c071a0>]}
[0m15:06:42.704622 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.705104 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:42.704951 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:06:42.705476 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.705730 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^.
[0m15:06:42.706283 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.708119 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.708324 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:06:42.708701 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.710083 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.713907 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:06:42.714528 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.715425 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:42.715600 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.715746 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:42.716328 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.717463 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.717634 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:06:42.718035 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.718622 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:06:42.718871 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c17020>]}
[0m15:06:42.719144 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:06:42.719385 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:06:42.719938 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.720088 [debug] [MainThread]: On master: BEGIN
[0m15:06:42.720212 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:06:42.720452 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.720582 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.720705 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.720819 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.721004 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.721129 [debug] [MainThread]: On master: Close
[0m15:06:42.721291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:06:42.721407 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:06:42.721511 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:06:42.721613 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:06:42.721765 [info ] [MainThread]: 
[0m15:06:42.721902 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m15:06:42.722361 [debug] [MainThread]: Command end result
[0m15:06:42.738961 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:42.740061 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:42.743290 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:06:42.743447 [info ] [MainThread]: 
[0m15:06:42.743609 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:06:42.743746 [info ] [MainThread]: 
[0m15:06:42.743921 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:06:42.744097 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^
[0m15:06:42.744240 [info ] [MainThread]: 
[0m15:06:42.744387 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:42.744518 [info ] [MainThread]: 
[0m15:06:42.744656 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:06:42.746063 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9248138, "process_in_blocks": "0", "process_kernel_time": 0.219528, "process_mem_max_rss": "191545344", "process_out_blocks": "0", "process_user_time": 1.368118}
[0m15:06:42.746275 [debug] [MainThread]: Command `dbt run` failed at 15:06:42.746236 after 0.93 seconds
[0m15:06:42.746446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6ea20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f655e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b5f950>]}
[0m15:06:42.746604 [debug] [MainThread]: Flushing usage events
[0m15:06:42.862855 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:14.481433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102026630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a699d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a69760>]}


============================== 15:07:14.494850 | 4acb4e33-df64-4d6d-9366-61a8597187a7 ==============================
[0m15:07:14.494850 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:07:14.495252 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/experiments', 'version_check': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'log_format': 'default', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'introspect': 'True', 'printer_width': '80', 'use_colors': 'True', 'target_path': 'None', 'debug': 'False', 'no_print': 'None', 'write_json': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt'}
[0m15:07:14.662898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041f5880>]}
[0m15:07:14.691639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102875d00>]}
[0m15:07:14.693065 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:14.746332 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:07:14.815028 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:07:14.815459 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:07:15.007936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c94470>]}
[0m15:07:15.082952 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:15.084364 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:15.098528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aea360>]}
[0m15:07:15.098828 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:07:15.099003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c65670>]}
[0m15:07:15.100678 [info ] [MainThread]: 
[0m15:07:15.100861 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:07:15.101003 [info ] [MainThread]: 
[0m15:07:15.101236 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:07:15.103419 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:07:15.132269 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:07:15.132512 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:07:15.132678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:07:15.151311 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:07:15.152275 [debug] [ThreadPool]: On list_analytics: Close
[0m15:07:15.152633 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:07:15.152864 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:07:15.156155 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.156326 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:07:15.156457 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:15.157277 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:07:15.157984 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.158140 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:07:15.158378 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.158507 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.158632 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:07:15.159056 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.159485 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:07:15.159626 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.159749 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:07:15.159956 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.160088 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:07:15.160772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:07:15.163496 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:15.163654 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:07:15.163841 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:15.164091 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.164218 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:15.164347 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:07:15.174887 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:07:15.175906 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:07:15.176871 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:07:15.177040 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:07:15.177907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aea000>]}
[0m15:07:15.178151 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.178292 [debug] [MainThread]: On master: BEGIN
[0m15:07:15.178416 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:15.178664 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.178793 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.178918 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.179036 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.179238 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.179362 [debug] [MainThread]: On master: Close
[0m15:07:15.180635 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.180871 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:07:15.181069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:07:15.181227 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.185052 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.185743 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.201496 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.202232 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.202416 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:07:15.202568 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:15.202926 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.203069 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.203245 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:07:15.223041 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m15:07:15.225904 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.226112 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:07:15.226629 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.228071 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.228255 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:07:15.228699 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.231993 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.232206 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:07:15.235267 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:07:15.236753 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.236928 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:07:15.237218 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.244787 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:07:15.245039 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.245202 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:07:15.247273 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:07:15.250157 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.250356 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:07:15.251521 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.252714 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:07:15.253805 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102112ea0>]}
[0m15:07:15.254119 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:07:15.254383 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.254732 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.254992 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:07:15.255284 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:07:15.255460 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.257079 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.257493 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.259026 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.259477 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.259649 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:07:15.259799 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:07:15.260352 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.260564 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.260783 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:07:15.270230 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:07:15.270945 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.271129 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:07:15.271665 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.272340 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.272592 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:07:15.273026 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.274588 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.274758 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:07:15.275057 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.276375 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.276548 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:07:15.276838 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.277602 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:07:15.277761 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.277906 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:07:15.279049 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.280200 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.280374 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:07:15.280767 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.281357 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:07:15.281608 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111037ec0>]}
[0m15:07:15.281883 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:07:15.282129 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.282450 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:07:15.282635 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.282880 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:07:15.283124 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:07:15.283369 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:07:15.283552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:07:15.283718 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:07:15.283874 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.285786 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.288487 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.289033 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.289209 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:07:15.291210 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.292822 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.293376 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.293577 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.293741 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:07:15.293904 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:07:15.294055 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:07:15.294200 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:15.294566 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.294724 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.294877 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.295171 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:07:15.295516 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.296914 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) 
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:07:15.311283 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m15:07:15.311551 [debug] [Thread-2 (]: SQL status: OK in 0.016 seconds
[0m15:07:15.315967 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.316533 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.316731 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */
alter table "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp" rename to "rpt_experiment_results_by_channel"
[0m15:07:15.316913 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:07:15.317407 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.318123 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: COMMIT
[0m15:07:15.318289 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.318453 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.319053 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.319212 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: COMMIT
[0m15:07:15.319368 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:07:15.319877 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.321018 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.321197 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

      drop table if exists "analytics"."main"."rpt_experiment_results_by_channel__dbt_backup" cascade
    
[0m15:07:15.321448 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:07:15.321636 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.322225 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:07:15.322498 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11102ce30>]}
[0m15:07:15.324264 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.322797 [info ] [Thread-1 (]: 4 of 4 OK created sql table model main.rpt_experiment_results_by_channel ....... [[32mOK[0m in 0.04s]
[0m15:07:15.324486 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:07:15.324732 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.325086 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.326545 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.326992 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:07:15.327293 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.328291 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:07:15.328445 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.328582 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:07:15.329077 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.330294 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.330464 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:07:15.330989 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.331600 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:07:15.331852 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f77e00>]}
[0m15:07:15.332116 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.05s]
[0m15:07:15.332351 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:07:15.332917 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.333064 [debug] [MainThread]: On master: BEGIN
[0m15:07:15.333182 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:07:15.333429 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.333559 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.333681 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.333792 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.333975 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.334098 [debug] [MainThread]: On master: Close
[0m15:07:15.334253 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:07:15.334369 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:07:15.334478 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:07:15.334588 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:07:15.334744 [info ] [MainThread]: 
[0m15:07:15.334885 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:07:15.335318 [debug] [MainThread]: Command end result
[0m15:07:15.355002 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:15.356091 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:15.359160 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:07:15.359328 [info ] [MainThread]: 
[0m15:07:15.359511 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:15.359653 [info ] [MainThread]: 
[0m15:07:15.359808 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m15:07:15.361125 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9408884, "process_in_blocks": "0", "process_kernel_time": 0.218551, "process_mem_max_rss": "199000064", "process_out_blocks": "0", "process_user_time": 1.357449}
[0m15:07:15.361336 [debug] [MainThread]: Command `dbt run` succeeded at 15:07:15.361298 after 0.94 seconds
[0m15:07:15.361508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d49250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a69a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d11610>]}
[0m15:07:15.361674 [debug] [MainThread]: Flushing usage events
[0m15:07:15.476653 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:33.564998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12943da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a5b59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a5b5670>]}


============================== 15:07:33.570425 | 7fb88111-30fb-4a14-983d-1d14e119028c ==============================
[0m15:07:33.570425 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:07:33.570830 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'empty': 'None', 'write_json': 'True', 'version_check': 'True', 'log_format': 'default', 'debug': 'False', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'introspect': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'indirect_selection': 'eager', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'fail_fast': 'False', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None'}
[0m15:07:33.739940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a54dfa0>]}
[0m15:07:33.767684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6522d0>]}
[0m15:07:33.769648 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:33.820941 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:07:33.890978 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:07:33.891234 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:07:33.891375 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:07:33.913782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b141010>]}
[0m15:07:33.959968 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:33.961277 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:33.979523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b7f44a0>]}
[0m15:07:33.979797 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:07:33.979958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b72ca10>]}
[0m15:07:33.981822 [info ] [MainThread]: 
[0m15:07:33.982006 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:07:33.982146 [info ] [MainThread]: 
[0m15:07:33.982388 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:07:33.984671 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m15:07:34.016576 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:34.016805 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:07:34.016941 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:07:34.030365 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:07:34.030576 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:34.030731 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:07:34.048434 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:07:34.049524 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:07:34.050517 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:07:34.050685 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:07:34.051603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b7140e0>]}
[0m15:07:34.051828 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.051956 [debug] [MainThread]: On master: BEGIN
[0m15:07:34.052075 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:34.052351 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.052483 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.052608 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.052728 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.052927 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.053051 [debug] [MainThread]: On master: Close
[0m15:07:34.054618 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.054818 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.055175 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.055343 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.055038 [info ] [Thread-1 (]: 1 of 14 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m15:07:34.055558 [info ] [Thread-2 (]: 2 of 14 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m15:07:34.055714 [info ] [Thread-3 (]: 3 of 14 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m15:07:34.055937 [info ] [Thread-4 (]: 4 of 14 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m15:07:34.056182 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m15:07:34.056407 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m15:07:34.056612 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m15:07:34.056809 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m15:07:34.056974 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.057133 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.057282 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.057429 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.067104 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.069801 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.073590 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.075357 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.077028 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.083616 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.085490 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.085690 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.087528 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.087699 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.120735 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.121978 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.122418 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.122596 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m15:07:34.122816 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.123036 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:07:34.123247 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m15:07:34.123438 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.123619 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.123911 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.124088 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m15:07:34.124249 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.124396 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m15:07:34.124608 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:07:34.124770 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.124925 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.125059 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:07:34.125270 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.125442 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.125595 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.125900 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m15:07:34.126082 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.126233 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.126466 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.126645 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m15:07:34.126847 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.127037 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.129413 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m15:07:34.129643 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.129820 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.130232 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m15:07:34.130387 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.131112 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m15:07:34.131285 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m15:07:34.131932 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m15:07:34.132575 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m15:07:34.132995 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m15:07:34.133718 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m15:07:34.133315 [info ] [Thread-4 (]: 4 of 14 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.08s]
[0m15:07:34.134186 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m15:07:34.134346 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m15:07:34.134519 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m15:07:34.134754 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.134908 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m15:07:34.135170 [info ] [Thread-3 (]: 3 of 14 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.08s]
[0m15:07:34.135582 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.135377 [info ] [Thread-1 (]: 1 of 14 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.08s]
[0m15:07:34.136090 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.135895 [info ] [Thread-2 (]: 2 of 14 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.08s]
[0m15:07:34.136303 [info ] [Thread-4 (]: 5 of 14 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m15:07:34.136538 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.136712 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.136943 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.137127 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m15:07:34.137307 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.137722 [info ] [Thread-3 (]: 6 of 14 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m15:07:34.137950 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.138130 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.138295 [info ] [Thread-1 (]: 7 of 14 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m15:07:34.138532 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m15:07:34.138720 [info ] [Thread-2 (]: 8 of 14 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m15:07:34.141211 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.141530 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m15:07:34.141887 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.142070 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m15:07:34.142290 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.144239 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.144453 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.146293 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.148730 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.148989 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.150323 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.150575 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.151767 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.151976 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.153122 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.153327 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.154359 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.154578 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.154749 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m15:07:34.154946 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.155248 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.155439 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.155600 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m15:07:34.155766 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.155923 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.156076 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m15:07:34.156222 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:07:34.156363 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m15:07:34.156508 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.156648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.156845 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.157014 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.157181 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157541 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157727 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157887 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.158051 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.158202 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.158344 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.158517 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.158697 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.158907 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.159624 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m15:07:34.160293 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m15:07:34.160474 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m15:07:34.160635 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.160783 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.161186 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.161046 [info ] [Thread-4 (]: 5 of 14 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.02s]
[0m15:07:34.161922 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m15:07:34.162709 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m15:07:34.163350 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m15:07:34.163611 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.164072 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m15:07:34.164476 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m15:07:34.164886 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m15:07:34.165076 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.165262 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m15:07:34.165423 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m15:07:34.165575 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m15:07:34.165736 [info ] [Thread-4 (]: 9 of 14 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m15:07:34.166042 [info ] [Thread-1 (]: 7 of 14 PASS not_null_fct_experiments_assignments_assigned_at .................. [[32mPASS[0m in 0.02s]
[0m15:07:34.166262 [info ] [Thread-3 (]: 6 of 14 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m15:07:34.166561 [info ] [Thread-2 (]: 8 of 14 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m15:07:34.166770 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m15:07:34.167004 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.167220 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.167428 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.167576 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.167741 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.167916 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.168100 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.171085 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.171284 [info ] [Thread-1 (]: 10 of 14 START test not_null_rpt_experiment_results_by_channel_acquisition_channel  [RUN]
[0m15:07:34.171485 [info ] [Thread-3 (]: 11 of 14 START test not_null_rpt_experiment_results_by_channel_conversion_event  [RUN]
[0m15:07:34.171707 [info ] [Thread-2 (]: 12 of 14 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m15:07:34.171971 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525)
[0m15:07:34.172139 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7)
[0m15:07:34.172295 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m15:07:34.172449 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.172631 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.172801 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.174719 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.176514 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.178269 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.178514 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.179816 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.180163 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.181510 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.181778 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.181975 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.183097 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.184284 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.184488 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.184700 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.184903 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: BEGIN
[0m15:07:34.185079 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m15:07:34.185273 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.185431 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.185601 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.185904 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.186134 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: BEGIN
[0m15:07:34.186305 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.186464 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m15:07:34.186617 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.186758 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.186917 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.187064 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:07:34.187213 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.187424 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results_by_channel"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.187629 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.187800 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.187969 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.188194 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.188418 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.188613 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select acquisition_channel
from "analytics"."main"."rpt_experiment_results_by_channel"
where acquisition_channel is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.188816 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.188980 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.189131 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.190249 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m15:07:34.190888 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: ROLLBACK
[0m15:07:34.191394 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m15:07:34.191576 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.191738 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.192173 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7'
[0m15:07:34.195710 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m15:07:34.196486 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: ROLLBACK
[0m15:07:34.196665 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: Close
[0m15:07:34.197307 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m15:07:34.198081 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525'
[0m15:07:34.197663 [info ] [Thread-4 (]: 9 of 14 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m15:07:34.198806 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m15:07:34.198977 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: Close
[0m15:07:34.198340 [info ] [Thread-3 (]: 11 of 14 PASS not_null_rpt_experiment_results_by_channel_conversion_event ...... [[32mPASS[0m in 0.03s]
[0m15:07:34.199254 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.199413 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m15:07:34.199856 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.199663 [info ] [Thread-1 (]: 10 of 14 PASS not_null_rpt_experiment_results_by_channel_acquisition_channel ... [[32mPASS[0m in 0.03s]
[0m15:07:34.200088 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.200483 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.200347 [info ] [Thread-2 (]: 12 of 14 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m15:07:34.200755 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.200925 [info ] [Thread-4 (]: 13 of 14 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m15:07:34.201100 [info ] [Thread-3 (]: 14 of 14 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m15:07:34.201338 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.201551 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m15:07:34.201733 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m15:07:34.201927 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.202084 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.205445 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.207310 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.207996 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.210264 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.210466 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.211583 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.211870 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.212039 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m15:07:34.212198 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.212507 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.212693 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m15:07:34.212860 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.213027 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.213245 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.213463 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:07:34.213632 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.213865 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.214029 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:07:34.216673 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.216967 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.217774 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m15:07:34.218468 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m15:07:34.218917 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m15:07:34.219343 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m15:07:34.219496 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m15:07:34.219673 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m15:07:34.220005 [info ] [Thread-3 (]: 14 of 14 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.02s]
[0m15:07:34.220479 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.220221 [info ] [Thread-4 (]: 13 of 14 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.02s]
[0m15:07:34.220793 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.221370 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.221510 [debug] [MainThread]: On master: BEGIN
[0m15:07:34.221625 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:07:34.221919 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.222046 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.222167 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.222278 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.222470 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.222589 [debug] [MainThread]: On master: Close
[0m15:07:34.222744 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:07:34.222851 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525' was properly closed.
[0m15:07:34.222954 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m15:07:34.223058 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m15:07:34.223157 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m15:07:34.223283 [info ] [MainThread]: 
[0m15:07:34.223412 [info ] [MainThread]: Finished running 14 data tests in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m15:07:34.224289 [debug] [MainThread]: Command end result
[0m15:07:34.240364 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:34.241538 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:34.244869 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:07:34.245042 [info ] [MainThread]: 
[0m15:07:34.245221 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:34.245364 [info ] [MainThread]: 
[0m15:07:34.245512 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=14
[0m15:07:34.246970 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.7206431, "process_in_blocks": "0", "process_kernel_time": 0.194477, "process_mem_max_rss": "151863296", "process_out_blocks": "0", "process_user_time": 1.183287}
[0m15:07:34.247225 [debug] [MainThread]: Command `dbt test` succeeded at 15:07:34.247180 after 0.72 seconds
[0m15:07:34.247407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1293f7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a470920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b140920>]}
[0m15:07:34.247580 [debug] [MainThread]: Flushing usage events
[0m15:07:34.359190 [debug] [MainThread]: An error was encountered while trying to flush usage events
