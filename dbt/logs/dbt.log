[0m15:05:10.018930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107eaa8a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c31520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c312e0>]}


============================== 15:05:10.021669 | 7199bb48-94c9-4f9f-b45f-3f98070f9984 ==============================
[0m15:05:10.021669 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:10.021985 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'static_parser': 'True', 'introspect': 'True', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'log_format': 'default', 'use_colors': 'True', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'target_path': 'None', 'version_check': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt debug'}
[0m15:05:10.030122 [info ] [MainThread]: dbt version: 1.11.4
[0m15:05:10.030436 [info ] [MainThread]: python version: 3.12.12
[0m15:05:10.030914 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:05:10.031202 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:05:10.085151 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:05:10.085441 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:05:10.085615 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:05:10.086781 [info ] [MainThread]: adapter type: duckdb
[0m15:05:10.086960 [info ] [MainThread]: adapter version: 1.10.0
[0m15:05:10.088732 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `data-paths` config has been renamed to `seed-paths`. Please update your
`dbt_project.yml` configuration to reflect this change.
[0m15:05:10.088934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '7199bb48-94c9-4f9f-b45f-3f98070f9984', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d7cc20>]}
[0m15:05:10.118647 [info ] [MainThread]: Configuration:
[0m15:05:10.118945 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:05:10.119102 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m15:05:10.119233 [info ] [MainThread]: Required dependencies:
[0m15:05:10.119453 [debug] [MainThread]: Executing "git --help"
[0m15:05:10.140509 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:05:10.141097 [debug] [MainThread]: STDERR: "b''"
[0m15:05:10.141295 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:05:10.141469 [info ] [MainThread]: Connection:
[0m15:05:10.141640 [info ] [MainThread]:   database: analytics
[0m15:05:10.141770 [info ] [MainThread]:   schema: main
[0m15:05:10.141898 [info ] [MainThread]:   path: Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:05:10.142022 [info ] [MainThread]:   config_options: None
[0m15:05:10.142141 [info ] [MainThread]:   extensions: None
[0m15:05:10.142255 [info ] [MainThread]:   settings: {}
[0m15:05:10.142373 [info ] [MainThread]:   external_root: .
[0m15:05:10.142486 [info ] [MainThread]:   use_credential_provider: None
[0m15:05:10.142598 [info ] [MainThread]:   attach: None
[0m15:05:10.142716 [info ] [MainThread]:   filesystems: None
[0m15:05:10.142826 [info ] [MainThread]:   remote: None
[0m15:05:10.142935 [info ] [MainThread]:   plugins: None
[0m15:05:10.143044 [info ] [MainThread]:   disable_transactions: False
[0m15:05:10.143398 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:10.366959 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:05:10.390493 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:05:10.390729 [debug] [MainThread]: On debug: select 1 as id
[0m15:05:10.390881 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:10.394680 [debug] [MainThread]: DuckDB adapter: Error running SQL: select 1 as id
[0m15:05:10.394849 [debug] [MainThread]: DuckDB adapter: Rolling back transaction.
[0m15:05:10.395055 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m15:05:10.395252 [info ] [MainThread]: [31m2 checks failed:[0m
[0m15:05:10.395385 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml


[0m15:05:10.395522 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  IO Error: Cannot open file "/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb": No such file or directory

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m15:05:10.395804 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ConfigDataPathDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:05:10.522773 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.5422592, "process_in_blocks": "0", "process_kernel_time": 0.157381, "process_mem_max_rss": "130727936", "process_out_blocks": "0", "process_user_time": 0.821021}
[0m15:05:10.523204 [debug] [MainThread]: Command `dbt debug` failed at 15:05:10.523137 after 0.54 seconds
[0m15:05:10.523412 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:05:10.523606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c313a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dfd1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108da5b20>]}
[0m15:05:10.523807 [debug] [MainThread]: Flushing usage events
[0m15:05:10.660077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:09:14.997574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dd6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109b5580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109b5370>]}


============================== 15:09:15.001893 | 53188ec5-48da-491f-8822-fa0cadb9d08a ==============================
[0m15:09:15.001893 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:09:15.002210 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'invocation_command': 'dbt debug', 'warn_error': 'None', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'fail_fast': 'False', 'target_path': 'None', 'printer_width': '80', 'cache_selected_only': 'False', 'empty': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'partial_parse': 'True', 'debug': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:09:15.010973 [info ] [MainThread]: dbt version: 1.11.4
[0m15:09:15.011387 [info ] [MainThread]: python version: 3.12.12
[0m15:09:15.011574 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:09:15.011718 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:09:15.121653 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:09:15.121960 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:09:15.122119 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:09:15.123283 [info ] [MainThread]: adapter type: duckdb
[0m15:09:15.123476 [info ] [MainThread]: adapter version: 1.10.0
[0m15:09:15.154099 [info ] [MainThread]: Configuration:
[0m15:09:15.154389 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:09:15.154537 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m15:09:15.154667 [info ] [MainThread]: Required dependencies:
[0m15:09:15.154879 [debug] [MainThread]: Executing "git --help"
[0m15:09:15.169182 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:09:15.169793 [debug] [MainThread]: STDERR: "b''"
[0m15:09:15.169996 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:09:15.170161 [info ] [MainThread]: Connection:
[0m15:09:15.170326 [info ] [MainThread]:   database: analytics
[0m15:09:15.170454 [info ] [MainThread]:   schema: main
[0m15:09:15.170577 [info ] [MainThread]:   path: /Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:09:15.170698 [info ] [MainThread]:   config_options: None
[0m15:09:15.170813 [info ] [MainThread]:   extensions: None
[0m15:09:15.170964 [info ] [MainThread]:   settings: {}
[0m15:09:15.171129 [info ] [MainThread]:   external_root: .
[0m15:09:15.171260 [info ] [MainThread]:   use_credential_provider: None
[0m15:09:15.171386 [info ] [MainThread]:   attach: None
[0m15:09:15.171503 [info ] [MainThread]:   filesystems: None
[0m15:09:15.171620 [info ] [MainThread]:   remote: None
[0m15:09:15.171735 [info ] [MainThread]:   plugins: None
[0m15:09:15.171850 [info ] [MainThread]:   disable_transactions: False
[0m15:09:15.172187 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:09:15.220982 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:09:15.245676 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:09:15.245945 [debug] [MainThread]: On debug: select 1 as id
[0m15:09:15.246092 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:09:15.260874 [debug] [MainThread]: SQL status: OK in 0.015 seconds
[0m15:09:15.261494 [debug] [MainThread]: On debug: Close
[0m15:09:15.261710 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:09:15.261883 [info ] [MainThread]: [31m1 check failed:[0m
[0m15:09:15.262015 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path []: Additional properties are not allowed ('config' was unexpected)

Error encountered in /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml


[0m15:09:15.263457 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.30779934, "process_in_blocks": "0", "process_kernel_time": 0.184507, "process_mem_max_rss": "132579328", "process_out_blocks": "0", "process_user_time": 0.796654}
[0m15:09:15.263733 [debug] [MainThread]: Command `dbt debug` failed at 15:09:15.263679 after 0.31 seconds
[0m15:09:15.263917 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:09:15.264098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d80bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dd6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110771cd0>]}
[0m15:09:15.264315 [debug] [MainThread]: Flushing usage events
[0m15:09:15.377537 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:09:33.872265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049e1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cd9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cd93d0>]}


============================== 15:09:33.875381 | ebe0baec-2ec5-402a-a245-201d76e95d57 ==============================
[0m15:09:33.875381 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:09:33.875709 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt debug', 'log_cache_events': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'warn_error': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'empty': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'write_json': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'no_print': 'None', 'target_path': 'None', 'quiet': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False'}
[0m15:09:33.883528 [info ] [MainThread]: dbt version: 1.11.4
[0m15:09:33.883792 [info ] [MainThread]: python version: 3.12.12
[0m15:09:33.883945 [info ] [MainThread]: python path: /opt/miniconda3/envs/dbt-duckdb/bin/python3.12
[0m15:09:33.884295 [info ] [MainThread]: os info: macOS-14.5-arm64-arm-64bit
[0m15:09:33.996431 [info ] [MainThread]: Using profiles dir at /Users/hazeldonaldson/.dbt
[0m15:09:33.996700 [info ] [MainThread]: Using profiles.yml file at /Users/hazeldonaldson/.dbt/profiles.yml
[0m15:09:33.996843 [info ] [MainThread]: Using dbt_project.yml file at /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/dbt_project.yml
[0m15:09:33.997972 [info ] [MainThread]: adapter type: duckdb
[0m15:09:33.998153 [info ] [MainThread]: adapter version: 1.10.0
[0m15:09:34.044590 [info ] [MainThread]: Configuration:
[0m15:09:34.044868 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:09:34.045046 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:09:34.045182 [info ] [MainThread]: Required dependencies:
[0m15:09:34.045388 [debug] [MainThread]: Executing "git --help"
[0m15:09:34.063455 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:09:34.064020 [debug] [MainThread]: STDERR: "b''"
[0m15:09:34.064208 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:09:34.064374 [info ] [MainThread]: Connection:
[0m15:09:34.064537 [info ] [MainThread]:   database: analytics
[0m15:09:34.064666 [info ] [MainThread]:   schema: main
[0m15:09:34.064797 [info ] [MainThread]:   path: /Users/hazeldonaldson/Documents/saas-analytics-platform/warehouse/analytics.duckdb
[0m15:09:34.064918 [info ] [MainThread]:   config_options: None
[0m15:09:34.065037 [info ] [MainThread]:   extensions: None
[0m15:09:34.065155 [info ] [MainThread]:   settings: {}
[0m15:09:34.065268 [info ] [MainThread]:   external_root: .
[0m15:09:34.065381 [info ] [MainThread]:   use_credential_provider: None
[0m15:09:34.065493 [info ] [MainThread]:   attach: None
[0m15:09:34.065601 [info ] [MainThread]:   filesystems: None
[0m15:09:34.065711 [info ] [MainThread]:   remote: None
[0m15:09:34.065823 [info ] [MainThread]:   plugins: None
[0m15:09:34.065931 [info ] [MainThread]:   disable_transactions: False
[0m15:09:34.066247 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:09:34.113549 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:09:34.135492 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:09:34.135730 [debug] [MainThread]: On debug: select 1 as id
[0m15:09:34.135873 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:09:34.149897 [debug] [MainThread]: SQL status: OK in 0.014 seconds
[0m15:09:34.150523 [debug] [MainThread]: On debug: Close
[0m15:09:34.150747 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:09:34.150913 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:09:34.152318 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.31489116, "process_in_blocks": "0", "process_kernel_time": 0.181845, "process_mem_max_rss": "133693440", "process_out_blocks": "0", "process_user_time": 0.812649}
[0m15:09:34.152596 [debug] [MainThread]: Command `dbt debug` succeeded at 15:09:34.152545 after 0.32 seconds
[0m15:09:34.152763 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:09:34.152952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ea9040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e78f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a72a80>]}
[0m15:09:34.153163 [debug] [MainThread]: Flushing usage events
[0m15:09:34.249012 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:52.273317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10306abd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10574b500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064b5760>]}


============================== 14:31:52.278246 | bc244f8d-0078-408a-965c-b548e369443e ==============================
[0m14:31:52.278246 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:31:52.278602 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'introspect': 'True', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'cache_selected_only': 'False', 'use_colors': 'True', 'write_json': 'True', 'printer_width': '80', 'empty': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_format': 'default', 'log_cache_events': 'False', 'fail_fast': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select staging', 'partial_parse': 'True'}
[0m14:31:52.764542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10644a030>]}
[0m14:31:52.792985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10646af00>]}
[0m14:31:52.794426 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:31:52.857093 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:31:52.857605 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:31:52.857807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e1400>]}
[0m14:31:53.224063 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.saas_analytics
[0m14:31:53.226483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e0a70>]}
[0m14:31:53.248032 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:31:53.249265 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:31:53.260527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073b9580>]}
[0m14:31:53.260789 [info ] [MainThread]: Found 472 macros
[0m14:31:53.260954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc244f8d-0078-408a-965c-b548e369443e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106752e70>]}
[0m14:31:53.261208 [warn ] [MainThread]: The selection criterion 'staging' does not match any enabled nodes
[0m14:31:53.261735 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:31:53.261986 [debug] [MainThread]: Command end result
[0m14:31:53.271796 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:31:53.272951 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:31:53.274596 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:31:53.294610 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.0616667, "process_in_blocks": "0", "process_kernel_time": 0.204515, "process_mem_max_rss": "130809856", "process_out_blocks": "0", "process_user_time": 1.276116}
[0m14:31:53.294932 [debug] [MainThread]: Command `dbt run` succeeded at 14:31:53.294877 after 1.06 seconds
[0m14:31:53.295166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060bec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107154260>]}
[0m14:31:53.295379 [debug] [MainThread]: Flushing usage events
[0m14:31:53.410041 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:11.669683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084d08f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be5760>]}


============================== 14:32:11.673279 | 7555be95-b896-47cf-be73-46ba0867ee81 ==============================
[0m14:32:11.673279 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:32:11.673625 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'partial_parse': 'True', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'static_parser': 'True', 'version_check': 'True', 'use_colors': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'fail_fast': 'False', 'no_print': 'None', 'empty': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'introspect': 'True', 'invocation_command': 'dbt test --select staging', 'write_json': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None'}
[0m14:32:11.835673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c4ddc0>]}
[0m14:32:11.863737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108508590>]}
[0m14:32:11.865715 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:32:11.916899 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:32:11.950541 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:32:11.950773 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:32:11.950908 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:32:11.951211 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.saas_analytics
[0m14:32:11.953851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ead730>]}
[0m14:32:11.973816 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:32:11.975156 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:32:11.990889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10925cb00>]}
[0m14:32:11.991164 [info ] [MainThread]: Found 472 macros
[0m14:32:11.991331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7555be95-b896-47cf-be73-46ba0867ee81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10916f3e0>]}
[0m14:32:11.991602 [warn ] [MainThread]: The selection criterion 'staging' does not match any enabled nodes
[0m14:32:11.992212 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:32:11.993098 [debug] [MainThread]: Command end result
[0m14:32:12.002825 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:32:12.003890 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:32:12.005541 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:32:12.006973 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.37557387, "process_in_blocks": "0", "process_kernel_time": 0.166873, "process_mem_max_rss": "132005888", "process_out_blocks": "0", "process_user_time": 0.878941}
[0m14:32:12.007226 [debug] [MainThread]: Command `dbt test` succeeded at 14:32:12.007179 after 0.38 seconds
[0m14:32:12.007416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108be5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108780920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092109e0>]}
[0m14:32:12.007595 [debug] [MainThread]: Flushing usage events
[0m14:32:12.108704 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:56.124482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10386cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb98e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb9670>]}


============================== 14:34:56.130856 | e3f22cac-a5a3-4d16-a8c4-77285b7b380e ==============================
[0m14:34:56.130856 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:56.131197 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'printer_width': '80', 'no_print': 'None', 'partial_parse': 'True', 'cache_selected_only': 'False', 'version_check': 'True', 'empty': 'False', 'invocation_command': 'dbt run --select staging', 'log_format': 'default', 'use_colors': 'True', 'warn_error': 'None', 'static_parser': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'debug': 'False', 'write_json': 'True'}
[0m14:34:56.292576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e3f22cac-a5a3-4d16-a8c4-77285b7b380e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f225d0>]}
[0m14:34:56.321948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e3f22cac-a5a3-4d16-a8c4-77285b7b380e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104878cb0>]}
[0m14:34:56.323371 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:56.374828 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:56.399595 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/sources.yml - Runtime Error
    Syntax error near line 7
    ------------------------------
    4  |   - name: raw
    5  |     description: >
    6  |     Raw data loaded directly into DuckDB. These tables are the unmodified
    7  |     source of truth and should only be referenced via staging models.
    8  |   schema: main
    9  | 
    10 |   tables:
    
    Raw Error:
    ------------------------------
    while scanning a simple key
      in "<unicode string>", line 6, column 5
    could not find expected ':'
      in "<unicode string>", line 7, column 5
[0m14:34:56.401117 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3159924, "process_in_blocks": "0", "process_kernel_time": 0.168035, "process_mem_max_rss": "130351104", "process_out_blocks": "0", "process_user_time": 0.844647}
[0m14:34:56.401376 [debug] [MainThread]: Command `dbt run` failed at 14:34:56.401325 after 0.32 seconds
[0m14:34:56.401579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eb9760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105185700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051623c0>]}
[0m14:34:56.401756 [debug] [MainThread]: Flushing usage events
[0m14:34:56.532681 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:32.956142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a5a600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105809a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058097f0>]}


============================== 14:35:32.960204 | 428b17c0-3778-49fc-ae26-cb90c34287aa ==============================
[0m14:35:32.960204 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:35:32.960579 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'write_json': 'True', 'invocation_command': 'dbt run --select staging', 'log_cache_events': 'False', 'no_print': 'None', 'fail_fast': 'False', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'printer_width': '80', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'introspect': 'True'}
[0m14:35:33.120081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105913aa0>]}
[0m14:35:33.149350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058a77a0>]}
[0m14:35:33.150903 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:35:33.205797 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:35:33.243913 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 5 files added, 0 files changed.
[0m14:35:33.244209 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_subscriptions.sql
[0m14:35:33.244373 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_users.sql
[0m14:35:33.244564 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/sources.yml
[0m14:35:33.244752 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/staging.yml
[0m14:35:33.244898 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/staging/stg_events.sql
[0m14:35:33.457309 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:35:33.457638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d1e20>]}
[0m14:35:33.557991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f93a40>]}
[0m14:35:33.599715 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:33.601312 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:33.613985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fa1d30>]}
[0m14:35:33.614272 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:35:33.614440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f8f380>]}
[0m14:35:33.615377 [info ] [MainThread]: 
[0m14:35:33.615730 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:35:33.615911 [info ] [MainThread]: 
[0m14:35:33.616161 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:35:33.618329 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:35:33.658412 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:35:33.658652 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:35:33.658796 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:33.676669 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:35:33.677457 [debug] [ThreadPool]: On list_analytics: Close
[0m14:35:33.677832 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:35:33.678083 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:35:33.681307 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.681479 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:35:33.681611 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:35:33.682396 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:35:33.682996 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.683136 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:35:33.683357 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.683482 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.683605 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:35:33.684062 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.684431 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:35:33.684557 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:35:33.684670 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:35:33.684862 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.684991 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:35:33.685539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:35:33.688198 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:33.688369 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:35:33.688553 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:35:33.688809 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:35:33.688942 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:33.689079 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:35:33.698901 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:35:33.699593 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:35:33.700640 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:35:33.700791 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:35:33.701241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f69190>]}
[0m14:35:33.701451 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.701578 [debug] [MainThread]: On master: BEGIN
[0m14:35:33.701689 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:35:33.701925 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.702051 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.702171 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.702287 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.702484 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.702606 [debug] [MainThread]: On master: Close
[0m14:35:33.704041 [debug] [Thread-1 (]: Began running node model.saas_analytics.stg_events
[0m14:35:33.704237 [debug] [Thread-2 (]: Began running node model.saas_analytics.stg_subscriptions
[0m14:35:33.704617 [debug] [Thread-3 (]: Began running node model.saas_analytics.stg_users
[0m14:35:33.704473 [info ] [Thread-1 (]: 1 of 3 START sql view model main.stg_events .................................... [RUN]
[0m14:35:33.704865 [info ] [Thread-2 (]: 2 of 3 START sql view model main.stg_subscriptions ............................. [RUN]
[0m14:35:33.705082 [info ] [Thread-3 (]: 3 of 3 START sql view model main.stg_users ..................................... [RUN]
[0m14:35:33.705327 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.stg_events)
[0m14:35:33.705585 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_subscriptions'
[0m14:35:33.705823 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_users'
[0m14:35:33.706004 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.stg_events
[0m14:35:33.706177 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.stg_subscriptions
[0m14:35:33.706329 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.stg_users
[0m14:35:33.710228 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.stg_events"
[0m14:35:33.712003 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.stg_subscriptions"
[0m14:35:33.714590 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.stg_users"
[0m14:35:33.715427 [debug] [Thread-1 (]: Began executing node model.saas_analytics.stg_events
[0m14:35:33.715628 [debug] [Thread-2 (]: Began executing node model.saas_analytics.stg_subscriptions
[0m14:35:33.715780 [debug] [Thread-3 (]: Began executing node model.saas_analytics.stg_users
[0m14:35:33.732455 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.stg_subscriptions"
[0m14:35:33.733792 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.stg_events"
[0m14:35:33.735391 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.stg_users"
[0m14:35:33.736106 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.736279 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: BEGIN
[0m14:35:33.736427 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:33.736766 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.737105 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:35:33.737308 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: BEGIN
[0m14:35:33.737495 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: BEGIN
[0m14:35:33.737657 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.737815 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:35:33.737966 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:35:33.738111 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.738453 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

  
  create view "analytics"."main"."stg_events__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_events"
),

renamed as (
    select
        event_id,
        user_id,
        event_timestamp as event_at,
        lower(trim(event_name)) as event_name,
        lower(trim(device_type)) as device_type,
        lower(trim(plan_type)) as plan_type,
        experiment_variant,
        event_properties
    from source 
)

select * from renamed
  );

[0m14:35:33.739016 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739178 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739351 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.739534 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:35:33.739689 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.739853 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

  
  create view "analytics"."main"."stg_subscriptions__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_subscriptions"
),

renamed as (
    select
        subscription_id,
        user_id,
        start_date as subscription_started_at,
        end_date as subscription_ended_at,
        lower(trim(plan)) as plan,
        lower(trim(status)) as subscription_status,
        monthly_revenue as monthly_revenue_usd,
        end_date is null as is_active,
        end_date is not null as is_churned
    from source
)

select * from renamed
  );

[0m14:35:33.740040 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquistion_channel)) as acquistion_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:35:33.743220 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.743629 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events__dbt_tmp" rename to "stg_events"
[0m14:35:33.744097 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:33.745662 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.745850 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions__dbt_tmp" rename to "stg_subscriptions"
[0m14:35:33.746019 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.751096 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:35:33.751340 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.752165 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquistion_channel)) as acquistion_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:35:33.752392 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:35:33.752562 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:35:33.752713 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m14:35:33.753579 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:35:33.753852 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: ROLLBACK
[0m14:35:33.754048 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.754463 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.755326 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:35:33.758281 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:35:33.758599 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

      drop view if exists "analytics"."main"."stg_events__dbt_backup" cascade
    
[0m14:35:33.758902 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:35:33.760173 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:35:33.760380 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

      drop view if exists "analytics"."main"."stg_subscriptions__dbt_backup" cascade
    
[0m14:35:33.760555 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.761910 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: Close
[0m14:35:33.763030 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:35:33.763759 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: Close
[0m14:35:33.764536 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106472db0>]}
[0m14:35:33.764716 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10647c920>]}
[0m14:35:33.765099 [info ] [Thread-1 (]: 1 of 3 OK created sql view model main.stg_events ............................... [[32mOK[0m in 0.06s]
[0m14:35:33.765648 [debug] [Thread-1 (]: Finished running node model.saas_analytics.stg_events
[0m14:35:33.765359 [info ] [Thread-2 (]: 2 of 3 OK created sql view model main.stg_subscriptions ........................ [[32mOK[0m in 0.06s]
[0m14:35:33.766060 [debug] [Thread-2 (]: Finished running node model.saas_analytics.stg_subscriptions
[0m14:35:33.769347 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.stg_users'
[0m14:35:33.769548 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: Close
[0m14:35:33.770929 [debug] [Thread-3 (]: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m14:35:33.771443 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '428b17c0-3778-49fc-ae26-cb90c34287aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106335f10>]}
[0m14:35:33.772110 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model main.stg_users ............................ [[31mERROR[0m in 0.07s]
[0m14:35:33.772467 [debug] [Thread-3 (]: Finished running node model.saas_analytics.stg_users
[0m14:35:33.772713 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.stg_users' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined.
[0m14:35:33.773779 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.773963 [debug] [MainThread]: On master: BEGIN
[0m14:35:33.774101 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:35:33.775150 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m14:35:33.775526 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.775874 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:33.776184 [debug] [MainThread]: On master: COMMIT
[0m14:35:33.776504 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:33.776656 [debug] [MainThread]: On master: Close
[0m14:35:33.776860 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:33.776983 [debug] [MainThread]: Connection 'model.saas_analytics.stg_events' was properly closed.
[0m14:35:33.777099 [debug] [MainThread]: Connection 'model.saas_analytics.stg_subscriptions' was properly closed.
[0m14:35:33.777205 [debug] [MainThread]: Connection 'model.saas_analytics.stg_users' was properly closed.
[0m14:35:33.777376 [info ] [MainThread]: 
[0m14:35:33.777536 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m14:35:33.778101 [debug] [MainThread]: Command end result
[0m14:35:33.795149 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:33.796263 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:33.799561 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:35:33.799798 [info ] [MainThread]: 
[0m14:35:33.799984 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:35:33.800132 [info ] [MainThread]: 
[0m14:35:33.800308 [error] [MainThread]: [31mFailure in model stg_users (models/staging/stg_users.sql)[0m
[0m14:35:33.800591 [error] [MainThread]:   Runtime Error in model stg_users (models/staging/stg_users.sql)
  Binder Error: Column "acquistion_channel" referenced that exists in the SELECT clause - but this column cannot be referenced before it is defined
[0m14:35:33.800720 [info ] [MainThread]: 
[0m14:35:33.800875 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/stg_users.sql
[0m14:35:33.801011 [info ] [MainThread]: 
[0m14:35:33.801158 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:35:33.801417 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:35:33.802867 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8866466, "process_in_blocks": "0", "process_kernel_time": 0.191036, "process_mem_max_rss": "151142400", "process_out_blocks": "0", "process_user_time": 1.314644}
[0m14:35:33.803140 [debug] [MainThread]: Command `dbt run` failed at 14:35:33.803092 after 0.89 seconds
[0m14:35:33.803345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058098b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105913380>]}
[0m14:35:33.803573 [debug] [MainThread]: Flushing usage events
[0m14:35:33.908272 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:22.178238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a4bf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779da60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779d820>]}


============================== 14:36:22.183847 | d8c7331f-53e0-4b4f-92d8-3d8162b04be5 ==============================
[0m14:36:22.183847 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:36:22.184193 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select staging', 'target_path': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'introspect': 'True', 'debug': 'False', 'write_json': 'True', 'static_parser': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'printer_width': '80', 'partial_parse': 'True'}
[0m14:36:22.349409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10798e1e0>]}
[0m14:36:22.378869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107659cd0>]}
[0m14:36:22.380327 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:36:22.431207 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:36:22.495110 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:36:22.495558 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_events.sql
[0m14:36:22.495783 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_subscriptions.sql
[0m14:36:22.495939 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/stg_users.sql
[0m14:36:22.654332 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:36:22.654663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fa52b0>]}
[0m14:36:22.715227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ffb8f0>]}
[0m14:36:22.787719 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:22.789639 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:22.802822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107276cc0>]}
[0m14:36:22.803112 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:36:22.803284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd6240>]}
[0m14:36:22.804262 [info ] [MainThread]: 
[0m14:36:22.804465 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:36:22.804615 [info ] [MainThread]: 
[0m14:36:22.804861 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:36:22.807020 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:36:22.836010 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:36:22.836255 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:36:22.836407 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:22.853946 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:36:22.854900 [debug] [ThreadPool]: On list_analytics: Close
[0m14:36:22.855244 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:36:22.855491 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:36:22.858824 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.859007 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:36:22.859144 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:36:22.859806 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:36:22.860442 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.860587 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:36:22.860815 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.860944 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.861075 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:36:22.861477 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.861864 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:36:22.861992 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:36:22.862103 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:36:22.862300 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.862426 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:36:22.863001 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:36:22.865781 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:22.871566 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:36:22.871822 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:36:22.872209 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:36:22.872395 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:22.872547 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:36:22.883496 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:36:22.884249 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:36:22.885212 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:36:22.885377 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:36:22.886146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fd3140>]}
[0m14:36:22.886370 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.886498 [debug] [MainThread]: On master: BEGIN
[0m14:36:22.886616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:36:22.886889 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.887046 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.887184 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.887304 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.887522 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.887648 [debug] [MainThread]: On master: Close
[0m14:36:22.889063 [debug] [Thread-1 (]: Began running node model.saas_analytics.stg_events
[0m14:36:22.889254 [debug] [Thread-2 (]: Began running node model.saas_analytics.stg_subscriptions
[0m14:36:22.889437 [debug] [Thread-3 (]: Began running node model.saas_analytics.stg_users
[0m14:36:22.889675 [info ] [Thread-1 (]: 1 of 3 START sql view model main.stg_events .................................... [RUN]
[0m14:36:22.889947 [info ] [Thread-2 (]: 2 of 3 START sql view model main.stg_subscriptions ............................. [RUN]
[0m14:36:22.890170 [info ] [Thread-3 (]: 3 of 3 START sql view model main.stg_users ..................................... [RUN]
[0m14:36:22.890385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.stg_events)
[0m14:36:22.890611 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_subscriptions'
[0m14:36:22.890812 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.stg_users'
[0m14:36:22.890973 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.stg_events
[0m14:36:22.891126 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.stg_subscriptions
[0m14:36:22.891273 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.stg_users
[0m14:36:22.894881 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.stg_events"
[0m14:36:22.896238 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.stg_subscriptions"
[0m14:36:22.897748 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.stg_users"
[0m14:36:22.898652 [debug] [Thread-3 (]: Began executing node model.saas_analytics.stg_users
[0m14:36:22.898893 [debug] [Thread-2 (]: Began executing node model.saas_analytics.stg_subscriptions
[0m14:36:22.914522 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.stg_users"
[0m14:36:22.916209 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.stg_subscriptions"
[0m14:36:22.916387 [debug] [Thread-1 (]: Began executing node model.saas_analytics.stg_events
[0m14:36:22.917984 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.stg_events"
[0m14:36:22.918621 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.918821 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.919045 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: BEGIN
[0m14:36:22.919241 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.919406 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: BEGIN
[0m14:36:22.919570 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:36:22.919718 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: BEGIN
[0m14:36:22.919863 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:36:22.920119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:22.920366 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.920598 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.920766 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.920923 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.921064 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.921230 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

  
  create view "analytics"."main"."stg_users__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_users"
),

renamed as (
    select
        user_id,
        signup_timestamp as signed_up_at,
        lower(trim(acquisition_channel)) as acquisition_channel,
        lower(trim(country)) as country
    from source 
)

select * from renamed
  );

[0m14:36:22.921386 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.921554 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

  
  create view "analytics"."main"."stg_subscriptions__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_subscriptions"
),

renamed as (
    select
        subscription_id,
        user_id,
        start_date as subscription_started_at,
        end_date as subscription_ended_at,
        lower(trim(plan)) as plan,
        lower(trim(status)) as subscription_status,
        monthly_revenue as monthly_revenue_usd,
        end_date is null as is_active,
        end_date is not null as is_churned
    from source
)

select * from renamed
  );

[0m14:36:22.921982 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

  
  create view "analytics"."main"."stg_events__dbt_tmp" as (
    with source as (
    select * from "analytics"."main"."raw_events"
),

renamed as (
    select
        event_id,
        user_id,
        event_timestamp as event_at,
        lower(trim(event_name)) as event_name,
        lower(trim(device_type)) as device_type,
        lower(trim(plan_type)) as plan_type,
        experiment_variant,
        event_properties
    from source 
)

select * from renamed
  );

[0m14:36:22.922315 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.925467 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.925661 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */
alter view "analytics"."main"."stg_users__dbt_tmp" rename to "stg_users"
[0m14:36:22.926028 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:36:22.926189 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:36:22.927575 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.927757 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.929114 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.929295 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions" rename to "stg_subscriptions__dbt_backup"
[0m14:36:22.933831 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: COMMIT
[0m14:36:22.934026 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events" rename to "stg_events__dbt_backup"
[0m14:36:22.934267 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.934474 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:36:22.934639 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: COMMIT
[0m14:36:22.934790 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:36:22.936130 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.937530 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.937704 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */
alter view "analytics"."main"."stg_subscriptions__dbt_tmp" rename to "stg_subscriptions"
[0m14:36:22.937864 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.938020 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */
alter view "analytics"."main"."stg_events__dbt_tmp" rename to "stg_events"
[0m14:36:22.941216 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.stg_users"
[0m14:36:22.941460 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m14:36:22.941620 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_users"} */

      drop view if exists "analytics"."main"."stg_users__dbt_backup" cascade
    
[0m14:36:22.943196 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:36:22.943352 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.943740 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.944410 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:36:22.944577 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: COMMIT
[0m14:36:22.944724 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.944876 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.945953 [debug] [Thread-3 (]: On model.saas_analytics.stg_users: Close
[0m14:36:22.946133 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: COMMIT
[0m14:36:22.946279 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.948149 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.stg_subscriptions"
[0m14:36:22.948319 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_subscriptions"} */

      drop view if exists "analytics"."main"."stg_subscriptions__dbt_backup" cascade
    
[0m14:36:22.948533 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.949802 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.stg_events"
[0m14:36:22.949970 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.stg_events"} */

      drop view if exists "analytics"."main"."stg_events__dbt_backup" cascade
    
[0m14:36:22.950131 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:36:22.950814 [debug] [Thread-2 (]: On model.saas_analytics.stg_subscriptions: Close
[0m14:36:22.951119 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e46f60>]}
[0m14:36:22.951296 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:22.951435 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108333140>]}
[0m14:36:22.951722 [info ] [Thread-3 (]: 3 of 3 OK created sql view model main.stg_users ................................ [[32mOK[0m in 0.06s]
[0m14:36:22.952348 [debug] [Thread-1 (]: On model.saas_analytics.stg_events: Close
[0m14:36:22.952841 [debug] [Thread-3 (]: Finished running node model.saas_analytics.stg_users
[0m14:36:22.952624 [info ] [Thread-2 (]: 2 of 3 OK created sql view model main.stg_subscriptions ........................ [[32mOK[0m in 0.06s]
[0m14:36:22.953171 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8c7331f-53e0-4b4f-92d8-3d8162b04be5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108240560>]}
[0m14:36:22.953388 [debug] [Thread-2 (]: Finished running node model.saas_analytics.stg_subscriptions
[0m14:36:22.953660 [info ] [Thread-1 (]: 1 of 3 OK created sql view model main.stg_events ............................... [[32mOK[0m in 0.06s]
[0m14:36:22.953983 [debug] [Thread-1 (]: Finished running node model.saas_analytics.stg_events
[0m14:36:22.954543 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.954679 [debug] [MainThread]: On master: BEGIN
[0m14:36:22.954794 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:36:22.955044 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.955174 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.955295 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:22.955402 [debug] [MainThread]: On master: COMMIT
[0m14:36:22.955593 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:22.955713 [debug] [MainThread]: On master: Close
[0m14:36:22.955868 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:22.955977 [debug] [MainThread]: Connection 'model.saas_analytics.stg_events' was properly closed.
[0m14:36:22.956079 [debug] [MainThread]: Connection 'model.saas_analytics.stg_subscriptions' was properly closed.
[0m14:36:22.956180 [debug] [MainThread]: Connection 'model.saas_analytics.stg_users' was properly closed.
[0m14:36:22.956319 [info ] [MainThread]: 
[0m14:36:22.956456 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m14:36:22.956840 [debug] [MainThread]: Command end result
[0m14:36:22.969148 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:22.970043 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:22.973210 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:36:22.973408 [info ] [MainThread]: 
[0m14:36:22.973781 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:36:22.973936 [info ] [MainThread]: 
[0m14:36:22.974097 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:36:22.974359 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 3 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:36:22.975851 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.8371948, "process_in_blocks": "0", "process_kernel_time": 0.197084, "process_mem_max_rss": "148176896", "process_out_blocks": "0", "process_user_time": 1.281671}
[0m14:36:22.976083 [debug] [MainThread]: Command `dbt run` succeeded at 14:36:22.976042 after 0.84 seconds
[0m14:36:22.976268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108087fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a3dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080dfec0>]}
[0m14:36:22.976444 [debug] [MainThread]: Flushing usage events
[0m14:36:23.100067 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:39.961749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10450ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048d1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048d1880>]}


============================== 14:36:39.964912 | d0756bc3-e391-464d-b037-6ff575cce075 ==============================
[0m14:36:39.964912 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:36:39.965237 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'empty': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'introspect': 'True', 'invocation_command': 'dbt test --select staging', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'write_json': 'True', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'indirect_selection': 'eager', 'no_print': 'None'}
[0m14:36:40.113281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a6e390>]}
[0m14:36:40.141423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104950350>]}
[0m14:36:40.142940 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:36:40.193902 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:36:40.256810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:36:40.257054 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:36:40.257190 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:36:40.277983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b71220>]}
[0m14:36:40.317898 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:40.319241 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:40.336816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105014950>]}
[0m14:36:40.337126 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:36:40.337313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd6de0>]}
[0m14:36:40.338471 [info ] [MainThread]: 
[0m14:36:40.338649 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:36:40.338789 [info ] [MainThread]: 
[0m14:36:40.339048 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:36:40.341266 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:36:40.369427 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:40.369669 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:36:40.369819 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:40.380138 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:36:40.380983 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:36:40.381661 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:36:40.395607 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m14:36:40.396495 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:36:40.397484 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:36:40.397655 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:36:40.398331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0756bc3-e391-464d-b037-6ff575cce075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f46d20>]}
[0m14:36:40.398569 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.398704 [debug] [MainThread]: On master: BEGIN
[0m14:36:40.398825 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:36:40.399096 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.399227 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.399353 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.399475 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.399677 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.399803 [debug] [MainThread]: On master: Close
[0m14:36:40.401300 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.401492 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.401841 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.401696 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired  [RUN]
[0m14:36:40.402055 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.402222 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:36:40.402396 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:36:40.402608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866)
[0m14:36:40.402778 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:36:40.403026 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:36:40.403764 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:36:40.403968 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.404214 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:36:40.404412 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.404575 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.413774 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.414041 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.418378 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.420297 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.422179 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.422627 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.428740 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.430965 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.432165 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.432360 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.432605 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.433740 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.434832 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.435144 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.435338 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: BEGIN
[0m14:36:40.435512 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.435708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.435882 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:36:40.436055 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.436223 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.436478 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:36:40.436639 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:36:40.436803 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.436949 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:36:40.437190 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:36:40.437356 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"
[0m14:36:40.437510 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.437643 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:36:40.437867 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','cancelled','expired'
)



  
  
      
    ) dbt_internal_test
[0m14:36:40.438036 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:36:40.438179 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.438479 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.438646 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.438799 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:36:40.439025 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:36:40.439203 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.439388 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.440537 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.443646 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:36:40.443925 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.444104 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.444541 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:36:40.444684 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.445405 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:36:40.445569 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:36:40.446203 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: ROLLBACK
[0m14:36:40.446878 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:36:40.447302 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:36:40.447592 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:36:40.448070 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866'
[0m14:36:40.448481 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:36:40.448645 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:36:40.448894 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:36:40.449531 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866: Close
[0m14:36:40.449700 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:36:40.450122 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.449964 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.05s]
[0m14:36:40.450443 [error] [Thread-1 (]: 1 of 17 FAIL 1 accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired  [[31mFAIL 1[0m in 0.05s]
[0m14:36:40.450688 [error] [Thread-2 (]: 2 of 17 FAIL 8000 not_null_stg_events_event_at ................................. [[31mFAIL 8000[0m in 0.05s]
[0m14:36:40.450876 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:36:40.451115 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:36:40.451334 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866
[0m14:36:40.451541 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:36:40.451713 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:36:40.451890 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.452066 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.452243 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.452404 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.452557 [info ] [Thread-4 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:36:40.452753 [info ] [Thread-1 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:36:40.452958 [info ] [Thread-2 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:36:40.456090 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.456511 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:36:40.456794 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired.61b3766866, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:36:40.456985 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:36:40.457234 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.457406 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.457575 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.459736 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.461592 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.463499 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.463683 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.465003 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.465355 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.465532 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.465696 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.466803 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.467026 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.468099 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.469256 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.469462 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:36:40.469694 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.469880 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.470188 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:36:40.470339 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.470516 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.470776 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.470965 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.471122 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:36:40.471280 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.471427 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:36:40.471597 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:36:40.471755 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.471906 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:36:40.472123 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.472456 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.472762 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.473145 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.473365 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.473535 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:36:40.473713 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:36:40.473886 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.474052 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.474245 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.474401 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.475277 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:36:40.475567 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.476267 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:36:40.476716 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:36:40.476920 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.477568 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:36:40.477995 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:36:40.478170 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:36:40.478845 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:36:40.479264 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:36:40.479428 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:36:40.479877 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:36:40.480296 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:36:40.480156 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:36:40.480606 [info ] [Thread-4 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.02s]
[0m14:36:40.480780 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:36:40.481212 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:36:40.481016 [info ] [Thread-2 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.02s]
[0m14:36:40.481464 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:36:40.481847 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.481694 [info ] [Thread-1 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.02s]
[0m14:36:40.482126 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:36:40.482297 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.482655 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:36:40.482467 [info ] [Thread-3 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:36:40.482861 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.483161 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.483020 [info ] [Thread-4 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:36:40.483389 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:36:40.483546 [info ] [Thread-2 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:36:40.483718 [info ] [Thread-1 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:36:40.483907 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:36:40.484070 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.484226 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:36:40.484395 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:36:40.484550 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.486749 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.486987 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.487167 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.489520 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.492542 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.526371 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.526750 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.527986 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.528215 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.529294 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.529542 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.530654 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.530876 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.531911 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.532149 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.532322 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:36:40.532511 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.532688 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.532855 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.533025 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:36:40.533300 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.533461 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:36:40.533622 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.533763 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.533908 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:36:40.534045 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.534187 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:36:40.534378 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.534570 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.534739 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.534901 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.535085 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:36:40.535300 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.535455 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:36:40.535625 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.535784 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:36:40.535947 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.536149 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.536338 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.537288 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:36:40.537557 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.538009 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:36:40.538169 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.538313 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.538505 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:36:40.539164 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:36:40.539845 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:36:40.540488 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:36:40.541201 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:36:40.540801 [info ] [Thread-3 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.06s]
[0m14:36:40.541680 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:36:40.542083 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:36:40.542230 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:36:40.542462 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:36:40.542624 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:36:40.542781 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:36:40.543181 [debug] [Thread-3 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.543027 [info ] [Thread-1 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.06s]
[0m14:36:40.543490 [info ] [Thread-2 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.06s]
[0m14:36:40.543691 [info ] [Thread-4 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.06s]
[0m14:36:40.544109 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:36:40.543878 [info ] [Thread-3 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:36:40.544362 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:36:40.544567 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:36:40.544732 [debug] [Thread-1 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.544916 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:36:40.545083 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.545250 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.545410 [info ] [Thread-1 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:36:40.545595 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.545770 [info ] [Thread-2 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:36:40.545995 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:36:40.546232 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:36:40.550291 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.550519 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:36:40.550697 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:36:40.550874 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.551076 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.551269 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.553618 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.557223 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.559207 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.559464 [debug] [Thread-3 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.560764 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.561204 [debug] [Thread-1 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.561385 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.561561 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.561747 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.563756 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.564815 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.565889 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.566070 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:36:40.566362 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:36:40.566813 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.567007 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.567178 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.567344 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:36:40.567506 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:36:40.567672 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.567817 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:36:40.567960 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:36:40.568109 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:40.568251 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:36:40.568421 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.568731 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.568920 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569086 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569372 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:36:40.569537 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:36:40.569687 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.569844 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.570039 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:36:40.570225 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:36:40.570689 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.572507 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:36:40.573645 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:36:40.574284 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:36:40.574489 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.574670 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:36:40.574836 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:36:40.575004 [debug] [Thread-3 (]: SQL status: OK in 0.006 seconds
[0m14:36:40.575427 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:36:40.576238 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:36:40.576966 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:36:40.577673 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:36:40.577939 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:36:40.578435 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:36:40.578886 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:36:40.582263 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:36:40.582510 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.582709 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:36:40.582878 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:36:40.583032 [debug] [Thread-2 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:36:40.583196 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:36:40.583536 [info ] [Thread-1 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.04s]
[0m14:36:40.583832 [info ] [Thread-3 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.04s]
[0m14:36:40.584248 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:36:40.584084 [info ] [Thread-2 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:36:40.584520 [debug] [Thread-1 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:36:40.584736 [debug] [Thread-3 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:36:40.584888 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.585086 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:36:40.587398 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.587990 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.589383 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.589997 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.590163 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:36:40.590306 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:36:40.590666 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:36:40.590817 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:36:40.590973 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:36:40.592546 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:36:40.593369 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:36:40.593853 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:36:40.594028 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:36:40.594308 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:36:40.594558 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:36:40.595256 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.595411 [debug] [MainThread]: On master: BEGIN
[0m14:36:40.595530 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:36:40.595799 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.595930 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.596059 [debug] [MainThread]: Using duckdb connection "master"
[0m14:36:40.596177 [debug] [MainThread]: On master: COMMIT
[0m14:36:40.596369 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:36:40.596494 [debug] [MainThread]: On master: Close
[0m14:36:40.596654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:40.596776 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:36:40.596882 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:36:40.596986 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:36:40.597086 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:36:40.597215 [info ] [MainThread]: 
[0m14:36:40.597352 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m14:36:40.598390 [debug] [MainThread]: Command end result
[0m14:36:40.612312 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:36:40.613423 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:36:40.616863 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:36:40.617040 [info ] [MainThread]: 
[0m14:36:40.617220 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m14:36:40.617364 [info ] [MainThread]: 
[0m14:36:40.617540 [error] [MainThread]: [31mFailure in test accepted_values_stg_subscriptions_subscription_status__active__cancelled__expired (models/staging/staging.yml)[0m
[0m14:36:40.617706 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:36:40.617826 [info ] [MainThread]: 
[0m14:36:40.617977 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/accepted_values_stg_subscripti_43218a3e992f3fc96ed40658d91620bb.sql
[0m14:36:40.618102 [info ] [MainThread]: 
[0m14:36:40.618252 [error] [MainThread]: [31mFailure in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:36:40.618397 [error] [MainThread]:   Got 8000 results, configured to fail if != 0
[0m14:36:40.618511 [info ] [MainThread]: 
[0m14:36:40.618650 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:36:40.618771 [info ] [MainThread]: 
[0m14:36:40.618910 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=17
[0m14:36:40.620338 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.6966848, "process_in_blocks": "0", "process_kernel_time": 0.193666, "process_mem_max_rss": "158597120", "process_out_blocks": "0", "process_user_time": 1.2042}
[0m14:36:40.620564 [debug] [MainThread]: Command `dbt test` failed at 14:36:40.620524 after 0.70 seconds
[0m14:36:40.620754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10450ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b71940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b5a510>]}
[0m14:36:40.620924 [debug] [MainThread]: Flushing usage events
[0m14:36:40.718193 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:47:15.528980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106670c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5640>]}


============================== 14:47:15.534951 | e26f83cc-4287-41b3-9d5a-ff4aa184806d ==============================
[0m14:47:15.534951 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:47:15.535287 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'no_print': 'None', 'empty': 'None', 'fail_fast': 'False', 'target_path': 'None', 'warn_error': 'None', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'write_json': 'True', 'introspect': 'True', 'version_check': 'True', 'quiet': 'False', 'log_format': 'default', 'log_cache_events': 'False', 'invocation_command': 'dbt test --select staging', 'static_parser': 'True', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:47:15.703626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e26f83cc-4287-41b3-9d5a-ff4aa184806d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10764a360>]}
[0m14:47:15.731817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e26f83cc-4287-41b3-9d5a-ff4aa184806d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072894c0>]}
[0m14:47:15.733106 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:47:15.786055 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:47:15.838290 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/staging.yml - Runtime Error
    Syntax error near line 49
    ------------------------------
    46 |         description: Date the event was fired.
    47 |         tests:
    48 |           - not_null
    49 |             config:
    50 |               severity: warn
    51 | 
    52 |       - name: event_name
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 49, column 19
[0m14:47:15.839879 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.3486674, "process_in_blocks": "0", "process_kernel_time": 0.16323, "process_mem_max_rss": "131710976", "process_out_blocks": "0", "process_user_time": 0.853429}
[0m14:47:15.840157 [debug] [MainThread]: Command `dbt test` failed at 14:47:15.840104 after 0.35 seconds
[0m14:47:15.840357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077574a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ec0ad0>]}
[0m14:47:15.840535 [debug] [MainThread]: Flushing usage events
[0m14:47:15.983865 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:41.991172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106956180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb58e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb56a0>]}


============================== 14:50:41.997498 | 4229c09d-8046-4dae-b851-167cf9ff406d ==============================
[0m14:50:41.997498 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:50:41.997914 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt test --select staging', 'fail_fast': 'False', 'warn_error': 'None', 'no_print': 'None', 'static_parser': 'True', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'debug': 'False', 'cache_selected_only': 'False', 'printer_width': '80', 'use_colors': 'True', 'quiet': 'False'}
[0m14:50:42.159731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4229c09d-8046-4dae-b851-167cf9ff406d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c8dee0>]}
[0m14:50:42.188386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4229c09d-8046-4dae-b851-167cf9ff406d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11906d2e0>]}
[0m14:50:42.189792 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:50:42.240964 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:50:42.293475 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: staging/staging.yml - Runtime Error
    Syntax error near line 45
    ------------------------------
    42 |               to: ref('stg_users')
    43 |               field: user_id
    44 | 
    45 |      - name: event_at
    46 |         description: Date the event was fired.
    47 |         tests:
    48 |           - not_null:
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 26, column 5
    did not find expected key
      in "<unicode string>", line 45, column 6
[0m14:50:42.294993 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.34547922, "process_in_blocks": "0", "process_kernel_time": 0.164853, "process_mem_max_rss": "132661248", "process_out_blocks": "0", "process_user_time": 0.859124}
[0m14:50:42.295259 [debug] [MainThread]: Command `dbt test` failed at 14:50:42.295204 after 0.35 seconds
[0m14:50:42.295456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cb5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190536e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d1e060>]}
[0m14:50:42.295644 [debug] [MainThread]: Flushing usage events
[0m14:50:42.431035 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:51:50.295375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043e6600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29730>]}


============================== 14:51:50.301963 | 56017025-6a9a-4d27-a371-92e6ffd68c47 ==============================
[0m14:51:50.301963 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:51:50.302292 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'printer_width': '80', 'fail_fast': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'write_json': 'True', 'target_path': 'None', 'quiet': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'invocation_command': 'dbt test --select staging', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'introspect': 'True', 'partial_parse': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True'}
[0m14:51:50.465699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10701a480>]}
[0m14:51:50.495468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065dd4c0>]}
[0m14:51:50.496930 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:51:50.549149 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:51:50.615571 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:51:50.616121 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:51:50.780249 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships` defined on 'stg_events' in
package 'saas_analytics' (models/staging/staging.yml). Arguments to generic
tests should be nested under the `arguments` property.
[0m14:51:50.780605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '56017025-6a9a-4d27-a371-92e6ffd68c47', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107469430>]}
[0m14:51:50.792209 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/staging.yml:
  	test definition dictionary must have exactly one key, got [('not null', None), ('config', {'severity': 'warn'})] instead (2 keys)
  	@: UnparsedModelUpdate(original_file_path='mode...ne)
[0m14:51:50.792598 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:51:50.797487 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.5418513, "process_in_blocks": "0", "process_kernel_time": 0.170472, "process_mem_max_rss": "136544256", "process_out_blocks": "0", "process_user_time": 1.0355}
[0m14:51:50.797826 [debug] [MainThread]: Command `dbt test` failed at 14:51:50.797764 after 0.54 seconds
[0m14:51:50.798140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e29790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107657a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107630c80>]}
[0m14:51:50.798365 [debug] [MainThread]: Flushing usage events
[0m14:51:50.903508 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:24.126563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dd1070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d790>]}


============================== 14:53:24.129776 | 5bb91f57-642f-4b57-bb86-820bee96a63f ==============================
[0m14:53:24.129776 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:53:24.130148 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt test --select staging', 'use_experimental_parser': 'False', 'log_format': 'default', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'debug': 'False', 'empty': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'introspect': 'True'}
[0m14:53:24.289339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b35ee0>]}
[0m14:53:24.318334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040d7c20>]}
[0m14:53:24.319678 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:53:24.370926 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:53:24.435501 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:53:24.436027 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:53:24.606536 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on
'stg_subscriptions' in package 'saas_analytics' (models/staging/staging.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m14:53:24.606860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10728cc80>]}
[0m14:53:24.652197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107355820>]}
[0m14:53:24.728337 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:24.729784 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:24.748584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107319c40>]}
[0m14:53:24.748907 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:53:24.749120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071ea210>]}
[0m14:53:24.750311 [info ] [MainThread]: 
[0m14:53:24.750510 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:53:24.750652 [info ] [MainThread]: 
[0m14:53:24.750891 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:53:24.753605 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:53:24.782595 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:24.782840 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:53:24.782993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:24.795574 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:53:24.795831 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:24.795987 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:53:24.812562 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:53:24.813616 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:53:24.814615 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:53:24.814811 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:53:24.815561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bb91f57-642f-4b57-bb86-820bee96a63f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d5b50>]}
[0m14:53:24.815814 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.816367 [debug] [MainThread]: On master: BEGIN
[0m14:53:24.816497 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:24.816815 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.816949 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.817077 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.817195 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.817397 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.817527 [debug] [MainThread]: On master: Close
[0m14:53:24.820501 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.820728 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.820901 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.821060 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.821301 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__cancelled  [RUN]
[0m14:53:24.821541 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:53:24.821798 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:53:24.821987 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:53:24.822207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931)
[0m14:53:24.822429 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:24.822642 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:24.822838 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:24.822994 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.823147 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.823291 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.823433 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.827664 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.832294 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.834239 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.835949 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.836661 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.844894 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.845163 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.845366 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.845545 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.846697 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.847759 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.848868 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.849110 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.849371 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: BEGIN
[0m14:53:24.849562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.849906 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.850090 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.850301 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.850517 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.850665 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:53:24.850814 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:53:24.850960 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:53:24.851112 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"
[0m14:53:24.851256 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:53:24.851398 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:53:24.851538 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:53:24.851705 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','cancelled'
)



  
  
      
    ) dbt_internal_test
[0m14:53:24.852058 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852342 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852490 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.852646 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:24.852800 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:24.852959 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:24.853125 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.853306 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.853477 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.854878 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.857267 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:53:24.857550 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:24.858019 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:24.858226 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m14:53:24.858407 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:53:24.858563 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:53:24.859238 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:53:24.859964 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:53:24.860865 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: ROLLBACK
[0m14:53:24.860263 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:53:24.861359 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:24.861765 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:24.862175 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931'
[0m14:53:24.862407 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:24.862579 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:53:24.862745 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:53:24.862895 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931: Close
[0m14:53:24.863074 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.863359 [warn ] [Thread-2 (]: 2 of 17 WARN 8000 not_null_stg_events_event_at ................................. [[33mWARN 8000[0m in 0.04s]
[0m14:53:24.863583 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.04s]
[0m14:53:24.863889 [error] [Thread-1 (]: 1 of 17 FAIL 1 accepted_values_stg_subscriptions_subscription_status__active__cancelled  [[31mFAIL 1[0m in 0.04s]
[0m14:53:24.864079 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:53:24.864337 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:24.864564 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:24.864785 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931
[0m14:53:24.864959 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:53:24.865129 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.865309 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.865485 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.865679 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.865857 [info ] [Thread-2 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:53:24.866043 [info ] [Thread-4 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:53:24.869790 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.866186 [info ] [Thread-1 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:53:24.870133 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:53:24.870350 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:53:24.870576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__cancelled.eb3959b931, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:53:24.870761 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.870936 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.871116 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.873081 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.873293 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.875169 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.876909 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.878073 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.878624 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.879757 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.879958 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.881036 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.881247 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.882360 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.882612 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.882808 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:53:24.882996 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.883180 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.883358 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.883509 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:53:24.883674 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.883930 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:53:24.884085 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.884252 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.884398 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:53:24.884552 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.884789 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:24.884975 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.885129 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.885368 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.885615 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.885784 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:24.885948 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.886181 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:24.886360 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.886546 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:24.886739 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.886958 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.887144 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.888467 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:53:24.889007 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:53:24.889224 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889461 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889650 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.889828 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:53:24.890686 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:53:24.891462 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:53:24.892176 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:53:24.892513 [info ] [Thread-2 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.02s]
[0m14:53:24.893060 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:53:24.893516 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:53:24.893756 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:24.894177 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:53:24.894369 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:53:24.894549 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:53:24.894749 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.894936 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:53:24.895205 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.895419 [info ] [Thread-1 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.02s]
[0m14:53:24.895623 [info ] [Thread-2 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:53:24.896193 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:24.895910 [info ] [Thread-4 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.03s]
[0m14:53:24.896526 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:24.896725 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:53:24.896915 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.897146 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:24.897315 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.897485 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.897649 [info ] [Thread-3 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:53:24.897876 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.898081 [info ] [Thread-1 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:53:24.900395 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.900630 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:53:24.900821 [info ] [Thread-4 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:53:24.901023 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:53:24.901239 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.901422 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:53:24.901581 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.903513 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.903736 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.905594 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.905801 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.909572 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.910936 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.911312 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.911511 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.912645 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.913751 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.913962 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.914153 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.914363 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:53:24.915517 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.915687 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.916060 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.916234 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:53:24.916420 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.916598 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.916744 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.916908 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:53:24.917068 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:24.917242 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.917450 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.917620 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.917785 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.917922 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:53:24.918203 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:24.918355 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.918493 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.918665 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.918834 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:24.919031 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.919227 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.919386 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.919585 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:24.920308 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:53:24.920472 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.920690 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.921163 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:53:24.921802 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:53:24.922006 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.922168 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:53:24.922644 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:53:24.922841 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.923470 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:53:24.923761 [info ] [Thread-2 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.03s]
[0m14:53:24.923955 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:53:24.924600 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:53:24.924842 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:24.925239 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:53:24.925493 [info ] [Thread-1 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.02s]
[0m14:53:24.925946 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:53:24.926120 [debug] [Thread-2 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.926305 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:53:24.926529 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:24.926682 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:53:24.926854 [info ] [Thread-2 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:53:24.927264 [debug] [Thread-1 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.927117 [info ] [Thread-3 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.03s]
[0m14:53:24.927707 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:53:24.927561 [info ] [Thread-4 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.928114 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:24.927917 [info ] [Thread-1 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:53:24.928312 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.928520 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:24.928680 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.928852 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:53:24.931511 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.931741 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.931910 [info ] [Thread-3 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:53:24.932122 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.932354 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:53:24.932588 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:53:24.934900 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.935127 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:53:24.935298 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.935531 [debug] [Thread-2 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.935677 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.938490 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.939816 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.940058 [debug] [Thread-1 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.942389 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.943803 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.944124 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.944342 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.946450 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.946656 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:53:24.946831 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.947044 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.947213 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:24.948263 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.948447 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:53:24.948837 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.949001 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:24.949234 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.949391 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:53:24.949634 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:24.949809 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.949950 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:24.950095 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.950274 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.950437 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:53:24.950636 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:24.950881 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.951037 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.951220 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:24.951394 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:24.951699 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.951870 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.952095 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:24.952267 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.954094 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:24.954401 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:53:24.955749 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:53:24.956546 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:53:24.956785 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:53:24.956975 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m14:53:24.957413 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:53:24.957804 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:53:24.958037 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:53:24.958694 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:53:24.958857 [debug] [Thread-1 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:53:24.959465 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:53:24.959784 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:53:24.960266 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:53:24.960522 [info ] [Thread-1 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.03s]
[0m14:53:24.960987 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:53:24.961210 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:24.961375 [debug] [Thread-3 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:53:24.961604 [debug] [Thread-1 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:24.961755 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:53:24.961934 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.962198 [info ] [Thread-3 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.03s]
[0m14:53:24.962452 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:53:24.962974 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:24.962726 [info ] [Thread-2 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.03s]
[0m14:53:24.963222 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:53:24.963507 [debug] [Thread-2 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:24.963676 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.966177 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.966909 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.968199 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.968728 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.968888 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:53:24.969031 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:24.969392 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:53:24.969533 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:24.969679 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:24.971196 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:24.972031 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:53:24.972557 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:53:24.972785 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:53:24.973165 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:53:24.973468 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:24.974186 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.974356 [debug] [MainThread]: On master: BEGIN
[0m14:53:24.974486 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:53:24.974906 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.975103 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.975304 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:24.975431 [debug] [MainThread]: On master: COMMIT
[0m14:53:24.975672 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:24.975804 [debug] [MainThread]: On master: Close
[0m14:53:24.975986 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:24.976102 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:53:24.976208 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:53:24.976311 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:53:24.976410 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:53:24.976548 [info ] [MainThread]: 
[0m14:53:24.976693 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m14:53:24.977761 [debug] [MainThread]: Command end result
[0m14:53:24.990606 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:24.991914 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:24.995508 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:53:24.995707 [info ] [MainThread]: 
[0m14:53:24.995880 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 1 warning:[0m
[0m14:53:24.996026 [info ] [MainThread]: 
[0m14:53:24.996209 [error] [MainThread]: [31mFailure in test accepted_values_stg_subscriptions_subscription_status__active__cancelled (models/staging/staging.yml)[0m
[0m14:53:24.996375 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:53:24.996494 [info ] [MainThread]: 
[0m14:53:24.996646 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/accepted_values_stg_subscripti_ed26b36fb4559cc7cdc0f9175b0b5654.sql
[0m14:53:24.996774 [info ] [MainThread]: 
[0m14:53:24.996924 [warn ] [MainThread]: [33mWarning in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:53:24.997076 [warn ] [MainThread]: Got 8000 results, configured to warn if != 0
[0m14:53:24.997196 [info ] [MainThread]: 
[0m14:53:24.997338 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:53:24.997462 [info ] [MainThread]: 
[0m14:53:24.997605 [info ] [MainThread]: Done. PASS=15 WARN=1 ERROR=1 SKIP=0 NO-OP=0 TOTAL=17
[0m14:53:24.997861 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:53:24.999304 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.90952873, "process_in_blocks": "0", "process_kernel_time": 0.205225, "process_mem_max_rss": "161267712", "process_out_blocks": "0", "process_user_time": 1.387335}
[0m14:53:24.999570 [debug] [MainThread]: Command `dbt test` failed at 14:53:24.999522 after 0.91 seconds
[0m14:53:24.999775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a2d9a0>]}
[0m14:53:24.999958 [debug] [MainThread]: Flushing usage events
[0m14:53:25.128046 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:43.580935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104915280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104003500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d700>]}


============================== 14:53:43.584032 | 2cde45ef-7bdf-4f12-98b5-6db7ff4d983e ==============================
[0m14:53:43.584032 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:53:43.584374 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'debug': 'False', 'invocation_command': 'dbt test --select staging', 'introspect': 'True', 'target_path': 'None', 'use_colors': 'True', 'empty': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'printer_width': '80', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None'}
[0m14:53:43.752542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102347e60>]}
[0m14:53:43.782425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dcdee0>]}
[0m14:53:43.784101 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:53:43.839910 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:53:43.907442 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:53:43.907989 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/staging/staging.yml
[0m14:53:44.077718 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on
'stg_subscriptions' in package 'saas_analytics' (models/staging/staging.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m14:53:44.078044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105531010>]}
[0m14:53:44.116305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c53da0>]}
[0m14:53:44.190277 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:44.192132 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:44.210314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054cd430>]}
[0m14:53:44.210621 [info ] [MainThread]: Found 3 models, 17 data tests, 3 sources, 472 macros
[0m14:53:44.210800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055b27e0>]}
[0m14:53:44.212168 [info ] [MainThread]: 
[0m14:53:44.212396 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:53:44.212545 [info ] [MainThread]: 
[0m14:53:44.212788 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:53:44.215021 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:53:44.241676 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:44.241911 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:53:44.242052 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:44.253901 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m14:53:44.254114 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:53:44.254263 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:53:44.271170 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:53:44.272242 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:53:44.273238 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:53:44.273409 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:53:44.274273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde45ef-7bdf-4f12-98b5-6db7ff4d983e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a01a0>]}
[0m14:53:44.274521 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.274650 [debug] [MainThread]: On master: BEGIN
[0m14:53:44.274771 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:44.275032 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.275164 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.275289 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.275403 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.275587 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.275713 [debug] [MainThread]: On master: Close
[0m14:53:44.277257 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.277444 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.277759 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.277915 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.277625 [info ] [Thread-1 (]: 1 of 17 START test accepted_values_stg_subscriptions_subscription_status__active__canceled  [RUN]
[0m14:53:44.278106 [info ] [Thread-2 (]: 2 of 17 START test not_null_stg_events_event_at ................................ [RUN]
[0m14:53:44.278298 [info ] [Thread-3 (]: 3 of 17 START test not_null_stg_events_event_id ................................ [RUN]
[0m14:53:44.278488 [info ] [Thread-4 (]: 4 of 17 START test not_null_stg_events_event_name .............................. [RUN]
[0m14:53:44.278694 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140)
[0m14:53:44.278910 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:44.279100 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:44.279284 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:44.279438 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.279582 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.279724 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.279861 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.284192 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.289120 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.294836 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.296652 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.297194 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.303431 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.305650 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.305879 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.307088 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.307284 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.308441 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.309568 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.309954 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.310178 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.310386 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: BEGIN
[0m14:53:44.310564 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: BEGIN
[0m14:53:44.310738 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.310953 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.311113 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.311256 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:53:44.311412 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: BEGIN
[0m14:53:44.311556 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: BEGIN
[0m14:53:44.311911 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:53:44.312080 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:53:44.312248 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312389 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312645 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"
[0m14:53:44.312812 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.312963 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"
[0m14:53:44.313105 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.313273 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        subscription_status as value_field,
        count(*) as n_records

    from "analytics"."main"."stg_subscriptions"
    group by subscription_status

)

select *
from all_values
where value_field not in (
    'active','canceled'
)



  
  
      
    ) dbt_internal_test
[0m14:53:44.313432 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"
[0m14:53:44.313583 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_id.715b3f7012"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_id
from "analytics"."main"."stg_events"
where event_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.313731 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"
[0m14:53:44.313974 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_name.682bd9eda3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_name
from "analytics"."main"."stg_events"
where event_name is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.314212 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select event_at
from "analytics"."main"."stg_events"
where event_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.315457 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.317710 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: ROLLBACK
[0m14:53:44.317947 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.318110 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.318534 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_id.715b3f7012'
[0m14:53:44.318678 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m14:53:44.319372 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: ROLLBACK
[0m14:53:44.319981 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: ROLLBACK
[0m14:53:44.320133 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_event_id.715b3f7012: Close
[0m14:53:44.320633 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35'
[0m14:53:44.321285 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: ROLLBACK
[0m14:53:44.321941 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_event_name.682bd9eda3'
[0m14:53:44.322112 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35: Close
[0m14:53:44.321562 [info ] [Thread-3 (]: 3 of 17 PASS not_null_stg_events_event_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:53:44.322632 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140'
[0m14:53:44.322789 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_events_event_name.682bd9eda3: Close
[0m14:53:44.323239 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_event_id.715b3f7012
[0m14:53:44.323037 [warn ] [Thread-2 (]: 2 of 17 WARN 8000 not_null_stg_events_event_at ................................. [[33mWARN 8000[0m in 0.04s]
[0m14:53:44.323442 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140: Close
[0m14:53:44.323818 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.323668 [info ] [Thread-4 (]: 4 of 17 PASS not_null_stg_events_event_name .................................... [[32mPASS[0m in 0.04s]
[0m14:53:44.324097 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35
[0m14:53:44.324330 [info ] [Thread-1 (]: 1 of 17 PASS accepted_values_stg_subscriptions_subscription_status__active__canceled  [[32mPASS[0m in 0.05s]
[0m14:53:44.324679 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_events_event_name.682bd9eda3
[0m14:53:44.324461 [info ] [Thread-3 (]: 5 of 17 START test not_null_stg_events_user_id ................................. [RUN]
[0m14:53:44.324878 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.325104 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140
[0m14:53:44.325262 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.325437 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_id.715b3f7012, now test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4)
[0m14:53:44.325739 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.325590 [info ] [Thread-2 (]: 6 of 17 START test not_null_stg_subscriptions_plan ............................. [RUN]
[0m14:53:44.326076 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.325944 [info ] [Thread-4 (]: 7 of 17 START test not_null_stg_subscriptions_subscription_id .................. [RUN]
[0m14:53:44.326268 [info ] [Thread-1 (]: 8 of 17 START test not_null_stg_subscriptions_subscription_started_at .......... [RUN]
[0m14:53:44.326439 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_at.7a6ca5da35, now test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19)
[0m14:53:44.328580 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.328823 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_event_name.682bd9eda3, now test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91)
[0m14:53:44.329045 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_stg_subscriptions_subscription_status__active__canceled.0bb4427140, now test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992)
[0m14:53:44.329218 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.329422 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.329593 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.331681 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.333599 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.333834 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.335790 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.337963 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.338280 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.338477 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.339611 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.340914 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.341179 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.341495 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.342777 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.342995 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: BEGIN
[0m14:53:44.343255 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.343446 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.343770 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.343951 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.344100 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: BEGIN
[0m14:53:44.344263 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.344433 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: BEGIN
[0m14:53:44.344591 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: BEGIN
[0m14:53:44.344741 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.344890 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"
[0m14:53:44.345034 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.345177 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.345404 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_events"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.345615 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.345817 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.346056 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"
[0m14:53:44.346205 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.346365 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"
[0m14:53:44.346537 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select plan
from "analytics"."main"."stg_subscriptions"
where plan is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.346715 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"
[0m14:53:44.346880 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.347035 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."stg_subscriptions"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.347289 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_started_at
from "analytics"."main"."stg_subscriptions"
where subscription_started_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.348501 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: ROLLBACK
[0m14:53:44.348991 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4'
[0m14:53:44.349145 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4: Close
[0m14:53:44.349314 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349478 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349639 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.349898 [info ] [Thread-3 (]: 5 of 17 PASS not_null_stg_events_user_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.350649 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: ROLLBACK
[0m14:53:44.351306 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: ROLLBACK
[0m14:53:44.351886 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: ROLLBACK
[0m14:53:44.352132 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4
[0m14:53:44.352597 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91'
[0m14:53:44.353006 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992'
[0m14:53:44.353416 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19'
[0m14:53:44.353590 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.353779 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91: Close
[0m14:53:44.353944 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992: Close
[0m14:53:44.354098 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19: Close
[0m14:53:44.354269 [info ] [Thread-3 (]: 9 of 17 START test not_null_stg_subscriptions_subscription_status .............. [RUN]
[0m14:53:44.354567 [info ] [Thread-4 (]: 7 of 17 PASS not_null_stg_subscriptions_subscription_id ........................ [[32mPASS[0m in 0.03s]
[0m14:53:44.354781 [info ] [Thread-1 (]: 8 of 17 PASS not_null_stg_subscriptions_subscription_started_at ................ [[32mPASS[0m in 0.03s]
[0m14:53:44.355277 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_events_user_id.63ec98fbc4, now test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581)
[0m14:53:44.355115 [info ] [Thread-2 (]: 6 of 17 PASS not_null_stg_subscriptions_plan ................................... [[32mPASS[0m in 0.03s]
[0m14:53:44.355543 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91
[0m14:53:44.355771 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992
[0m14:53:44.355938 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.356154 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19
[0m14:53:44.356325 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.356523 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.359071 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.359353 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.359531 [info ] [Thread-4 (]: 10 of 17 START test not_null_stg_subscriptions_user_id ......................... [RUN]
[0m14:53:44.359716 [info ] [Thread-1 (]: 11 of 17 START test not_null_stg_users_signed_up_at ............................ [RUN]
[0m14:53:44.359991 [info ] [Thread-2 (]: 12 of 17 START test not_null_stg_users_user_id ................................. [RUN]
[0m14:53:44.360245 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_id.68a3edda91, now test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2)
[0m14:53:44.360479 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_started_at.e883455992, now test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b)
[0m14:53:44.360638 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.360797 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_plan.680a75aa19, now test.saas_analytics.not_null_stg_users_user_id.980dfc1b77)
[0m14:53:44.360949 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.361093 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.362275 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.362429 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.364195 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.365887 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.367844 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.368223 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.368397 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: BEGIN
[0m14:53:44.368566 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.368906 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.370225 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.370420 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.370583 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.370736 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.370918 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"
[0m14:53:44.373054 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.374505 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.374762 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_status
from "analytics"."main"."stg_subscriptions"
where subscription_status is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.374995 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.375385 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: BEGIN
[0m14:53:44.375590 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.375858 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.376044 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.376200 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.376346 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: BEGIN
[0m14:53:44.376494 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.376645 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: BEGIN
[0m14:53:44.376796 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"
[0m14:53:44.376934 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.377115 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.377831 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: ROLLBACK
[0m14:53:44.378008 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_user_id.980dfc1b77"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_users"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.378614 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581'
[0m14:53:44.378769 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.378995 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.379165 [debug] [Thread-3 (]: On test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581: Close
[0m14:53:44.379352 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"
[0m14:53:44.379517 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"
[0m14:53:44.379668 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.380082 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signed_up_at
from "analytics"."main"."stg_users"
where signed_up_at is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.379940 [info ] [Thread-3 (]: 9 of 17 PASS not_null_stg_subscriptions_subscription_status .................... [[32mPASS[0m in 0.02s]
[0m14:53:44.380300 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."stg_subscriptions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.380964 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: ROLLBACK
[0m14:53:44.381257 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581
[0m14:53:44.381565 [debug] [Thread-3 (]: Began running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.381994 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_user_id.980dfc1b77'
[0m14:53:44.382200 [info ] [Thread-3 (]: 13 of 17 START test relationships_stg_events_user_id__user_id__ref_stg_users_ .. [RUN]
[0m14:53:44.382384 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.382528 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.382671 [debug] [Thread-2 (]: On test.saas_analytics.not_null_stg_users_user_id.980dfc1b77: Close
[0m14:53:44.382849 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_subscription_status.6b8d4c5581, now test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244)
[0m14:53:44.383478 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: ROLLBACK
[0m14:53:44.384195 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: ROLLBACK
[0m14:53:44.384567 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.384427 [info ] [Thread-2 (]: 12 of 17 PASS not_null_stg_users_user_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.385045 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2'
[0m14:53:44.385472 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b'
[0m14:53:44.388030 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.388295 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_stg_users_user_id.980dfc1b77
[0m14:53:44.388457 [debug] [Thread-4 (]: On test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2: Close
[0m14:53:44.388604 [debug] [Thread-1 (]: On test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b: Close
[0m14:53:44.388836 [debug] [Thread-2 (]: Began running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.389224 [info ] [Thread-1 (]: 11 of 17 PASS not_null_stg_users_signed_up_at .................................. [[32mPASS[0m in 0.03s]
[0m14:53:44.389475 [info ] [Thread-4 (]: 10 of 17 PASS not_null_stg_subscriptions_user_id ............................... [[32mPASS[0m in 0.03s]
[0m14:53:44.389688 [info ] [Thread-2 (]: 14 of 17 START test relationships_stg_subscriptions_user_id__user_id__ref_stg_users_  [RUN]
[0m14:53:44.389963 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b
[0m14:53:44.390343 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2
[0m14:53:44.390569 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_user_id.980dfc1b77, now test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e)
[0m14:53:44.390787 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.390950 [debug] [Thread-3 (]: Began executing node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.391131 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.391315 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.391478 [info ] [Thread-1 (]: 15 of 17 START test unique_stg_events_event_id ................................. [RUN]
[0m14:53:44.392795 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.392963 [info ] [Thread-4 (]: 16 of 17 START test unique_stg_subscriptions_subscription_id ................... [RUN]
[0m14:53:44.395506 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.395753 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_users_signed_up_at.a092ef506b, now test.saas_analytics.unique_stg_events_event_id.66ebb17438)
[0m14:53:44.396001 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_stg_subscriptions_user_id.3c73aaf4e2, now test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e)
[0m14:53:44.396225 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.396404 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.399359 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.401121 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.401322 [debug] [Thread-2 (]: Began executing node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.401527 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.402751 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.402926 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: BEGIN
[0m14:53:44.403162 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:53:44.403434 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.403604 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.404732 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.405857 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.406052 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.406226 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m14:53:44.406475 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: BEGIN
[0m14:53:44.406650 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"
[0m14:53:44.406928 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:53:44.407125 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_events"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.407365 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.407545 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.407929 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: BEGIN
[0m14:53:44.408088 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.408240 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: BEGIN
[0m14:53:44.408413 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:44.408565 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"
[0m14:53:44.408717 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.408962 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select user_id as from_field
    from "analytics"."main"."stg_subscriptions"
    where user_id is not null
),

parent as (
    select user_id as to_field
    from "analytics"."main"."stg_users"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m14:53:44.409183 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.409424 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.409601 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_stg_events_event_id.66ebb17438"
[0m14:53:44.409784 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"
[0m14:53:44.409954 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_events_event_id.66ebb17438"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    event_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_events"
where event_id is not null
group by event_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.410185 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    subscription_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_subscriptions"
where subscription_id is not null
group by subscription_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.412123 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:53:44.413310 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: ROLLBACK
[0m14:53:44.413558 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.414060 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e'
[0m14:53:44.414254 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:53:44.414414 [debug] [Thread-3 (]: SQL status: OK in 0.007 seconds
[0m14:53:44.414607 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e: Close
[0m14:53:44.415347 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: ROLLBACK
[0m14:53:44.416044 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: ROLLBACK
[0m14:53:44.416671 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: ROLLBACK
[0m14:53:44.416939 [info ] [Thread-4 (]: 16 of 17 PASS unique_stg_subscriptions_subscription_id ......................... [[32mPASS[0m in 0.02s]
[0m14:53:44.417435 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e'
[0m14:53:44.417885 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_stg_events_event_id.66ebb17438'
[0m14:53:44.418286 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244'
[0m14:53:44.418516 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e
[0m14:53:44.418684 [debug] [Thread-2 (]: On test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e: Close
[0m14:53:44.418848 [debug] [Thread-1 (]: On test.saas_analytics.unique_stg_events_event_id.66ebb17438: Close
[0m14:53:44.418999 [debug] [Thread-3 (]: On test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244: Close
[0m14:53:44.419174 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.419459 [info ] [Thread-2 (]: 14 of 17 PASS relationships_stg_subscriptions_user_id__user_id__ref_stg_users_ . [[32mPASS[0m in 0.03s]
[0m14:53:44.419656 [info ] [Thread-1 (]: 15 of 17 PASS unique_stg_events_event_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:53:44.419949 [info ] [Thread-3 (]: 13 of 17 PASS relationships_stg_events_user_id__user_id__ref_stg_users_ ........ [[32mPASS[0m in 0.04s]
[0m14:53:44.420130 [info ] [Thread-4 (]: 17 of 17 START test unique_stg_users_user_id ................................... [RUN]
[0m14:53:44.420365 [debug] [Thread-2 (]: Finished running node test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e
[0m14:53:44.420580 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_stg_events_event_id.66ebb17438
[0m14:53:44.420785 [debug] [Thread-3 (]: Finished running node test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244
[0m14:53:44.420953 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.unique_stg_subscriptions_subscription_id.d45e893a6e, now test.saas_analytics.unique_stg_users_user_id.c2ff477e6b)
[0m14:53:44.421214 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.423519 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.424028 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.426267 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.426668 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.426826 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: BEGIN
[0m14:53:44.426964 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:53:44.427356 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:53:44.427528 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"
[0m14:53:44.427681 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_stg_users_user_id.c2ff477e6b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."stg_users"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:53:44.429288 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:53:44.430197 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: ROLLBACK
[0m14:53:44.430742 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b'
[0m14:53:44.430935 [debug] [Thread-4 (]: On test.saas_analytics.unique_stg_users_user_id.c2ff477e6b: Close
[0m14:53:44.431249 [info ] [Thread-4 (]: 17 of 17 PASS unique_stg_users_user_id ......................................... [[32mPASS[0m in 0.01s]
[0m14:53:44.431514 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_stg_users_user_id.c2ff477e6b
[0m14:53:44.432187 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.432358 [debug] [MainThread]: On master: BEGIN
[0m14:53:44.432487 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:53:44.432813 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.432949 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.433076 [debug] [MainThread]: Using duckdb connection "master"
[0m14:53:44.433197 [debug] [MainThread]: On master: COMMIT
[0m14:53:44.433400 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:53:44.433529 [debug] [MainThread]: On master: Close
[0m14:53:44.433696 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:44.433815 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_events_event_id.66ebb17438' was properly closed.
[0m14:53:44.433923 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_subscriptions_user_id__user_id__ref_stg_users_.4795ab721e' was properly closed.
[0m14:53:44.434033 [debug] [MainThread]: Connection 'test.saas_analytics.relationships_stg_events_user_id__user_id__ref_stg_users_.df7ad60244' was properly closed.
[0m14:53:44.434137 [debug] [MainThread]: Connection 'test.saas_analytics.unique_stg_users_user_id.c2ff477e6b' was properly closed.
[0m14:53:44.434285 [info ] [MainThread]: 
[0m14:53:44.434425 [info ] [MainThread]: Finished running 17 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:53:44.435459 [debug] [MainThread]: Command end result
[0m14:53:44.448409 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:53:44.449581 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:53:44.453093 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:53:44.453285 [info ] [MainThread]: 
[0m14:53:44.453473 [info ] [MainThread]: [33mCompleted with 1 warning:[0m
[0m14:53:44.453620 [info ] [MainThread]: 
[0m14:53:44.453798 [warn ] [MainThread]: [33mWarning in test not_null_stg_events_event_at (models/staging/staging.yml)[0m
[0m14:53:44.453961 [warn ] [MainThread]: Got 8000 results, configured to warn if != 0
[0m14:53:44.454086 [info ] [MainThread]: 
[0m14:53:44.454235 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/staging/staging.yml/not_null_stg_events_event_at.sql
[0m14:53:44.454369 [info ] [MainThread]: 
[0m14:53:44.454514 [info ] [MainThread]: Done. PASS=16 WARN=1 ERROR=0 SKIP=0 NO-OP=0 TOTAL=17
[0m14:53:44.454783 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:53:44.456274 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.9109618, "process_in_blocks": "0", "process_kernel_time": 0.205519, "process_mem_max_rss": "162791424", "process_out_blocks": "0", "process_user_time": 1.385866}
[0m14:53:44.456550 [debug] [MainThread]: Command `dbt test` succeeded at 14:53:44.456501 after 0.91 seconds
[0m14:53:44.456755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048dfe60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c9d7f0>]}
[0m14:53:44.456942 [debug] [MainThread]: Flushing usage events
[0m14:53:44.557202 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:48:55.068522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106810a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a9640>]}


============================== 15:48:55.076073 | 3f942e24-110a-4e97-b1ae-cda05f7fc5e9 ==============================
[0m15:48:55.076073 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:48:55.076415 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'static_parser': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'partial_parse': 'True', 'empty': 'False', 'log_cache_events': 'False', 'target_path': 'None', 'introspect': 'True', 'printer_width': '80', 'no_print': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'use_colors': 'True', 'warn_error': 'None', 'quiet': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt run --select marts', 'write_json': 'True'}
[0m15:48:55.237961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093d5550>]}
[0m15:48:55.266494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c68aa0>]}
[0m15:48:55.267926 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:48:55.320730 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:48:55.389240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m15:48:55.389583 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/dim_users.sql
[0m15:48:55.389746 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_events.sql
[0m15:48:55.389954 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/marts.yml
[0m15:48:55.599257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099afbf0>]}
[0m15:48:55.673790 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:48:55.675805 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:48:55.689512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b48e30>]}
[0m15:48:55.689791 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:48:55.689961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bbcb90>]}
[0m15:48:55.690923 [info ] [MainThread]: 
[0m15:48:55.691089 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:48:55.691221 [info ] [MainThread]: 
[0m15:48:55.691449 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:48:55.693523 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:48:55.723101 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:48:55.723344 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:48:55.724251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:48:55.741120 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:48:55.742089 [debug] [ThreadPool]: On list_analytics: Close
[0m15:48:55.742451 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:48:55.742688 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:48:55.746618 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.746790 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:48:55.746924 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:48:55.747793 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:48:55.748419 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.748556 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:48:55.748775 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.748902 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.749022 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:48:55.749480 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.749850 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:48:55.749979 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:48:55.750094 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:48:55.750287 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.750412 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:48:55.750992 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:48:55.753651 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:48:55.753870 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:48:55.754021 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:48:55.754460 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:48:55.754597 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:48:55.754733 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:48:55.765396 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:48:55.766210 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:48:55.767210 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:48:55.767352 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:48:55.767972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ae9130>]}
[0m15:48:55.768238 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.768372 [debug] [MainThread]: On master: BEGIN
[0m15:48:55.768496 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:48:55.768744 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.768869 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.768991 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.769109 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.769290 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.769412 [debug] [MainThread]: On master: Close
[0m15:48:55.770865 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:48:55.771088 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:48:55.771342 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:48:55.771590 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:48:55.771832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:48:55.772051 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:48:55.772218 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:48:55.772371 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:48:55.776377 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:48:55.778027 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:48:55.778694 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:48:55.778875 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:48:55.795042 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:48:55.796655 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:48:55.797221 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:48:55.797389 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:48:55.797536 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:48:55.797843 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:48:55.798112 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:48:55.798285 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:48:55.798429 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:48:55.798602 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:48:55.798883 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:48:55.802426 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:48:55.802660 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:48:55.802819 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:48:55.803038 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:48:55.803194 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:48:55.803463 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
        u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:48:55.804014 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
        u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:48:55.804249 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:48:55.805066 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:48:55.809541 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:48:55.809720 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:48:55.810458 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:48:55.810652 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:48:55.812431 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:48:55.813848 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^
[0m15:48:55.814035 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ecf4a0>]}
[0m15:48:55.814246 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f942e24-110a-4e97-b1ae-cda05f7fc5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ef5f10>]}
[0m15:48:55.814565 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:48:55.815101 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:48:55.814884 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:48:55.815388 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^.
[0m15:48:55.815736 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:48:55.816273 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^.
[0m15:48:55.816899 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.817042 [debug] [MainThread]: On master: BEGIN
[0m15:48:55.817163 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:48:55.817497 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.817629 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.817753 [debug] [MainThread]: Using duckdb connection "master"
[0m15:48:55.817868 [debug] [MainThread]: On master: COMMIT
[0m15:48:55.818076 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:48:55.818235 [debug] [MainThread]: On master: Close
[0m15:48:55.818526 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:48:55.818684 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:48:55.818814 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:48:55.818991 [info ] [MainThread]: 
[0m15:48:55.819146 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:48:55.819522 [debug] [MainThread]: Command end result
[0m15:48:55.839413 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:48:55.840520 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:48:55.844101 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:48:55.844281 [info ] [MainThread]: 
[0m15:48:55.844463 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:48:55.844600 [info ] [MainThread]: 
[0m15:48:55.844773 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:48:55.844930 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:48:55.845056 [info ] [MainThread]: 
[0m15:48:55.845198 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:48:55.845325 [info ] [MainThread]: 
[0m15:48:55.845472 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:48:55.845617 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 113:         u.user_id = e.user_id
                    ^
[0m15:48:55.845730 [info ] [MainThread]: 
[0m15:48:55.845869 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:48:55.845985 [info ] [MainThread]: 
[0m15:48:55.846123 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:48:55.847171 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.817876, "process_in_blocks": "0", "process_kernel_time": 0.205791, "process_mem_max_rss": "149913600", "process_out_blocks": "0", "process_user_time": 1.270329}
[0m15:48:55.847386 [debug] [MainThread]: Command `dbt run` failed at 15:48:55.847344 after 0.82 seconds
[0m15:48:55.847562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106810a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10854cbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109165cd0>]}
[0m15:48:55.847734 [debug] [MainThread]: Flushing usage events
[0m15:48:55.981522 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:49:47.157057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ce2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b57c0>]}


============================== 15:49:47.163631 | d0b003f2-98bd-43ba-9a7e-61f863ea5223 ==============================
[0m15:49:47.163631 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:49:47.163974 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'version_check': 'True', 'introspect': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'empty': 'False', 'printer_width': '80', 'invocation_command': 'dbt run --select marts', 'warn_error': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'no_print': 'None', 'indirect_selection': 'eager', 'fail_fast': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False'}
[0m15:49:47.318618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090e4e90>]}
[0m15:49:47.346908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109557830>]}
[0m15:49:47.348280 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:49:47.399718 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:49:47.464483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:49:47.464906 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/dim_users.sql
[0m15:49:47.663716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1adf70>]}
[0m15:49:47.736966 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:49:47.738370 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:49:47.750321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbcafc0>]}
[0m15:49:47.750602 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:49:47.750779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d162840>]}
[0m15:49:47.751760 [info ] [MainThread]: 
[0m15:49:47.751938 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:49:47.752071 [info ] [MainThread]: 
[0m15:49:47.752300 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:49:47.754384 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:49:47.781139 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:49:47.781389 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:49:47.781537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:49:47.796252 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:49:47.797017 [debug] [ThreadPool]: On list_analytics: Close
[0m15:49:47.797366 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:49:47.797581 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:49:47.800831 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.801007 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:49:47.801139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:49:47.801824 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:49:47.802503 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.802655 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:49:47.802892 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.803024 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.803149 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:49:47.803548 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.803936 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:49:47.804070 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:49:47.804184 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:49:47.804379 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.804507 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:49:47.805130 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:49:47.807690 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:49:47.808634 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:49:47.808815 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:49:47.809104 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:49:47.809240 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:49:47.809385 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:49:47.819103 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:49:47.819861 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:49:47.820793 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:49:47.820956 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:49:47.821582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc22ed0>]}
[0m15:49:47.821794 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.821918 [debug] [MainThread]: On master: BEGIN
[0m15:49:47.822034 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:49:47.822280 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.822407 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.822524 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.822638 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.822841 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.822962 [debug] [MainThread]: On master: Close
[0m15:49:47.824206 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:49:47.824393 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:49:47.824631 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:49:47.824873 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:49:47.825094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:49:47.825313 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:49:47.825475 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:49:47.825627 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:49:47.829585 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:49:47.831320 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:49:47.831834 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:49:47.844615 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:49:47.847868 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:49:47.849480 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:49:47.849993 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:49:47.850187 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:49:47.850346 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:49:47.850504 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:49:47.850656 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:49:47.850801 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:49:47.851202 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:49:47.851423 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:49:47.851573 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:49:47.851730 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:49:47.851948 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:49:47.852264 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:49:47.854772 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * form events_with_context
    );
  
  
[0m15:49:47.855238 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:49:47.855438 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:49:47.856777 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:49:47.857043 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:49:47.857251 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:49:47.861436 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:49:47.861630 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:49:47.862322 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:49:47.862992 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:49:47.864157 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:49:47.865468 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:49:47.865841 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10899e870>]}
[0m15:49:47.866005 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0b003f2-98bd-43ba-9a7e-61f863ea5223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4f6480>]}
[0m15:49:47.866295 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:49:47.866801 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:49:47.866533 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:49:47.867073 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:49:47.867306 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:49:47.867828 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^.
[0m15:49:47.868406 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.868554 [debug] [MainThread]: On master: BEGIN
[0m15:49:47.868681 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:49:47.868997 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.869128 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.869249 [debug] [MainThread]: Using duckdb connection "master"
[0m15:49:47.869365 [debug] [MainThread]: On master: COMMIT
[0m15:49:47.869572 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:49:47.869697 [debug] [MainThread]: On master: Close
[0m15:49:47.869855 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:49:47.869973 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:49:47.870081 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:49:47.870223 [info ] [MainThread]: 
[0m15:49:47.870355 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m15:49:47.870702 [debug] [MainThread]: Command end result
[0m15:49:47.884824 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:49:47.885880 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:49:47.888592 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:49:47.888753 [info ] [MainThread]: 
[0m15:49:47.888930 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:49:47.889072 [info ] [MainThread]: 
[0m15:49:47.889243 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:49:47.889407 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:49:47.889532 [info ] [MainThread]: 
[0m15:49:47.889678 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:49:47.889808 [info ] [MainThread]: 
[0m15:49:47.889953 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:49:47.890098 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Parser Error: syntax error at or near "events_with_context"
  
  LINE 56: select * form events_with_context
                         ^
[0m15:49:47.890218 [info ] [MainThread]: 
[0m15:49:47.890357 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:49:47.890472 [info ] [MainThread]: 
[0m15:49:47.890605 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:49:47.891982 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.77687764, "process_in_blocks": "0", "process_kernel_time": 0.182145, "process_mem_max_rss": "150044672", "process_out_blocks": "0", "process_user_time": 1.240899}
[0m15:49:47.892185 [debug] [MainThread]: Command `dbt run` failed at 15:49:47.892149 after 0.78 seconds
[0m15:49:47.892357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089d05c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089d14f0>]}
[0m15:49:47.892521 [debug] [MainThread]: Flushing usage events
[0m15:49:47.991649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:50:05.112324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1103f77d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5520>]}


============================== 15:50:05.116283 | 2a9d5b37-6906-4a77-ba46-59cb7c9a9107 ==============================
[0m15:50:05.116283 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:50:05.116621 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'quiet': 'False', 'debug': 'False', 'log_format': 'default', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'write_json': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'no_print': 'None', 'warn_error': 'None', 'invocation_command': 'dbt run --select marts', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'printer_width': '80', 'partial_parse': 'True'}
[0m15:50:05.290337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f49880>]}
[0m15:50:05.318625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111370830>]}
[0m15:50:05.319968 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:50:05.379575 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:50:05.447209 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:50:05.447656 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/fct_events.sql
[0m15:50:05.640187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111fc6db0>]}
[0m15:50:05.720040 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:50:05.721941 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:50:05.737235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c060f0>]}
[0m15:50:05.737521 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:50:05.737699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128d2ea0>]}
[0m15:50:05.738619 [info ] [MainThread]: 
[0m15:50:05.738801 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:50:05.738937 [info ] [MainThread]: 
[0m15:50:05.739162 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:50:05.741266 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:50:05.770275 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:50:05.770508 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:50:05.770658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:05.788097 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:50:05.788872 [debug] [ThreadPool]: On list_analytics: Close
[0m15:50:05.789228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:50:05.789485 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:50:05.792676 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.792845 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:50:05.792977 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:50:05.793632 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:50:05.794245 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.794376 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:50:05.794597 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.794717 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.794908 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:50:05.795219 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.795598 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:50:05.795726 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:50:05.795837 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:50:05.796034 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.796160 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:50:05.796734 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:50:05.799273 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:50:05.799422 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:50:05.799595 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:50:05.799837 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:50:05.799966 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:50:05.800097 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:50:05.810819 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:50:05.811561 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:50:05.812544 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:50:05.812707 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:50:05.813536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128d27e0>]}
[0m15:50:05.813763 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.813894 [debug] [MainThread]: On master: BEGIN
[0m15:50:05.814012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:50:05.814260 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.814387 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.814510 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.814627 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.814820 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.814941 [debug] [MainThread]: On master: Close
[0m15:50:05.816294 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:50:05.816470 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:50:05.816702 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:50:05.817111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:50:05.816904 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:50:05.817313 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:50:05.817520 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:50:05.822525 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:50:05.822776 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:50:05.824726 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:50:05.825343 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:50:05.825556 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:50:05.842563 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:50:05.843355 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:50:05.843989 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:50:05.844170 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:50:05.844316 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:50:05.844609 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:50:05.844776 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:50:05.844916 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:50:05.845169 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:50:05.845315 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:50:05.845551 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:50:05.846092 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:50:05.846247 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:50:05.846428 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:50:05.849929 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquistion_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:50:05.850286 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:50:05.850533 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m15:50:05.850698 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:50:05.850926 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: ROLLBACK
[0m15:50:05.851123 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:50:05.856411 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:50:05.857122 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.fct_events'
[0m15:50:05.857290 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:50:05.857468 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:50:05.859171 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:50:05.860120 [debug] [Thread-2 (]: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^
[0m15:50:05.861216 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112803500>]}
[0m15:50:05.861378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a9d5b37-6906-4a77-ba46-59cb7c9a9107', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b274d0>]}
[0m15:50:05.861689 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.fct_events .......................... [[31mERROR[0m in 0.04s]
[0m15:50:05.862191 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:50:05.861928 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:50:05.862466 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_events' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^.
[0m15:50:05.862695 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:50:05.863207 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:50:05.863793 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.863938 [debug] [MainThread]: On master: BEGIN
[0m15:50:05.864057 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:50:05.864341 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.864469 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.864592 [debug] [MainThread]: Using duckdb connection "master"
[0m15:50:05.864708 [debug] [MainThread]: On master: COMMIT
[0m15:50:05.864895 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:50:05.865019 [debug] [MainThread]: On master: Close
[0m15:50:05.865178 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:50:05.865287 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:50:05.865389 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:50:05.865529 [info ] [MainThread]: 
[0m15:50:05.865664 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m15:50:05.866006 [debug] [MainThread]: Command end result
[0m15:50:05.880415 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:50:05.881595 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:50:05.884364 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:50:05.884533 [info ] [MainThread]: 
[0m15:50:05.884712 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:50:05.884859 [info ] [MainThread]: 
[0m15:50:05.885045 [error] [MainThread]: [31mFailure in model fct_events (models/marts/fct_events.sql)[0m
[0m15:50:05.885209 [error] [MainThread]:   Runtime Error in model fct_events (models/marts/fct_events.sql)
  Binder Error: Values list "users" does not have a column named "acquistion_channel"
  
  LINE 30:         users.acquistion_channel,
                   ^
[0m15:50:05.885335 [info ] [MainThread]: 
[0m15:50:05.885483 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/fct_events.sql
[0m15:50:05.885607 [info ] [MainThread]: 
[0m15:50:05.885751 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:50:05.885903 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:50:05.886025 [info ] [MainThread]: 
[0m15:50:05.886163 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:50:05.886280 [info ] [MainThread]: 
[0m15:50:05.886420 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m15:50:05.887777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8143798, "process_in_blocks": "0", "process_kernel_time": 0.198754, "process_mem_max_rss": "150077440", "process_out_blocks": "0", "process_user_time": 1.238241}
[0m15:50:05.888016 [debug] [MainThread]: Command `dbt run` failed at 15:50:05.887973 after 0.81 seconds
[0m15:50:05.888203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114b5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ee08f0>]}
[0m15:50:05.888373 [debug] [MainThread]: Flushing usage events
[0m15:50:05.985365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:51:52.959202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a0ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058759d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105875760>]}


============================== 15:51:52.966284 | b5001dfd-e186-44be-8218-e9a93fa37978 ==============================
[0m15:51:52.966284 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:51:52.966608 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'write_json': 'True', 'partial_parse': 'True', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'static_parser': 'True', 'version_check': 'True', 'no_print': 'None', 'quiet': 'False', 'debug': 'False', 'log_format': 'default', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select marts', 'target_path': 'None', 'use_colors': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'indirect_selection': 'eager'}
[0m15:51:53.133528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105678b30>]}
[0m15:51:53.162450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105233e30>]}
[0m15:51:53.163867 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:51:53.215460 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:51:53.282808 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:53.283215 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/fct_events.sql
[0m15:51:53.476855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616f8f0>]}
[0m15:51:53.552299 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:51:53.553536 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:51:53.566788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106068800>]}
[0m15:51:53.567078 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:51:53.567262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d63140>]}
[0m15:51:53.568198 [info ] [MainThread]: 
[0m15:51:53.568457 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:51:53.568635 [info ] [MainThread]: 
[0m15:51:53.568885 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:51:53.571033 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:51:53.600525 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:51:53.600764 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:51:53.600994 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:53.618958 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:51:53.619947 [debug] [ThreadPool]: On list_analytics: Close
[0m15:51:53.620295 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:51:53.620515 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:51:53.623725 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.623894 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:51:53.624024 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:51:53.624763 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:51:53.625379 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.625515 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:51:53.625852 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.625976 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.626100 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:51:53.626377 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.626834 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:51:53.626993 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:51:53.627124 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:51:53.627341 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.627478 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:51:53.628086 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:51:53.630731 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:51:53.630897 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:51:53.631081 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:51:53.631327 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:51:53.631459 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:51:53.631590 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:51:53.642325 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:51:53.643048 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:51:53.644026 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:51:53.644185 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:51:53.645035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e8fef0>]}
[0m15:51:53.645265 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.645391 [debug] [MainThread]: On master: BEGIN
[0m15:51:53.645505 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:51:53.645751 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.645875 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.645994 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.646109 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.646292 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.646417 [debug] [MainThread]: On master: Close
[0m15:51:53.647722 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:51:53.647913 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:51:53.648146 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:51:53.648549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:51:53.648340 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:51:53.648751 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:51:53.648958 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:51:53.653750 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:51:53.653957 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:51:53.655835 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:51:53.656543 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:51:53.656759 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:51:53.672267 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:51:53.673820 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:51:53.674538 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.674706 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:51:53.674853 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:51:53.675200 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:51:53.675384 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:51:53.675536 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:51:53.675785 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.675968 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.676177 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquisition_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:51:53.676551 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.676694 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:51:53.676923 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:51:53.682012 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."stg_users"
),

events as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

user_events as (
    select
        user_id,
        min(event_at) as first_event_at,
        max(event_at) as last_event_at,
        count(*) as total_events,
        count(distinct event_name) as unique_event_types
    from events 
    where event_at is not null --exclude events with null timestamps
    group by user_id 
),

user_current_subscription as (
    select 
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed
    from (
        select
            *,
            row_number() over (
                partition by user_id
                order by  
                    is_active desc, -- active subscriptions first
                    subscription_started_at desc -- then most recent
            ) as rn 
        from subscriptions
    )
    where rn = 1
),

user_subscription_metrics as (
    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns
    from subscriptions 
    group by user_id
),

-- bringing it all together
final as (
    select
        u.user_id,
        u.signed_up_at,
        u.acquistion_channel,
        u.country,

        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        --lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,
        datediff('day', u.signed_up_at, current_date) as days_since_signup,
        case
            when e.last_event_at is null then null 
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event
    from users u 
    left join user_events e 
       on u.user_id = e.user_id
    left join user_current_subscription cs 
        on u.user_id = cs.user_id 
    left join user_subscription_metrics sm 
        on u.user_id = sm.user_id 
)

select * from final
    );
  
  
[0m15:51:53.682310 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:51:53.682545 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: ROLLBACK
[0m15:51:53.687723 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.dim_users'
[0m15:51:53.687970 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:51:53.689347 [debug] [Thread-1 (]: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:51:53.690429 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063eaba0>]}
[0m15:51:53.690781 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.dim_users ........................... [[31mERROR[0m in 0.04s]
[0m15:51:53.691117 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:51:53.691368 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.dim_users' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^.
[0m15:51:53.774349 [debug] [Thread-2 (]: SQL status: OK in 0.098 seconds
[0m15:51:53.778205 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.778562 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events__dbt_tmp" rename to "fct_events"
[0m15:51:53.779462 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.787641 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:51:53.787861 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.788023 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:51:53.795183 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m15:51:53.798014 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:51:53.798196 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

      drop table if exists "analytics"."main"."fct_events__dbt_backup" cascade
    
[0m15:51:53.798865 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:51:53.799972 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:51:53.800257 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5001dfd-e186-44be-8218-e9a93fa37978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102f1ee40>]}
[0m15:51:53.800551 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.fct_events .............................. [[32mOK[0m in 0.15s]
[0m15:51:53.800801 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:51:53.801440 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.801607 [debug] [MainThread]: On master: BEGIN
[0m15:51:53.801737 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:51:53.802000 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.802130 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.802255 [debug] [MainThread]: Using duckdb connection "master"
[0m15:51:53.802373 [debug] [MainThread]: On master: COMMIT
[0m15:51:53.802572 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:51:53.802697 [debug] [MainThread]: On master: Close
[0m15:51:53.802859 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:51:53.802987 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:51:53.803099 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:51:53.803255 [info ] [MainThread]: 
[0m15:51:53.803395 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:51:53.803745 [debug] [MainThread]: Command end result
[0m15:51:53.819054 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:51:53.820188 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:51:53.823331 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:51:53.823535 [info ] [MainThread]: 
[0m15:51:53.823723 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:51:53.823875 [info ] [MainThread]: 
[0m15:51:53.824059 [error] [MainThread]: [31mFailure in model dim_users (models/marts/dim_users.sql)[0m
[0m15:51:53.824235 [error] [MainThread]:   Runtime Error in model dim_users (models/marts/dim_users.sql)
  Binder Error: Referenced column "event_at" not found in FROM clause!
  Candidate bindings: "signed_up_at", "user_id", "country"
  
  LINE 33:     where event_at is not null --exclude events with null timestamps
                     ^
[0m15:51:53.824368 [info ] [MainThread]: 
[0m15:51:53.824519 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/dim_users.sql
[0m15:51:53.824655 [info ] [MainThread]: 
[0m15:51:53.824803 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m15:51:53.826339 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9080825, "process_in_blocks": "0", "process_kernel_time": 0.203356, "process_mem_max_rss": "174784512", "process_out_blocks": "0", "process_user_time": 1.442706}
[0m15:51:53.826589 [debug] [MainThread]: Command `dbt run` failed at 15:51:53.826546 after 0.91 seconds
[0m15:51:53.826782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053eb140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105731e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b56cf0>]}
[0m15:51:53.826958 [debug] [MainThread]: Flushing usage events
[0m15:51:53.945795 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:53:00.973497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107327500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5910>]}


============================== 15:53:00.981338 | 93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e ==============================
[0m15:53:00.981338 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:53:00.981667 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select marts', 'indirect_selection': 'eager', 'use_colors': 'True', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'partial_parse': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'empty': 'False', 'version_check': 'True', 'introspect': 'True', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'debug': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'send_anonymous_usage_stats': 'True'}
[0m15:53:01.148225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e52000>]}
[0m15:53:01.176852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077b0b60>]}
[0m15:53:01.178165 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:53:01.230092 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:53:01.297035 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:53:01.297448 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/dim_users.sql
[0m15:53:01.495509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11286e870>]}
[0m15:53:01.571767 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:53:01.573535 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:53:01.589769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129a95b0>]}
[0m15:53:01.590069 [info ] [MainThread]: Found 5 models, 26 data tests, 3 sources, 472 macros
[0m15:53:01.590245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bafa70>]}
[0m15:53:01.591151 [info ] [MainThread]: 
[0m15:53:01.591316 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:53:01.591448 [info ] [MainThread]: 
[0m15:53:01.591675 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:53:01.593774 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:53:01.622185 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:53:01.622430 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:53:01.622575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:01.639161 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:53:01.640132 [debug] [ThreadPool]: On list_analytics: Close
[0m15:53:01.640481 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:53:01.640725 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:53:01.643958 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.644125 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:53:01.644259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:53:01.645045 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:53:01.645637 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.645772 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:53:01.645986 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.646111 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.646234 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:53:01.646694 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.647055 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:53:01.647184 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:53:01.647296 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:53:01.647486 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.647611 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:53:01.648339 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:53:01.651049 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:53:01.651957 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:53:01.652103 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:53:01.652423 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:53:01.652556 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:53:01.652689 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:53:01.666459 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m15:53:01.667774 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:53:01.668814 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:53:01.668965 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:53:01.669677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e3b110>]}
[0m15:53:01.669933 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.670070 [debug] [MainThread]: On master: BEGIN
[0m15:53:01.670190 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:53:01.670445 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.670575 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.670699 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.670811 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.671000 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.671126 [debug] [MainThread]: On master: Close
[0m15:53:01.672596 [debug] [Thread-1 (]: Began running node model.saas_analytics.dim_users
[0m15:53:01.672773 [debug] [Thread-2 (]: Began running node model.saas_analytics.fct_events
[0m15:53:01.673013 [info ] [Thread-1 (]: 1 of 2 START sql table model main.dim_users .................................... [RUN]
[0m15:53:01.673429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.dim_users)
[0m15:53:01.673211 [info ] [Thread-2 (]: 2 of 2 START sql table model main.fct_events ................................... [RUN]
[0m15:53:01.673637 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.dim_users
[0m15:53:01.673846 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_events'
[0m15:53:01.678047 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.dim_users"
[0m15:53:01.678246 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.fct_events
[0m15:53:01.679932 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.fct_events"
[0m15:53:01.680639 [debug] [Thread-1 (]: Began executing node model.saas_analytics.dim_users
[0m15:53:01.693430 [debug] [Thread-2 (]: Began executing node model.saas_analytics.fct_events
[0m15:53:01.696892 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.dim_users"
[0m15:53:01.698607 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.fct_events"
[0m15:53:01.699263 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.699438 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: BEGIN
[0m15:53:01.699619 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.699797 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:53:01.699988 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: BEGIN
[0m15:53:01.700297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:53:01.700526 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.700688 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.700842 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.701040 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

  
    
    

    create  table
      "analytics"."main"."fct_events__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."stg_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

-- join events to their user context
events_with_users as (
    select
        events.*,
        users.signed_up_at,
        users.acquisition_channel,
        users.country
    from events 
    left join users 
        on events.user_id = users.user_id 
),

--join events to subscriptions that are active at event time
-- this is a point-in-time join: which subscription was active when the event fired?
events_with_context as (
    select 
        e.*,
        s.subscription_id,
        s.plan as subscription_plan,
        s.subscription_status,
        s.subscription_started_at,
        s.subscription_ended_at,
        s.monthly_revenue_usd,
        datediff('day', e.signed_up_at, e.event_at) as days_since_signup
    from events_with_users e 
    left join subscriptions s 
        on e.user_id = s.user_id
        and e.event_at >= s.subscription_started_at
        and (e.event_at < s.subscription_ended_at or s.subscription_ended_at is null)
)

select * from events_with_context
    );
  
  
[0m15:53:01.701232 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.701636 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

  
    
    

    create  table
      "analytics"."main"."dim_users__dbt_tmp"
  
    as (
      

with users as (

    select * from "analytics"."main"."stg_users"

),

events as (

    select * from "analytics"."main"."stg_events"

),

subscriptions as (

    select * from "analytics"."main"."stg_subscriptions"

),

-- Aggregate event activity per user
user_events as (

    select
        events.user_id,
        min(events.event_at) as first_event_at,
        max(events.event_at) as last_event_at,
        count(*) as total_events,
        count(distinct events.event_name) as unique_event_types

    from events
    where events.event_at is not null  -- exclude events with null timestamps
    group by events.user_id

),

-- Get current subscription (most recent active or most recent ended)
user_current_subscription as (

    select
        user_id,
        subscription_id as current_subscription_id,
        plan as current_plan,
        subscription_status as current_subscription_status,
        subscription_started_at as current_subscription_started_at,
        subscription_ended_at as current_subscription_ended_at,
        monthly_revenue_usd as current_monthly_revenue_usd,
        is_active as is_currently_subscribed

    from (
        select
            *,
            row_number() over (
                partition by user_id 
                order by 
                    is_active desc,  -- active subscriptions first
                    subscription_started_at desc  -- then most recent
            ) as rn
        from subscriptions
    )
    where rn = 1

),

-- Calculate lifetime subscription metrics
user_subscription_metrics as (

    select
        user_id,
        count(*) as total_subscriptions,
        sum(monthly_revenue_usd) as lifetime_revenue_usd,
        max(case when is_churned then subscription_ended_at end) as last_churn_date,
        sum(case when is_churned then 1 else 0 end) as total_churns

    from subscriptions
    group by user_id

),

-- Bring it all together
final as (

    select
        -- user identity
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,

        -- event activity
        e.first_event_at,
        e.last_event_at,
        e.total_events,
        e.unique_event_types,

        -- current subscription
        cs.current_subscription_id,
        cs.current_plan,
        cs.current_subscription_status,
        cs.current_subscription_started_at,
        cs.current_subscription_ended_at,
        cs.current_monthly_revenue_usd,
        cs.is_currently_subscribed,

        -- lifetime subscription metrics
        coalesce(sm.total_subscriptions, 0) as total_subscriptions,
        coalesce(sm.lifetime_revenue_usd, 0) as lifetime_revenue_usd,
        sm.last_churn_date,
        coalesce(sm.total_churns, 0) as total_churns,

        -- derived: user lifecycle stage
        case
            when cs.is_currently_subscribed then 'active'
            when sm.total_churns > 0 then 'churned'
            when sm.total_subscriptions = 0 then 'never_subscribed'
            else 'other'
        end as user_lifecycle_stage,

        -- derived: days since signup
        datediff('day', u.signed_up_at, current_date) as days_since_signup,

        -- derived: recency (days since last event)
        case
            when e.last_event_at is null then null
            else datediff('day', e.last_event_at, current_date)
        end as days_since_last_event

    from users u
    left join user_events e
        on u.user_id = e.user_id
    left join user_current_subscription cs
        on u.user_id = cs.user_id
    left join user_subscription_metrics sm
        on u.user_id = sm.user_id

)

select * from final
    );
  
  
[0m15:53:01.725036 [debug] [Thread-1 (]: SQL status: OK in 0.023 seconds
[0m15:53:01.728827 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.729060 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */
alter table "analytics"."main"."dim_users__dbt_tmp" rename to "dim_users"
[0m15:53:01.734296 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m15:53:01.741855 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: COMMIT
[0m15:53:01.742228 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.742412 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: COMMIT
[0m15:53:01.746510 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m15:53:01.751648 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.dim_users"
[0m15:53:01.751947 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.dim_users"} */

      drop table if exists "analytics"."main"."dim_users__dbt_backup" cascade
    
[0m15:53:01.752748 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.754046 [debug] [Thread-1 (]: On model.saas_analytics.dim_users: Close
[0m15:53:01.755196 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112baf5c0>]}
[0m15:53:01.755566 [info ] [Thread-1 (]: 1 of 2 OK created sql table model main.dim_users ............................... [[32mOK[0m in 0.08s]
[0m15:53:01.755911 [debug] [Thread-1 (]: Finished running node model.saas_analytics.dim_users
[0m15:53:01.811255 [debug] [Thread-2 (]: SQL status: OK in 0.107 seconds
[0m15:53:01.815248 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.815501 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_events'
  
[0m15:53:01.816062 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.816710 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.816873 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_events'
  
[0m15:53:01.818052 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:53:01.820646 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.820867 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events" rename to "fct_events__dbt_backup"
[0m15:53:01.821300 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.822885 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.823071 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */
alter table "analytics"."main"."fct_events__dbt_tmp" rename to "fct_events"
[0m15:53:01.823437 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:53:01.825439 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:53:01.825656 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.825822 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: COMMIT
[0m15:53:01.838610 [debug] [Thread-2 (]: SQL status: OK in 0.013 seconds
[0m15:53:01.840503 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.fct_events"
[0m15:53:01.840770 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_events"} */

      drop table if exists "analytics"."main"."fct_events__dbt_backup" cascade
    
[0m15:53:01.842653 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:53:01.843603 [debug] [Thread-2 (]: On model.saas_analytics.fct_events: Close
[0m15:53:01.843937 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93e6d075-ecf7-4cf3-a6f6-cf7067f79d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ef67b0>]}
[0m15:53:01.844291 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.fct_events .............................. [[32mOK[0m in 0.17s]
[0m15:53:01.844622 [debug] [Thread-2 (]: Finished running node model.saas_analytics.fct_events
[0m15:53:01.845385 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.845564 [debug] [MainThread]: On master: BEGIN
[0m15:53:01.845699 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:53:01.846055 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.846193 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.846324 [debug] [MainThread]: Using duckdb connection "master"
[0m15:53:01.846445 [debug] [MainThread]: On master: COMMIT
[0m15:53:01.846660 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:53:01.846791 [debug] [MainThread]: On master: Close
[0m15:53:01.846975 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:53:01.847094 [debug] [MainThread]: Connection 'model.saas_analytics.dim_users' was properly closed.
[0m15:53:01.847207 [debug] [MainThread]: Connection 'model.saas_analytics.fct_events' was properly closed.
[0m15:53:01.847384 [info ] [MainThread]: 
[0m15:53:01.847569 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m15:53:01.847966 [debug] [MainThread]: Command end result
[0m15:53:01.864131 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:53:01.865412 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:53:01.868395 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:53:01.868565 [info ] [MainThread]: 
[0m15:53:01.868760 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:53:01.868974 [info ] [MainThread]: 
[0m15:53:01.869147 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m15:53:01.870540 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9370321, "process_in_blocks": "0", "process_kernel_time": 0.212514, "process_mem_max_rss": "200802304", "process_out_blocks": "0", "process_user_time": 1.467614}
[0m15:53:01.870761 [debug] [MainThread]: Command `dbt run` succeeded at 15:53:01.870720 after 0.94 seconds
[0m15:53:01.870942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ab5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107327500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107696180>]}
[0m15:53:01.871109 [debug] [MainThread]: Flushing usage events
[0m15:53:01.969783 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:17:12.151069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10430b530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050618b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105061640>]}


============================== 14:17:12.163623 | 1a8631c7-db73-4902-9451-ec73ef810e99 ==============================
[0m14:17:12.163623 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:17:12.163961 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select experiments', 'introspect': 'True', 'quiet': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'use_colors': 'True', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'log_format': 'default', 'write_json': 'True'}
[0m14:17:12.332657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a8631c7-db73-4902-9451-ec73ef810e99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105060e90>]}
[0m14:17:12.362573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a8631c7-db73-4902-9451-ec73ef810e99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10518c470>]}
[0m14:17:12.363938 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:17:12.417182 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:17:12.475148 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: marts/experiments.yml - Runtime Error
    Syntax error near line 44
    ------------------------------
    41 |       - name: assigned_at_signup
    42 |         description: True if the user was assigned to the experiment on their signup day.
    43 | 
    44 |     - name: fct_experiment_conversions
    45 |       description: >
    46 |         One row per user-conversion_event pair, showing when (and if) a user converted
    47 |         on a key metric after being assigned to an experiment variant. Only includes
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 4, column 5
    did not find expected key
      in "<unicode string>", line 44, column 5
[0m14:17:12.481796 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.3722872, "process_in_blocks": "0", "process_kernel_time": 0.197851, "process_mem_max_rss": "132775936", "process_out_blocks": "0", "process_user_time": 0.896562}
[0m14:17:12.482142 [debug] [MainThread]: Command `dbt run` failed at 14:17:12.482066 after 0.37 seconds
[0m14:17:12.482398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105061700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052519a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052ebd40>]}
[0m14:17:12.482616 [debug] [MainThread]: Flushing usage events
[0m14:17:12.650255 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:17:41.234370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cbb620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331850>]}


============================== 14:17:41.238478 | 5a23bb44-6884-462a-8a23-651fdfc4240a ==============================
[0m14:17:41.238478 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:17:41.238848 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'indirect_selection': 'eager', 'log_format': 'default', 'invocation_command': 'dbt run --select experiments', 'fail_fast': 'False', 'empty': 'False', 'static_parser': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'printer_width': '80', 'warn_error': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'target_path': 'None', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:17:41.408643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a23bb44-6884-462a-8a23-651fdfc4240a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092ca2d0>]}
[0m14:17:41.437829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a23bb44-6884-462a-8a23-651fdfc4240a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c73bc0>]}
[0m14:17:41.439281 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:17:41.501940 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:17:41.573858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:17:41.574250 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_assignments.sql
[0m14:17:41.574423 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:17:41.574640 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:17:41.574776 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:17:41.768484 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.fct_experiment_conversions' (models/marts/fct_experiment_conversions.sql) depends on a node named 'fct_experiments_assignments' which was not found
[0m14:17:41.770196 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5788519, "process_in_blocks": "0", "process_kernel_time": 0.172149, "process_mem_max_rss": "137019392", "process_out_blocks": "0", "process_user_time": 1.060322}
[0m14:17:41.770471 [debug] [MainThread]: Command `dbt run` failed at 14:17:41.770421 after 0.58 seconds
[0m14:17:41.770683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109331910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b56f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a0c560>]}
[0m14:17:41.770867 [debug] [MainThread]: Flushing usage events
[0m14:17:41.882810 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:23.465706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108249eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1910>]}


============================== 14:19:23.476739 | 11a35a8c-2080-4508-af78-0ee2a677d63c ==============================
[0m14:19:23.476739 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:19:23.477148 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None', 'indirect_selection': 'eager', 'empty': 'False', 'invocation_command': 'dbt run --select experiments', 'warn_error': 'None', 'log_format': 'default', 'write_json': 'True', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'printer_width': '80', 'introspect': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'no_print': 'None'}
[0m14:19:23.643549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '11a35a8c-2080-4508-af78-0ee2a677d63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10903e3f0>]}
[0m14:19:23.672144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '11a35a8c-2080-4508-af78-0ee2a677d63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7b020>]}
[0m14:19:23.673451 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:19:23.724922 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:19:23.795157 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:19:23.795586 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:19:23.795824 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:19:23.795967 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:19:23.796101 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:19:23.978886 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.rpt_experiment_results' (models/marts/rpt_experiment_results.sql) depends on a node named 'fct_experiment_assignments' which was not found
[0m14:19:23.980760 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5573125, "process_in_blocks": "0", "process_kernel_time": 0.174344, "process_mem_max_rss": "138018816", "process_out_blocks": "0", "process_user_time": 1.039068}
[0m14:19:23.981015 [debug] [MainThread]: Command `dbt run` failed at 14:19:23.980966 after 0.56 seconds
[0m14:19:23.981214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108fa1a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10926af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096a3c20>]}
[0m14:19:23.981393 [debug] [MainThread]: Flushing usage events
[0m14:19:24.114566 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:56.849075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caaff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053698e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105369670>]}


============================== 14:19:56.852623 | 282d0e2e-cd3e-44f0-9b96-91259616d698 ==============================
[0m14:19:56.852623 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:19:56.852942 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'introspect': 'True', 'version_check': 'True', 'target_path': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select experiments', 'profiles_dir': '/Users/hazeldonaldson/.dbt'}
[0m14:19:56.997600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105394fe0>]}
[0m14:19:57.026589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d24e90>]}
[0m14:19:57.028741 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:19:57.080456 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:19:57.146821 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:19:57.147223 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments.yml
[0m14:19:57.147393 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:19:57.147534 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:19:57.147663 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:19:57.394372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105cc4cb0>]}
[0m14:19:57.441615 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:19:57.443840 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:19:57.460470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b4acf0>]}
[0m14:19:57.460780 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:19:57.460960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '282d0e2e-cd3e-44f0-9b96-91259616d698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595c440>]}
[0m14:19:57.461363 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:19:57.462056 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:19:57.462318 [debug] [MainThread]: Command end result
[0m14:19:57.483119 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:19:57.484265 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:19:57.486600 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:19:57.488050 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.6735688, "process_in_blocks": "0", "process_kernel_time": 0.156827, "process_mem_max_rss": "139689984", "process_out_blocks": "0", "process_user_time": 1.169049}
[0m14:19:57.488302 [debug] [MainThread]: Command `dbt run` succeeded at 14:19:57.488255 after 0.67 seconds
[0m14:19:57.488489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105406fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104610140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105740050>]}
[0m14:19:57.488666 [debug] [MainThread]: Flushing usage events
[0m14:19:57.611441 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:20:15.294776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bf9b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc9760>]}


============================== 14:20:15.298703 | 4afa1aa9-613a-4d5f-a699-f3a280e364e1 ==============================
[0m14:20:15.298703 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:20:15.299068 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'printer_width': '80', 'version_check': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt test --select experiments', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'no_print': 'None', 'indirect_selection': 'eager'}
[0m14:20:15.461019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ea8350>]}
[0m14:20:15.492113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bfa4b0>]}
[0m14:20:15.494439 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:20:15.551756 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:20:15.617242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:15.617487 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:20:15.617621 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:15.639731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10908f9b0>]}
[0m14:20:15.682207 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:15.683552 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:15.701349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109533410>]}
[0m14:20:15.701631 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:20:15.701812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4afa1aa9-613a-4d5f-a699-f3a280e364e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e31a60>]}
[0m14:20:15.702219 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:20:15.702894 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:20:15.703791 [debug] [MainThread]: Command end result
[0m14:20:15.723113 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:15.724128 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:15.725780 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:20:15.727214 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.46840212, "process_in_blocks": "0", "process_kernel_time": 0.149678, "process_mem_max_rss": "134905856", "process_out_blocks": "0", "process_user_time": 0.965844}
[0m14:20:15.727464 [debug] [MainThread]: Command `dbt test` succeeded at 14:20:15.727418 after 0.47 seconds
[0m14:20:15.727652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108dc9a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1095332f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109068800>]}
[0m14:20:15.727816 [debug] [MainThread]: Flushing usage events
[0m14:20:15.829038 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:20:54.359005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e92d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b59d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b5760>]}


============================== 14:20:54.370046 | 4a73ed28-a999-40d6-9d22-dcaaddd42f3a ==============================
[0m14:20:54.370046 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:20:54.370433 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'no_print': 'None', 'use_colors': 'True', 'quiet': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'invocation_command': 'dbt run --select experiments', 'introspect': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m14:20:54.532257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d57a40>]}
[0m14:20:54.560498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105863260>]}
[0m14:20:54.561836 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:20:54.614418 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:20:54.683152 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:54.683398 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:20:54.683539 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:54.705652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10721cb00>]}
[0m14:20:54.751620 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:54.753381 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:54.768013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107679b50>]}
[0m14:20:54.768303 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:20:54.768474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a73ed28-a999-40d6-9d22-dcaaddd42f3a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078f4f20>]}
[0m14:20:54.768895 [warn ] [MainThread]: The selection criterion 'experiments' does not match any enabled nodes
[0m14:20:54.769587 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:20:54.770480 [debug] [MainThread]: Command end result
[0m14:20:54.789199 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:20:54.790212 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:20:54.791928 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:20:54.793406 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.47304237, "process_in_blocks": "0", "process_kernel_time": 0.180919, "process_mem_max_rss": "134479872", "process_out_blocks": "0", "process_user_time": 0.961316}
[0m14:20:54.793669 [debug] [MainThread]: Command `dbt run` succeeded at 14:20:54.793622 after 0.47 seconds
[0m14:20:54.793855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105856c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f46cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107149d00>]}
[0m14:20:54.794023 [debug] [MainThread]: Flushing usage events
[0m14:20:54.898285 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:23:03.578912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10722fd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187b5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187b5700>]}


============================== 14:23:03.590383 | 13e0e240-facd-40a0-be1a-2e9b48ea2347 ==============================
[0m14:23:03.590383 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:23:03.590716 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'fail_fast': 'False', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'version_check': 'True', 'printer_width': '80', 'target_path': 'None', 'no_print': 'None', 'invocation_command': 'dbt run --select marts.experiment', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'use_experimental_parser': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'use_colors': 'True'}
[0m14:23:03.755437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cfba70>]}
[0m14:23:03.783930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11878b830>]}
[0m14:23:03.785272 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:23:03.837432 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:23:03.907829 [debug] [MainThread]: Partial parsing enabled: 4 files deleted, 4 files added, 0 files changed.
[0m14:23:03.908189 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/fct_experiment_conversions.sql
[0m14:23:03.908422 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/experiments.yml
[0m14:23:03.908575 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:23:03.908708 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:23:03.908929 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/fct_experiments_assignments.sql
[0m14:23:03.909061 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/rpt_experiment_results.sql
[0m14:23:03.909184 [debug] [MainThread]: Partial parsing: deleted file: saas_analytics://models/marts/fct_experiment_conversions.sql
[0m14:23:04.154675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194ae2a0>]}
[0m14:23:04.199080 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:23:04.201260 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:23:04.216277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1194c6540>]}
[0m14:23:04.216568 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:23:04.216747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13e0e240-facd-40a0-be1a-2e9b48ea2347', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11935ec00>]}
[0m14:23:04.217158 [warn ] [MainThread]: The selection criterion 'marts.experiment' does not match any enabled nodes
[0m14:23:04.217841 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:23:04.218095 [debug] [MainThread]: Command end result
[0m14:23:04.268178 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:23:04.272217 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:23:04.274383 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:23:04.275559 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7338924, "process_in_blocks": "0", "process_kernel_time": 0.181641, "process_mem_max_rss": "140509184", "process_out_blocks": "0", "process_user_time": 1.165126}
[0m14:23:04.275804 [debug] [MainThread]: Command `dbt run` succeeded at 14:23:04.275760 after 0.73 seconds
[0m14:23:04.275996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e59580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e5abd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106853230>]}
[0m14:23:04.276162 [debug] [MainThread]: Flushing usage events
[0m14:23:04.395226 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:48.067198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108881ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2b5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2b55e0>]}


============================== 14:24:48.079447 | d8bb8961-327c-4fa2-b7dc-e01ea31b6005 ==============================
[0m14:24:48.079447 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:24:48.079815 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'write_json': 'True', 'partial_parse': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'empty': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'quiet': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'introspect': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'printer_width': '80'}
[0m14:24:48.248531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a391820>]}
[0m14:24:48.277299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd95b0>]}
[0m14:24:48.278669 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:24:48.330944 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:24:48.400911 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:24:48.401186 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:24:48.401327 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:24:48.424013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a31c740>]}
[0m14:24:48.472583 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:24:48.474405 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:24:48.489375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b512990>]}
[0m14:24:48.489700 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:24:48.489881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b57deb0>]}
[0m14:24:48.491641 [info ] [MainThread]: 
[0m14:24:48.491831 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:24:48.491966 [info ] [MainThread]: 
[0m14:24:48.492214 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:24:48.494523 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:24:48.537615 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:24:48.537875 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:24:48.538034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:48.556796 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:24:48.557628 [debug] [ThreadPool]: On list_analytics: Close
[0m14:24:48.557990 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:24:48.558251 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:24:48.561715 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.561903 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:24:48.562058 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:48.562833 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:24:48.563500 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.563641 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:24:48.563867 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.563993 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.564114 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:24:48.564351 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.564730 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:24:48.564865 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:24:48.564983 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:24:48.565206 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.565336 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:24:48.566618 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:24:48.569727 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:24:48.569917 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:24:48.570049 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:48.570411 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:24:48.570544 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:24:48.570677 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:24:48.580800 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m14:24:48.581681 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:24:48.582719 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:24:48.582873 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:24:48.583759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a392630>]}
[0m14:24:48.584046 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.584200 [debug] [MainThread]: On master: BEGIN
[0m14:24:48.584334 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:48.584657 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.584791 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.584918 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.585036 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.585236 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.585361 [debug] [MainThread]: On master: Close
[0m14:24:48.587045 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.587323 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:24:48.587550 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:24:48.587714 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.592435 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.593156 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.608863 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.609459 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.609637 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:24:48.609793 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:48.610169 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:24:48.610319 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:24:48.610498 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partiion by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:24:48.616441 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partiion by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:24:48.616760 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:24:48.617017 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: ROLLBACK
[0m14:24:48.621406 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_experiments_assignments'
[0m14:24:48.621609 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:24:48.622825 [debug] [Thread-1 (]: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^
[0m14:24:48.623868 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8bb8961-327c-4fa2-b7dc-e01ea31b6005', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b01e030>]}
[0m14:24:48.624203 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_experiments_assignments ......... [[31mERROR[0m in 0.04s]
[0m14:24:48.624485 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:24:48.624738 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiments_assignments' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^.
[0m14:24:48.625399 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:24:48.625661 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.fct_experiment_conversions ........................... [[33mSKIP[0m]
[0m14:24:48.625896 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:24:48.626154 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:24:48.626361 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:24:48.626567 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:24:48.627077 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.627227 [debug] [MainThread]: On master: BEGIN
[0m14:24:48.627351 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:24:48.627900 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.628056 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.628187 [debug] [MainThread]: Using duckdb connection "master"
[0m14:24:48.628307 [debug] [MainThread]: On master: COMMIT
[0m14:24:48.628529 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:24:48.628654 [debug] [MainThread]: On master: Close
[0m14:24:48.628829 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:48.628951 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:24:48.629110 [info ] [MainThread]: 
[0m14:24:48.629263 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m14:24:48.629708 [debug] [MainThread]: Command end result
[0m14:24:48.682815 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:24:48.683935 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:24:48.687308 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:24:48.687523 [info ] [MainThread]: 
[0m14:24:48.687704 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:24:48.687856 [info ] [MainThread]: 
[0m14:24:48.688038 [error] [MainThread]: [31mFailure in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)[0m
[0m14:24:48.688212 [error] [MainThread]:   Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Parser Error: syntax error at or near "by"
  
  LINE 40:         row_number() over (partiion by user_id order by assigned_at) as rn 
                                               ^
[0m14:24:48.688344 [info ] [MainThread]: 
[0m14:24:48.688500 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiments_assignments.sql
[0m14:24:48.688629 [info ] [MainThread]: 
[0m14:24:48.688771 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:24:48.690199 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.66042906, "process_in_blocks": "0", "process_kernel_time": 0.195312, "process_mem_max_rss": "146915328", "process_out_blocks": "0", "process_user_time": 1.077618}
[0m14:24:48.690422 [debug] [MainThread]: Command `dbt run` failed at 14:24:48.690382 after 0.66 seconds
[0m14:24:48.690604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107330d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd95b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10820ec90>]}
[0m14:24:48.690773 [debug] [MainThread]: Flushing usage events
[0m14:24:48.823731 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:50.599010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5e5d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5730>]}


============================== 14:25:50.610824 | f69783b7-731d-47a0-a0ab-3e3860ea9ff3 ==============================
[0m14:25:50.610824 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:25:50.611233 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/experiments', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'write_json': 'True', 'log_format': 'default', 'target_path': 'None', 'log_cache_events': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'printer_width': '80', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'no_print': 'None', 'warn_error': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'introspect': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:25:50.785792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca34200>]}
[0m14:25:50.814287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d1bb90>]}
[0m14:25:50.815568 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:25:50.867381 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:25:50.936899 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:50.937325 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:25:51.137716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6fb920>]}
[0m14:25:51.214739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:25:51.216501 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:25:51.231020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d323c50>]}
[0m14:25:51.231309 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:25:51.231481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d163740>]}
[0m14:25:51.233046 [info ] [MainThread]: 
[0m14:25:51.233223 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:25:51.233359 [info ] [MainThread]: 
[0m14:25:51.233584 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:25:51.235722 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:25:51.264760 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:25:51.264997 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:25:51.265336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:51.284407 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:25:51.285377 [debug] [ThreadPool]: On list_analytics: Close
[0m14:25:51.285722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:25:51.285962 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:25:51.289204 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.289499 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:25:51.289686 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:51.290406 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:25:51.291194 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.291329 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:25:51.291545 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.291670 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.291792 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:25:51.292030 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.292392 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:25:51.292517 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:25:51.292627 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:25:51.292813 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.292938 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:25:51.293622 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:25:51.296354 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:25:51.296509 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:25:51.296687 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:51.296942 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:25:51.297071 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:25:51.297201 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:25:51.307919 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:25:51.308648 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:25:51.309588 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:25:51.309733 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:25:51.310643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da9de20>]}
[0m14:25:51.310870 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.310998 [debug] [MainThread]: On master: BEGIN
[0m14:25:51.311111 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:51.311423 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.311591 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.311729 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.311848 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.312053 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.312180 [debug] [MainThread]: On master: Close
[0m14:25:51.313620 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.313882 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:25:51.314098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:25:51.314265 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.318172 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.319533 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.335191 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.336690 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.336874 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:25:51.337028 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:25:51.337393 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:25:51.337547 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:25:51.337733 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:25:51.342155 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquistion_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:25:51.342399 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:25:51.342621 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: ROLLBACK
[0m14:25:51.348320 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_experiments_assignments'
[0m14:25:51.348521 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:25:51.349702 [debug] [Thread-1 (]: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^
[0m14:25:51.350634 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f69783b7-731d-47a0-a0ab-3e3860ea9ff3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db2adb0>]}
[0m14:25:51.350948 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_experiments_assignments ......... [[31mERROR[0m in 0.04s]
[0m14:25:51.351219 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:25:51.351456 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiments_assignments' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^.
[0m14:25:51.352084 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:25:51.352346 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.fct_experiment_conversions ........................... [[33mSKIP[0m]
[0m14:25:51.352582 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:25:51.352844 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:25:51.353048 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:25:51.353255 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:25:51.353753 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.353901 [debug] [MainThread]: On master: BEGIN
[0m14:25:51.354027 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:25:51.354346 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.354479 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.354604 [debug] [MainThread]: Using duckdb connection "master"
[0m14:25:51.354731 [debug] [MainThread]: On master: COMMIT
[0m14:25:51.354919 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:25:51.355044 [debug] [MainThread]: On master: Close
[0m14:25:51.355202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:51.355319 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:25:51.355470 [info ] [MainThread]: 
[0m14:25:51.355604 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m14:25:51.355918 [debug] [MainThread]: Command end result
[0m14:25:51.372573 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:25:51.373565 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:25:51.376247 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:25:51.376411 [info ] [MainThread]: 
[0m14:25:51.376578 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:25:51.376715 [info ] [MainThread]: 
[0m14:25:51.376897 [error] [MainThread]: [31mFailure in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)[0m
[0m14:25:51.377068 [error] [MainThread]:   Runtime Error in model fct_experiments_assignments (models/marts/experiments/fct_experiments_assignments.sql)
  Binder Error: Values list "u" does not have a column named "acquistion_channel"
  
  LINE 52:         u.acquistion_channel,
                   ^
[0m14:25:51.377260 [info ] [MainThread]: 
[0m14:25:51.377412 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiments_assignments.sql
[0m14:25:51.377542 [info ] [MainThread]: 
[0m14:25:51.377913 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:25:51.379436 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.82057434, "process_in_blocks": "0", "process_kernel_time": 0.194725, "process_mem_max_rss": "149553152", "process_out_blocks": "0", "process_user_time": 1.254209}
[0m14:25:51.379687 [debug] [MainThread]: Command `dbt run` failed at 14:25:51.379642 after 0.82 seconds
[0m14:25:51.379882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b57f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8b5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce7a900>]}
[0m14:25:51.380062 [debug] [MainThread]: Flushing usage events
[0m14:25:51.481053 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:18.861900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f2c950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb56d0>]}


============================== 14:26:18.868149 | 7b6b96ee-c6c1-49fe-a8e2-2759c38abf54 ==============================
[0m14:26:18.868149 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:26:18.868571 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'warn_error': 'None', 'empty': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'introspect': 'True', 'target_path': 'None', 'no_print': 'None', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'version_check': 'True', 'printer_width': '80', 'debug': 'False', 'cache_selected_only': 'False'}
[0m14:26:19.028888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f543b0>]}
[0m14:26:19.057090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c77e60>]}
[0m14:26:19.058576 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:26:19.111510 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:26:19.177987 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:19.178451 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:26:19.374159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afb2a50>]}
[0m14:26:19.448456 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:19.449816 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:19.462287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aec79b0>]}
[0m14:26:19.462573 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:26:19.462749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10affb6e0>]}
[0m14:26:19.464385 [info ] [MainThread]: 
[0m14:26:19.464565 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:26:19.464760 [info ] [MainThread]: 
[0m14:26:19.465070 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:26:19.467258 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:26:19.496643 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:26:19.496892 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:26:19.497039 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:19.515049 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:26:19.516036 [debug] [ThreadPool]: On list_analytics: Close
[0m14:26:19.516389 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:26:19.516635 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:26:19.519913 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.520085 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:26:19.520224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:19.521107 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:26:19.521912 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.522071 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:26:19.522315 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.522453 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.522583 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:26:19.523097 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.523530 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:19.523669 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:19.523793 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:19.524003 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.524132 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:26:19.524821 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:26:19.527475 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:19.527635 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:26:19.527820 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:19.528069 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:19.528199 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:19.528388 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:26:19.545994 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:26:19.546759 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:26:19.547738 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:26:19.547897 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:26:19.548607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af45190>]}
[0m14:26:19.548879 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.549076 [debug] [MainThread]: On master: BEGIN
[0m14:26:19.549207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:19.549541 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.549679 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.550029 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.550318 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.550529 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.550660 [debug] [MainThread]: On master: Close
[0m14:26:19.555066 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.555331 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:26:19.555596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:26:19.555809 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.559939 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.560674 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.576119 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.576672 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.576846 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:26:19.577000 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:19.577364 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.577513 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.577690 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:26:19.604837 [debug] [Thread-1 (]: SQL status: OK in 0.027 seconds
[0m14:26:19.608146 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.608339 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:26:19.609219 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:19.616132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:19.616370 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.616534 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:19.618571 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:26:19.621980 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:19.622168 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:26:19.622731 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.623875 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:26:19.624780 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a2ecf0>]}
[0m14:26:19.625084 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:26:19.625347 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:26:19.625672 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.625920 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:26:19.626179 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:19.626341 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.627915 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.628307 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.629864 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.630217 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.630385 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:26:19.630535 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:26:19.630851 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:19.631004 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:19.631233 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:19.634700 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:19.635063 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m14:26:19.635398 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: ROLLBACK
[0m14:26:19.639640 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:19.639828 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:26:19.640981 [debug] [Thread-3 (]: Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^
[0m14:26:19.641227 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7b6b96ee-c6c1-49fe-a8e2-2759c38abf54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbaf590>]}
[0m14:26:19.641545 [error] [Thread-3 (]: 2 of 3 ERROR creating sql table model main.fct_experiment_conversions .......... [[31mERROR[0m in 0.02s]
[0m14:26:19.641816 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:26:19.642059 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_experiment_conversions' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^.
[0m14:26:19.642704 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:26:19.642938 [info ] [Thread-2 (]: 3 of 3 SKIP relation main.rpt_experiment_results ............................... [[33mSKIP[0m]
[0m14:26:19.643162 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:26:19.644893 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.645051 [debug] [MainThread]: On master: BEGIN
[0m14:26:19.645187 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:26:19.645563 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.645696 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.645823 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:19.645934 [debug] [MainThread]: On master: COMMIT
[0m14:26:19.646126 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:19.646251 [debug] [MainThread]: On master: Close
[0m14:26:19.646422 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:19.646539 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:26:19.646648 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:26:19.646809 [info ] [MainThread]: 
[0m14:26:19.646967 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m14:26:19.647445 [debug] [MainThread]: Command end result
[0m14:26:19.664739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:19.665789 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:19.668409 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:26:19.668570 [info ] [MainThread]: 
[0m14:26:19.668744 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:26:19.668885 [info ] [MainThread]: 
[0m14:26:19.669062 [error] [MainThread]: [31mFailure in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)[0m
[0m14:26:19.669226 [error] [MainThread]:   Runtime Error in model fct_experiment_conversions (models/marts/experiments/fct_experiment_conversions.sql)
  Parser Error: syntax error at or near "datediff"
  
  LINE 46:         datediff('day', a.assigned_at, c.event_at) as days_to_conve...
                   ^
[0m14:26:19.669352 [info ] [MainThread]: 
[0m14:26:19.669503 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/fct_experiment_conversions.sql
[0m14:26:19.669631 [info ] [MainThread]: 
[0m14:26:19.669772 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 NO-OP=0 TOTAL=3
[0m14:26:19.670786 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.84552443, "process_in_blocks": "0", "process_kernel_time": 0.196724, "process_mem_max_rss": "180895744", "process_out_blocks": "0", "process_user_time": 1.292083}
[0m14:26:19.670981 [debug] [MainThread]: Command `dbt run` failed at 14:26:19.670945 after 0.85 seconds
[0m14:26:19.671151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b4c1a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10988a600>]}
[0m14:26:19.671316 [debug] [MainThread]: Flushing usage events
[0m14:26:19.784737 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:42.785364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bbe540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107432510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f8eb70>]}


============================== 14:26:42.793804 | afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19 ==============================
[0m14:26:42.793804 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:26:42.794141 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'fail_fast': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'quiet': 'False', 'debug': 'False', 'write_json': 'True', 'no_print': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'introspect': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default'}
[0m14:26:42.958946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779e150>]}
[0m14:26:42.987463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10769a4e0>]}
[0m14:26:42.988799 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:26:43.041233 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:26:43.111032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:43.111443 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiment_conversions.sql
[0m14:26:43.311564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f44650>]}
[0m14:26:43.389425 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:43.390722 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:43.404242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f627e0>]}
[0m14:26:43.404512 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:26:43.404688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e269f0>]}
[0m14:26:43.406293 [info ] [MainThread]: 
[0m14:26:43.406480 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:26:43.406622 [info ] [MainThread]: 
[0m14:26:43.406851 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:26:43.408990 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:26:43.439431 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:26:43.439660 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:26:43.439808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:43.457454 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:26:43.458443 [debug] [ThreadPool]: On list_analytics: Close
[0m14:26:43.458798 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:26:43.459049 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:26:43.462608 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.462784 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:26:43.462911 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:43.463741 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:26:43.464386 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.464512 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:26:43.464722 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.464843 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.464963 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:26:43.465203 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.465579 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:43.465711 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:26:43.465823 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:26:43.466014 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.466144 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:26:43.466803 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:26:43.469397 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:43.469553 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:26:43.469728 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:43.469963 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:26:43.470089 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:26:43.470217 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:26:43.480951 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m14:26:43.481697 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:26:43.482651 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:26:43.482794 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:26:43.483535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e6030>]}
[0m14:26:43.483795 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.483944 [debug] [MainThread]: On master: BEGIN
[0m14:26:43.484072 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:43.484337 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.484470 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.484596 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.484715 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.484914 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.485038 [debug] [MainThread]: On master: Close
[0m14:26:43.486404 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.486638 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:26:43.486838 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:26:43.486999 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.490741 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.491526 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.507500 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.508350 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.508547 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:26:43.508913 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:43.509284 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.509439 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.509622 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:26:43.538402 [debug] [Thread-1 (]: SQL status: OK in 0.028 seconds
[0m14:26:43.541255 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.541451 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:26:43.541958 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.542740 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.543038 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:26:43.543451 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.546989 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.547383 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:26:43.548365 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.550614 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.550796 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:26:43.551128 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.558733 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:43.558976 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.559132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:26:43.561378 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:26:43.564208 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:26:43.564410 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:26:43.565085 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.566256 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:26:43.567210 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104caedb0>]}
[0m14:26:43.567518 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.08s]
[0m14:26:43.567792 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:26:43.568128 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.568398 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:26:43.568692 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:26:43.568858 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.570602 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.571053 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.572606 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.573137 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.573306 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:26:43.573452 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:26:43.573758 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.573920 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.574121 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:26:43.584113 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m14:26:43.585834 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.586013 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:26:43.586385 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.587146 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:26:43.587307 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.588165 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:26:43.589743 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:26:43.590998 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:26:43.591174 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:26:43.591492 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.592136 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:26:43.592390 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e6b350>]}
[0m14:26:43.592676 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.02s]
[0m14:26:43.592925 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:26:43.593254 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:26:43.593544 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:26:43.593835 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:26:43.594015 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:26:43.596368 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.596844 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:26:43.598653 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.599071 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.599280 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:26:43.599423 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:26:43.599778 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:26:43.599929 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:26:43.600254 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vs.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:26:43.606047 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vs.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:26:43.606458 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m14:26:43.606691 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: ROLLBACK
[0m14:26:43.610987 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results'
[0m14:26:43.611192 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:26:43.612695 [debug] [Thread-2 (]: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^
[0m14:26:43.613456 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afd2df10-16fe-4cea-9b5a-a9fd8e4e4b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10825e4e0>]}
[0m14:26:43.613814 [error] [Thread-2 (]: 3 of 3 ERROR creating sql table model main.rpt_experiment_results .............. [[31mERROR[0m in 0.02s]
[0m14:26:43.614129 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:26:43.614365 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^.
[0m14:26:43.615301 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.618844 [debug] [MainThread]: On master: BEGIN
[0m14:26:43.619000 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:26:43.619393 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.619566 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.619708 [debug] [MainThread]: Using duckdb connection "master"
[0m14:26:43.619831 [debug] [MainThread]: On master: COMMIT
[0m14:26:43.620037 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:26:43.620167 [debug] [MainThread]: On master: Close
[0m14:26:43.620349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:43.620502 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:26:43.620632 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:26:43.620746 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:26:43.620914 [info ] [MainThread]: 
[0m14:26:43.621062 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m14:26:43.621475 [debug] [MainThread]: Command end result
[0m14:26:43.637822 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:26:43.639005 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:26:43.642114 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:26:43.642267 [info ] [MainThread]: 
[0m14:26:43.642433 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:26:43.642574 [info ] [MainThread]: 
[0m14:26:43.642748 [error] [MainThread]: [31mFailure in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)[0m
[0m14:26:43.642916 [error] [MainThread]:   Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced table "vs" not found!
  Candidate tables: "vt", "vc", "events"
  
  LINE 47:         coalesce(vs.converted_users, 0) as converted_users,
                            ^
[0m14:26:43.643042 [info ] [MainThread]: 
[0m14:26:43.643190 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results.sql
[0m14:26:43.643321 [info ] [MainThread]: 
[0m14:26:43.643461 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:26:43.644831 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.89537007, "process_in_blocks": "0", "process_kernel_time": 0.212696, "process_mem_max_rss": "187138048", "process_out_blocks": "0", "process_user_time": 1.344202}
[0m14:26:43.645043 [debug] [MainThread]: Command `dbt run` failed at 14:26:43.645003 after 0.90 seconds
[0m14:26:43.645265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bbe540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107595e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068ad4f0>]}
[0m14:26:43.645473 [debug] [MainThread]: Flushing usage events
[0m14:26:43.754961 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:07.267017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10758fa40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109249a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092497c0>]}


============================== 14:27:07.272007 | c3c0f5e5-953b-42ea-a6d8-148267016d0c ==============================
[0m14:27:07.272007 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:27:07.272440 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'log_format': 'default', 'empty': 'False', 'write_json': 'True', 'target_path': 'None'}
[0m14:27:07.434465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092d0140>]}
[0m14:27:07.463764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108623740>]}
[0m14:27:07.465998 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:27:07.517376 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:27:07.584785 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:27:07.585221 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:27:07.778225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad8fe0>]}
[0m14:27:07.853520 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:27:07.854919 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:27:07.867944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096ff4d0>]}
[0m14:27:07.868233 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:27:07.868411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a7d430>]}
[0m14:27:07.870006 [info ] [MainThread]: 
[0m14:27:07.870187 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:27:07.870327 [info ] [MainThread]: 
[0m14:27:07.870554 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:27:07.872683 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:27:07.901060 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:27:07.901296 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:27:07.901442 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:07.919291 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m14:27:07.920265 [debug] [ThreadPool]: On list_analytics: Close
[0m14:27:07.920632 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:27:07.920852 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:27:07.924096 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.924267 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:27:07.924402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:27:07.925160 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:27:07.926090 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.926454 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:27:07.926775 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.926932 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.927078 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:27:07.927596 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.928034 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:27:07.928218 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:27:07.928338 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:27:07.928625 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.928953 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:27:07.929732 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:27:07.932504 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:27:07.932684 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:27:07.932922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:27:07.933340 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:27:07.933813 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:27:07.934225 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:27:07.949177 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:27:07.950362 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:27:07.951338 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:27:07.951507 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:27:07.952359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d77b00>]}
[0m14:27:07.952594 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:07.952731 [debug] [MainThread]: On master: BEGIN
[0m14:27:07.952848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:27:07.953133 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:07.953268 [debug] [MainThread]: On master: COMMIT
[0m14:27:07.953399 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:07.953517 [debug] [MainThread]: On master: COMMIT
[0m14:27:07.953721 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:07.953848 [debug] [MainThread]: On master: Close
[0m14:27:07.955166 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.955428 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:27:07.955646 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:27:07.955819 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.959658 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.960267 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:27:07.975776 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.976593 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.976779 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:27:07.976928 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:07.977284 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:07.977429 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.977605 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:27:07.994990 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m14:27:07.997962 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.998177 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:27:07.998690 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:07.999385 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:07.999560 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:27:08.000007 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.003103 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.003301 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:27:08.004163 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.006246 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.006423 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:27:08.006717 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.013882 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:27:08.014097 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.014308 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:27:08.016648 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:27:08.019588 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:27:08.019785 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:27:08.020651 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.021823 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:27:08.022771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068f6ba0>]}
[0m14:27:08.023080 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:27:08.023342 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:27:08.023680 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.023891 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:27:08.024197 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:27:08.024358 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.025974 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.026607 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.028238 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.028784 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.028968 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:27:08.029131 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:27:08.029572 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.029754 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.029967 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:27:08.038977 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m14:27:08.039675 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.039946 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:27:08.040424 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.041019 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.041187 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:27:08.041610 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.043228 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.043408 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:27:08.043718 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.045010 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.045170 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:27:08.045576 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.046416 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:27:08.046583 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.046725 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:27:08.047904 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:27:08.048970 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:27:08.049128 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:27:08.049512 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.050100 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:27:08.050353 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109dd1400>]}
[0m14:27:08.050631 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m14:27:08.050877 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:27:08.051211 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:27:08.051454 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:27:08.051687 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:27:08.051848 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:27:08.053680 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.054112 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:27:08.055630 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.055998 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.056153 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:27:08.056293 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:27:08.056589 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:27:08.056774 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:27:08.057086 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:27:08.062721 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant
variant_totals as (
    select 
        experiment_variant,
        count(distinct user_id) as total_users
    from assignments 
    group by experiment_variant
),

--count conversion per variant per conversion event
variant_conversions as (
    select 
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion
    from conversions 
    group by experiment_variant, conversion_event 
),

--calculate conversion rates
conversion_rates as (
    select
        vt.experiment_variant,
        vc.conversion_event,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion
    from variant_totals vt 
    cross join (select distinct conversion_event from variant_conversions) events 
    left join variant_conversions vc 
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event
),

--pivot to get control vs treatment side-by-side for each metric
results_pivoted as (
    select
        conversion_event,

        --variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        --variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion
    from conversion_rates
    group by conversion_event 
),

--calculate statistical metrics
final as (
    select 
        conversion_event,
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        --lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --standard error for each variant (for confidence interval)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        --pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        --z-score (test statistic)
        case 
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null 
        end as z_score,

        --95% confidence interval for B-A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) /  b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance (at 95% confidernce level, z > 1.96)
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --which variant won?
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted

)

select * from final
    );
  
  
[0m14:27:08.063111 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m14:27:08.063367 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: ROLLBACK
[0m14:27:08.072189 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results'
[0m14:27:08.072431 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:27:08.073662 [debug] [Thread-2 (]: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^
[0m14:27:08.073916 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3c0f5e5-953b-42ea-a6d8-148267016d0c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f94170>]}
[0m14:27:08.074219 [error] [Thread-2 (]: 3 of 3 ERROR creating sql table model main.rpt_experiment_results .............. [[31mERROR[0m in 0.02s]
[0m14:27:08.074485 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:27:08.074747 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^.
[0m14:27:08.075709 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:08.075878 [debug] [MainThread]: On master: BEGIN
[0m14:27:08.076007 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:27:08.076324 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:08.076465 [debug] [MainThread]: On master: COMMIT
[0m14:27:08.076714 [debug] [MainThread]: Using duckdb connection "master"
[0m14:27:08.076837 [debug] [MainThread]: On master: COMMIT
[0m14:27:08.077056 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:27:08.077401 [debug] [MainThread]: On master: Close
[0m14:27:08.077621 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:08.077757 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:27:08.077966 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:27:08.078083 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:27:08.078300 [info ] [MainThread]: 
[0m14:27:08.078774 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m14:27:08.079468 [debug] [MainThread]: Command end result
[0m14:27:08.101205 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:27:08.102313 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:27:08.105167 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:27:08.105330 [info ] [MainThread]: 
[0m14:27:08.105504 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:27:08.105645 [info ] [MainThread]: 
[0m14:27:08.105829 [error] [MainThread]: [31mFailure in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)[0m
[0m14:27:08.106275 [error] [MainThread]:   Runtime Error in model rpt_experiment_results (models/marts/experiments/rpt_experiment_results.sql)
  Binder Error: Referenced column "total_users" not found in FROM clause!
  Candidate bindings: "converted_users", "avg_days_to_conversion", "median_days_to_conversion"
  
  LINE 64:         max(case when experiment_variant = 'A' then total_users end) as a_total_users,
                                                               ^
[0m14:27:08.106455 [info ] [MainThread]: 
[0m14:27:08.106642 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results.sql
[0m14:27:08.106791 [info ] [MainThread]: 
[0m14:27:08.106942 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:27:08.108416 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8814913, "process_in_blocks": "0", "process_kernel_time": 0.204851, "process_mem_max_rss": "186695680", "process_out_blocks": "0", "process_user_time": 1.333466}
[0m14:27:08.108670 [debug] [MainThread]: Command `dbt run` failed at 14:27:08.108623 after 0.88 seconds
[0m14:27:08.108934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092497f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084ec620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084ed550>]}
[0m14:27:08.109118 [debug] [MainThread]: Flushing usage events
[0m14:27:08.219622 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:53.564543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061f79b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d670>]}


============================== 14:28:53.576937 | 99b2c78e-c422-4455-b4a5-298216317fe2 ==============================
[0m14:28:53.576937 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:28:53.577270 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'version_check': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'use_colors': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'log_format': 'default', 'no_print': 'None', 'static_parser': 'True', 'indirect_selection': 'eager'}
[0m14:28:53.752306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99b2c78e-c422-4455-b4a5-298216317fe2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ef41a0>]}
[0m14:28:53.780992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99b2c78e-c422-4455-b4a5-298216317fe2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065a0b60>]}
[0m14:28:53.782376 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:28:53.834388 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:28:53.903663 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:28:53.904078 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:28:54.059605 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.saas_analytics.rpt_experiment_results' (models/marts/experiments/rpt_experiment_results.sql) depends on a node named 'fct_experiment_assignments' which was not found
[0m14:28:54.061361 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5351511, "process_in_blocks": "0", "process_kernel_time": 0.168743, "process_mem_max_rss": "137887744", "process_out_blocks": "0", "process_user_time": 1.016422}
[0m14:28:54.061626 [debug] [MainThread]: Command `dbt run` failed at 14:28:54.061575 after 0.54 seconds
[0m14:28:54.061829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c7d6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a1d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107419f40>]}
[0m14:28:54.062007 [debug] [MainThread]: Flushing usage events
[0m14:28:54.191517 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:15.802034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104813f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed17f0>]}


============================== 14:29:15.806436 | e8805335-8dbb-41c4-9050-6a5026f22abd ==============================
[0m14:29:15.806436 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:29:15.806800 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'target_path': 'None', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'quiet': 'False', 'debug': 'False', 'no_print': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'version_check': 'True', 'empty': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'warn_error': 'None', 'log_cache_events': 'False'}
[0m14:29:15.975006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f3bef0>]}
[0m14:29:16.003438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049abe60>]}
[0m14:29:16.004961 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:29:16.057170 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:29:16.124834 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:29:16.125255 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results.sql
[0m14:29:16.318272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10580d640>]}
[0m14:29:16.394353 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:16.395945 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:16.409725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10552ab10>]}
[0m14:29:16.410010 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:29:16.410180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c1880>]}
[0m14:29:16.411738 [info ] [MainThread]: 
[0m14:29:16.411924 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:29:16.412062 [info ] [MainThread]: 
[0m14:29:16.412290 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:29:16.414481 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:29:16.445496 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:29:16.445769 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:29:16.445925 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:16.466230 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m14:29:16.467267 [debug] [ThreadPool]: On list_analytics: Close
[0m14:29:16.467831 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:29:16.468108 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:29:16.471874 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.472112 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:29:16.472252 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:29:16.473082 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:29:16.473945 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.474095 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:29:16.474328 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.474454 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.474576 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:29:16.474820 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.475240 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:29:16.475376 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:29:16.475491 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:29:16.475691 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.475820 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:29:16.476546 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:29:16.479711 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:29:16.479924 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:29:16.480139 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:29:16.480446 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:29:16.480577 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:29:16.480710 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:29:16.497026 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:29:16.498003 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:29:16.499016 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:29:16.499169 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:29:16.500233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059ffb00>]}
[0m14:29:16.500474 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.500609 [debug] [MainThread]: On master: BEGIN
[0m14:29:16.500731 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:29:16.501054 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.501186 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.501309 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.501427 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.501626 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.501750 [debug] [MainThread]: On master: Close
[0m14:29:16.503260 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.503513 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:29:16.503725 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:29:16.503891 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.508859 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.509681 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.525170 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.526422 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.526614 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:29:16.526774 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:29:16.527197 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.527349 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.527528 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:29:16.547629 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m14:29:16.550417 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.550618 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:29:16.551114 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.551803 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.551970 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:29:16.552401 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.555921 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.556153 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:29:16.557239 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.559524 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.559704 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:29:16.560012 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.567298 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:29:16.567518 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.567674 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:29:16.569882 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:29:16.572625 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:29:16.572810 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:29:16.573604 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.574721 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:29:16.575653 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10414ad20>]}
[0m14:29:16.575951 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:29:16.576209 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:29:16.576575 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.576819 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:29:16.577074 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:29:16.577239 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.578818 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.579459 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.581033 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.581564 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.581731 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:29:16.581879 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:29:16.582311 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.582566 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.582773 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:29:16.592480 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m14:29:16.593164 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.593429 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:29:16.593901 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.594487 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.594646 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:29:16.595064 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.596722 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.596899 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:29:16.597213 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.598526 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.598679 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:29:16.598943 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.599689 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:29:16.599842 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.599980 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:29:16.601509 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:29:16.602603 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:29:16.602760 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:29:16.603107 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.603701 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:29:16.603970 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a67710>]}
[0m14:29:16.604247 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m14:29:16.604487 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:29:16.604801 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:29:16.605081 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:29:16.605366 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:29:16.605536 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:29:16.607405 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.608062 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:29:16.609569 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.610265 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.610419 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:29:16.610562 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:29:16.610861 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.611007 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.611292 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m14:29:16.625631 [debug] [Thread-2 (]: SQL status: OK in 0.014 seconds
[0m14:29:16.628092 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.628270 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m14:29:16.628627 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.629312 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:29:16.629464 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.629598 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:29:16.630108 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.631260 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:29:16.631428 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m14:29:16.631702 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:29:16.632308 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:29:16.632558 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8805335-8dbb-41c4-9050-6a5026f22abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b9b4d0>]}
[0m14:29:16.632824 [info ] [Thread-2 (]: 3 of 3 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.03s]
[0m14:29:16.633061 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:29:16.633618 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.633797 [debug] [MainThread]: On master: BEGIN
[0m14:29:16.633930 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:29:16.634179 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.634311 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.634434 [debug] [MainThread]: Using duckdb connection "master"
[0m14:29:16.634547 [debug] [MainThread]: On master: COMMIT
[0m14:29:16.634731 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:29:16.634852 [debug] [MainThread]: On master: Close
[0m14:29:16.635011 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:29:16.635123 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:29:16.635224 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:29:16.635322 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:29:16.635466 [info ] [MainThread]: 
[0m14:29:16.635604 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:29:16.635984 [debug] [MainThread]: Command end result
[0m14:29:16.651655 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:16.652705 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:16.655592 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:29:16.655756 [info ] [MainThread]: 
[0m14:29:16.655932 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:16.656065 [info ] [MainThread]: 
[0m14:29:16.656214 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:29:16.657618 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.89273214, "process_in_blocks": "0", "process_kernel_time": 0.207586, "process_mem_max_rss": "193691648", "process_out_blocks": "0", "process_user_time": 1.340497}
[0m14:29:16.657831 [debug] [MainThread]: Command `dbt run` succeeded at 14:29:16.657793 after 0.89 seconds
[0m14:29:16.658000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ed1940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104813f20>]}
[0m14:29:16.658164 [debug] [MainThread]: Flushing usage events
[0m14:29:16.763014 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:41.560270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044165d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db57c0>]}


============================== 14:29:41.570257 | adfbecbe-671f-486d-9c72-f90746c9618b ==============================
[0m14:29:41.570257 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:29:41.570658 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select path:models/marts/experiements', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'empty': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'cache_selected_only': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80'}
[0m14:29:41.776213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f1dc10>]}
[0m14:29:41.804099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107999e20>]}
[0m14:29:41.805376 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:29:41.856505 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:29:41.924356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:29:41.924597 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:29:41.924735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:29:41.946864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3bd40>]}
[0m14:29:41.990888 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:41.992443 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:42.010332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114967a40>]}
[0m14:29:42.010616 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:29:42.010783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adfbecbe-671f-486d-9c72-f90746c9618b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123feff0>]}
[0m14:29:42.011860 [warn ] [MainThread]: The selection criterion 'path:models/marts/experiements' does not match any enabled nodes
[0m14:29:42.012567 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m14:29:42.013511 [debug] [MainThread]: Command end result
[0m14:29:42.034607 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:29:42.035635 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:29:42.037177 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:29:42.038612 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.5171716, "process_in_blocks": "0", "process_kernel_time": 0.173199, "process_mem_max_rss": "136593408", "process_out_blocks": "0", "process_user_time": 0.962304}
[0m14:29:42.038894 [debug] [MainThread]: Command `dbt test` succeeded at 14:29:42.038844 after 0.52 seconds
[0m14:29:42.039096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107db50a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110640da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fe4050>]}
[0m14:29:42.039267 [debug] [MainThread]: Flushing usage events
[0m14:29:42.144577 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:30:13.223560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102bd6f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057118b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105711670>]}


============================== 14:30:13.228381 | 2268c34c-d5d5-4008-a35d-03c85f0838b1 ==============================
[0m14:30:13.228381 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:30:13.228713 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt test --select path:models/marts/experiments', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'debug': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'target_path': 'None', 'cache_selected_only': 'False', 'write_json': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'empty': 'None', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'warn_error': 'None'}
[0m14:30:13.396998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bdd520>]}
[0m14:30:13.427317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e764b0>]}
[0m14:30:13.428987 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:30:13.491648 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:30:13.561231 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:30:13.561474 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:30:13.561614 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:30:13.583971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057787d0>]}
[0m14:30:13.626980 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:30:13.628300 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:30:13.647592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105df7da0>]}
[0m14:30:13.647896 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:30:13.648078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10599cd10>]}
[0m14:30:13.649865 [info ] [MainThread]: 
[0m14:30:13.650048 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:30:13.650192 [info ] [MainThread]: 
[0m14:30:13.650439 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:30:13.652733 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:30:13.682773 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:30:13.683004 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:30:13.683144 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:13.696224 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:30:13.696443 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:30:13.696593 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:30:13.713176 [debug] [ThreadPool]: SQL status: OK in 0.016 seconds
[0m14:30:13.714394 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:30:13.715472 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:30:13.715622 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:30:13.716712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2268c34c-d5d5-4008-a35d-03c85f0838b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059b0560>]}
[0m14:30:13.716943 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.717080 [debug] [MainThread]: On master: BEGIN
[0m14:30:13.717201 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:30:13.717465 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.717593 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.717717 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.717837 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.718036 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.718162 [debug] [MainThread]: On master: Close
[0m14:30:13.719799 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.720000 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.720348 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.720206 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:30:13.720557 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.720725 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:30:13.720908 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:30:13.721151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:30:13.721323 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:30:13.721544 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:30:13.721757 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:30:13.721929 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.722119 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:30:13.722271 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.722417 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.731985 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.732222 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.734711 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.738415 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.740258 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.740814 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.741030 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.741209 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.741379 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.749519 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.750711 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.751803 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.753502 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.754077 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.754288 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.754481 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.754642 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:30:13.754812 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.754979 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:30:13.755141 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:30:13.755292 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:30:13.755441 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:30:13.755582 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:30:13.755728 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.755977 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:30:13.756203 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756457 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756671 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.756845 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:30:13.757004 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.757149 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:30:13.757302 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:30:13.757457 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.757613 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:30:13.757771 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.757951 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:30:13.758203 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:30:13.759429 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.759608 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.761841 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:30:13.762573 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:30:13.763052 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:30:13.763220 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:30:13.763398 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m14:30:13.763808 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:30:13.763963 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m14:30:13.764439 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:30:13.764289 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.04s]
[0m14:30:13.797207 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:30:13.797642 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:30:13.798320 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:30:13.798647 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.08s]
[0m14:30:13.799112 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:30:13.799338 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.799778 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:30:13.800011 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:30:13.800199 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:30:13.800365 [info ] [Thread-4 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:30:13.800540 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:30:13.800703 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.800966 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.08s]
[0m14:30:13.801182 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:30:13.801424 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.08s]
[0m14:30:13.801603 [info ] [Thread-3 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:30:13.801830 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:30:13.801983 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.802189 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:30:13.802353 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:30:13.802525 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.804587 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.804843 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.805011 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.805191 [info ] [Thread-2 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:30:13.805448 [info ] [Thread-1 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:30:13.807333 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.807539 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:30:13.807723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:30:13.807986 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.808142 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.808333 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.810146 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.811321 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.813305 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.813495 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.814974 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.815443 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.815635 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.815840 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:30:13.816104 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.817353 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.817584 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.817745 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:30:13.818811 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.819023 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:30:13.819484 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.819645 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:30:13.819907 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:30:13.820106 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:30:13.820333 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.820499 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:30:13.820662 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.820809 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:30:13.820971 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:30:13.821175 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:30:13.821355 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.821519 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.821656 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.821823 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.822070 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:30:13.822394 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.822584 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.822792 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:30:13.822950 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:30:13.823116 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.823338 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.823513 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.824281 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:30:13.824860 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:30:13.825451 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:30:13.826139 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:30:13.826330 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.826781 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:30:13.826954 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:30:13.827373 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:30:13.828030 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:30:13.828192 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:30:13.828380 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:30:13.828671 [info ] [Thread-4 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.03s]
[0m14:30:13.829165 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:30:13.829409 [info ] [Thread-3 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:30:13.829925 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:30:13.829699 [error] [Thread-2 (]: 7 of 12 FAIL 3300 not_null_fct_experiments_assignments_assigned_at ............. [[31mFAIL 3300[0m in 0.02s]
[0m14:30:13.830143 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:30:13.830375 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:30:13.830567 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.830806 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:30:13.831216 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.831061 [info ] [Thread-1 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:30:13.831447 [info ] [Thread-4 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:30:13.831654 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.831844 [info ] [Thread-3 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:30:13.832103 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:30:13.832294 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:30:13.832468 [info ] [Thread-2 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:30:13.832655 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:30:13.832836 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.833010 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.833195 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:30:13.833373 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.833553 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:30:13.836009 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.836284 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.839605 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.839878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:30:13.843052 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.843316 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.845270 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.845524 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.845708 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.845878 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.847046 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.848318 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.849972 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.850345 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.851802 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.852166 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.852393 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.852588 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.852767 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:30:13.852943 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.853095 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:30:13.853249 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:30:13.853403 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:30:13.853552 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:30:13.853694 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:30:13.853836 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:30:13.854088 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:13.854307 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.854574 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.854735 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:30:13.854886 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.855031 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.855175 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:30:13.855333 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.855507 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:30:13.855676 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:30:13.855861 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:30:13.856175 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:30:13.856378 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:30:13.856629 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.857193 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:30:13.858141 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:30:13.858900 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:30:13.859989 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:30:13.860187 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m14:30:13.862308 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:30:13.862756 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:30:13.863599 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:30:13.863802 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:30:13.864439 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:30:13.864603 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:30:13.865049 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:30:13.865785 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:30:13.865355 [info ] [Thread-3 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:30:13.866017 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:30:13.866403 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:30:13.866259 [info ] [Thread-4 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:30:13.866681 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:30:13.866925 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:30:13.867384 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:30:13.867141 [info ] [Thread-2 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:30:13.867714 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:30:13.867961 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:30:13.868592 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.868732 [debug] [MainThread]: On master: BEGIN
[0m14:30:13.868847 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:30:13.869144 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.869265 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.869381 [debug] [MainThread]: Using duckdb connection "master"
[0m14:30:13.869493 [debug] [MainThread]: On master: COMMIT
[0m14:30:13.869686 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:30:13.869803 [debug] [MainThread]: On master: Close
[0m14:30:13.869951 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:30:13.870069 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:30:13.870178 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:30:13.870282 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:30:13.870386 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:30:13.870517 [info ] [MainThread]: 
[0m14:30:13.870650 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:30:13.871509 [debug] [MainThread]: Command end result
[0m14:30:13.893915 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:30:13.895103 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:30:13.898768 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:30:13.898958 [info ] [MainThread]: 
[0m14:30:13.899128 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:30:13.899263 [info ] [MainThread]: 
[0m14:30:13.899436 [error] [MainThread]: [31mFailure in test not_null_fct_experiments_assignments_assigned_at (models/marts/experiments/experiments.yml)[0m
[0m14:30:13.899596 [error] [MainThread]:   Got 3300 results, configured to fail if != 0
[0m14:30:13.899714 [info ] [MainThread]: 
[0m14:30:13.899863 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/experiments.yml/not_null_fct_experiments_assignments_assigned_at.sql
[0m14:30:13.899991 [info ] [MainThread]: 
[0m14:30:13.900129 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m14:30:13.901179 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.7169268, "process_in_blocks": "0", "process_kernel_time": 0.208098, "process_mem_max_rss": "153927680", "process_out_blocks": "0", "process_user_time": 1.154499}
[0m14:30:13.901386 [debug] [MainThread]: Command `dbt test` failed at 14:30:13.901348 after 0.72 seconds
[0m14:30:13.901564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105927e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105924050>]}
[0m14:30:13.901737 [debug] [MainThread]: Flushing usage events
[0m14:30:14.030136 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:08.790235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106683500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff99a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff9790>]}


============================== 14:34:08.803928 | c04275c5-4f96-4002-9102-daf8febb49b4 ==============================
[0m14:34:08.803928 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:08.804277 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'empty': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'indirect_selection': 'eager', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'target_path': 'None', 'debug': 'False', 'log_cache_events': 'False', 'version_check': 'True', 'no_print': 'None', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'write_json': 'True', 'printer_width': '80', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'partial_parse': 'True'}
[0m14:34:08.978175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070d8350>]}
[0m14:34:09.006826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070960c0>]}
[0m14:34:09.008243 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:09.060129 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:09.130534 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:34:09.130957 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/fct_experiments_assignments.sql
[0m14:34:09.332171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10793c650>]}
[0m14:34:09.411430 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:09.413744 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:09.435316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077a07a0>]}
[0m14:34:09.435635 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:34:09.435820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067d28d0>]}
[0m14:34:09.437766 [info ] [MainThread]: 
[0m14:34:09.437993 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:34:09.438138 [info ] [MainThread]: 
[0m14:34:09.438388 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:34:09.440840 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:34:09.473652 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:09.473883 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:34:09.474025 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:09.487401 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m14:34:09.487647 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:09.487795 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:34:09.504836 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:34:09.506170 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:34:09.507205 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:34:09.507533 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:34:09.508674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c04275c5-4f96-4002-9102-daf8febb49b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107958830>]}
[0m14:34:09.508929 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.509071 [debug] [MainThread]: On master: BEGIN
[0m14:34:09.509201 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:34:09.509533 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.509665 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.509794 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.509911 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.510107 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.510237 [debug] [MainThread]: On master: Close
[0m14:34:09.511869 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.512067 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.512409 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.512569 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.512270 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:34:09.512776 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:34:09.512917 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:34:09.513301 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:34:09.513096 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:34:09.513568 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:34:09.513769 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:34:09.513931 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.514116 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:34:09.514262 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.514407 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.520249 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.520539 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.522897 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.527681 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.529664 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.530761 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.537207 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.539640 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.540870 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.541141 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.541294 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.542447 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.543458 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.543744 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.543989 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.544176 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:34:09.544385 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:34:09.544584 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.544863 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.545025 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:34:09.545188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.545338 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:34:09.545486 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:34:09.545848 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:34:09.546052 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:34:09.546222 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546370 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546651 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:34:09.546818 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.546978 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:34:09.547120 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.547292 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:34:09.547463 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:34:09.547629 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:34:09.547785 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:34:09.548032 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.548269 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.549395 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.549556 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.551765 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:34:09.552496 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:34:09.552710 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m14:34:09.553150 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:34:09.553537 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:34:09.553725 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:34:09.553878 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m14:34:09.554502 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:34:09.554656 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:34:09.554983 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.04s]
[0m14:34:09.555681 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:34:09.556157 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:34:09.556410 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.04s]
[0m14:34:09.556681 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:34:09.557136 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:34:09.557306 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:34:09.557547 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:34:09.557733 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.557920 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:34:09.558316 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.558169 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.04s]
[0m14:34:09.558542 [info ] [Thread-3 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:34:09.558750 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.05s]
[0m14:34:09.559170 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:34:09.558934 [info ] [Thread-4 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:34:09.559384 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:34:09.559599 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:34:09.559764 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.559949 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:34:09.560106 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.560278 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.560636 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.560453 [info ] [Thread-1 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:34:09.563112 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.563409 [info ] [Thread-2 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:34:09.565504 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.565735 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:34:09.566018 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:34:09.566314 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.566502 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.568408 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.570325 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.570540 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.570704 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.572694 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.573755 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.574011 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.574170 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.575262 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.576318 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.576603 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.576800 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.576989 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:34:09.577165 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:34:09.577341 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.577494 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:34:09.577667 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:34:09.577826 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:34:09.577998 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.578329 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.578493 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:34:09.578654 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.578794 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.578978 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:34:09.579130 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:34:09.579275 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.579412 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:34:09.579622 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.579788 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:34:09.579933 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.580080 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.580321 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.580500 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:34:09.580718 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:09.580969 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.581157 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:34:09.581992 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:34:09.582935 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:34:09.583221 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.583385 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.583838 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:34:09.584311 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:34:09.585760 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:34:09.586005 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:34:09.586714 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:34:09.586872 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:34:09.587383 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:34:09.588136 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:34:09.587700 [info ] [Thread-3 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.03s]
[0m14:34:09.588615 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:34:09.588473 [info ] [Thread-4 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:34:09.588878 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:34:09.589182 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:34:09.589707 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:34:09.589494 [info ] [Thread-2 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:34:09.590190 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.590022 [error] [Thread-1 (]: 7 of 12 FAIL 3300 not_null_fct_experiments_assignments_assigned_at ............. [[31mFAIL 3300[0m in 0.02s]
[0m14:34:09.590453 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.590690 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:34:09.591062 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:34:09.590864 [info ] [Thread-3 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:34:09.591278 [info ] [Thread-4 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:34:09.591464 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.591656 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.591854 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:34:09.592033 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:34:09.592202 [info ] [Thread-2 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:34:09.592385 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:34:09.592561 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.592725 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.592893 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:34:09.593067 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:34:09.595340 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.597174 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.597361 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.597523 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.600549 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.602750 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.603200 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.603399 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.605641 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.606951 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.607134 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.607418 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.609588 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.610771 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.611092 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.611290 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:34:09.611466 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:34:09.611771 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.611986 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:34:09.612145 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:34:09.612329 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.612588 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.612754 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.612924 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:34:09.613097 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:34:09.613246 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.613409 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:34:09.613562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:09.613708 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:34:09.613863 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:34:09.614032 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.614330 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:34:09.614502 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.614729 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.614948 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:34:09.615120 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:34:09.615296 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:34:09.615460 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.615605 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:34:09.615798 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:34:09.616741 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:34:09.617413 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:34:09.618029 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:34:09.618455 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:34:09.618795 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:34:09.618988 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:34:09.619153 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:34:09.619307 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:34:09.619605 [info ] [Thread-4 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:34:09.620360 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:34:09.620615 [info ] [Thread-3 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:34:09.620898 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:34:09.621526 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:34:09.621982 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:34:09.622197 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:34:09.622680 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:34:09.622829 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:34:09.623024 [debug] [Thread-2 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:34:09.623297 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:34:09.623743 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:34:09.623500 [info ] [Thread-2 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:34:09.624035 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:34:09.624605 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.624755 [debug] [MainThread]: On master: BEGIN
[0m14:34:09.624877 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:34:09.625221 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.625385 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.625522 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:09.625643 [debug] [MainThread]: On master: COMMIT
[0m14:34:09.625852 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:09.625983 [debug] [MainThread]: On master: Close
[0m14:34:09.626156 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:09.626276 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:34:09.626392 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:34:09.626505 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:34:09.626614 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:34:09.626751 [info ] [MainThread]: 
[0m14:34:09.626889 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m14:34:09.627693 [debug] [MainThread]: Command end result
[0m14:34:09.644106 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:09.646098 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:09.652347 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:34:09.652570 [info ] [MainThread]: 
[0m14:34:09.652745 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:34:09.652885 [info ] [MainThread]: 
[0m14:34:09.653058 [error] [MainThread]: [31mFailure in test not_null_fct_experiments_assignments_assigned_at (models/marts/experiments/experiments.yml)[0m
[0m14:34:09.653217 [error] [MainThread]:   Got 3300 results, configured to fail if != 0
[0m14:34:09.653337 [info ] [MainThread]: 
[0m14:34:09.653484 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/experiments.yml/not_null_fct_experiments_assignments_assigned_at.sql
[0m14:34:09.653612 [info ] [MainThread]: 
[0m14:34:09.653751 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=12
[0m14:34:09.655165 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.9032498, "process_in_blocks": "0", "process_kernel_time": 0.204495, "process_mem_max_rss": "156286976", "process_out_blocks": "0", "process_user_time": 1.33338}
[0m14:34:09.655382 [debug] [MainThread]: Command `dbt test` failed at 14:34:09.655343 after 0.90 seconds
[0m14:34:09.655562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ff99d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106299580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062980e0>]}
[0m14:34:09.655727 [debug] [MainThread]: Flushing usage events
[0m14:34:10.185967 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:56.538549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065e7530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d9d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d9d760>]}


============================== 14:34:56.550873 | 6c9e2de0-5409-4fd6-82c5-409b6c2789ff ==============================
[0m14:34:56.550873 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:34:56.551212 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'introspect': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'log_format': 'default', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'debug': 'False', 'partial_parse': 'True', 'write_json': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'quiet': 'False', 'printer_width': '80', 'empty': 'False', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:34:56.717130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ecc440>]}
[0m14:34:56.745633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106844e30>]}
[0m14:34:56.747075 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:34:56.799077 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:34:56.868850 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:56.869089 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:34:56.869224 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:56.891518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107028ef0>]}
[0m14:34:56.935683 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:56.937233 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:56.951630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073fdee0>]}
[0m14:34:56.951905 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:34:56.952077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a0290>]}
[0m14:34:56.953689 [info ] [MainThread]: 
[0m14:34:56.953869 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:34:56.954012 [info ] [MainThread]: 
[0m14:34:56.954261 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:34:56.956428 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:34:56.986453 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:34:56.986688 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:34:56.986835 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:57.006048 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m14:34:57.007053 [debug] [ThreadPool]: On list_analytics: Close
[0m14:34:57.007417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:34:57.007667 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:34:57.011059 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.011252 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:34:57.011386 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:34:57.012195 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:34:57.012807 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.012948 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:34:57.013163 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.013287 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.013409 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:34:57.013973 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.014461 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:34:57.014616 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:34:57.014748 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:34:57.014969 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.015108 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:34:57.015995 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:34:57.018841 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:57.019011 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:34:57.019138 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:34:57.019439 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:34:57.019571 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:34:57.019705 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:34:57.031720 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m14:34:57.032637 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:34:57.033626 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:34:57.033761 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:34:57.034900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e7e660>]}
[0m14:34:57.035126 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.035253 [debug] [MainThread]: On master: BEGIN
[0m14:34:57.035367 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:34:57.035610 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.035735 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.035855 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.035970 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.036155 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.036277 [debug] [MainThread]: On master: Close
[0m14:34:57.037738 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.037981 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m14:34:57.038177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m14:34:57.038334 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.043325 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.044609 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.060189 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.060898 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.061074 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m14:34:57.061236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:57.061626 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.061792 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.061973 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m14:34:57.080931 [debug] [Thread-1 (]: SQL status: OK in 0.019 seconds
[0m14:34:57.083860 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.084069 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:34:57.084558 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.085203 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.085381 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m14:34:57.085833 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.088968 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.089160 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m14:34:57.089992 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:57.091374 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.091543 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m14:34:57.091844 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.099051 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:34:57.099260 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.099425 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m14:34:57.101533 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:34:57.104079 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m14:34:57.104261 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m14:34:57.104995 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:34:57.106132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m14:34:57.107076 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104446f60>]}
[0m14:34:57.107395 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m14:34:57.107661 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m14:34:57.108010 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.108290 [info ] [Thread-3 (]: 2 of 3 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m14:34:57.108645 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m14:34:57.108844 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.110469 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.111163 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.112721 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.113409 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.113598 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m14:34:57.113758 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:34:57.114147 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.114304 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.114510 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m14:34:57.129525 [debug] [Thread-3 (]: SQL status: OK in 0.015 seconds
[0m14:34:57.130220 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.130399 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:34:57.130870 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.131450 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.131614 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m14:34:57.132033 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.133737 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.133914 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m14:34:57.134230 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.136279 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.136445 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m14:34:57.136750 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.137478 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:34:57.137634 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.137770 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m14:34:57.140105 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:34:57.172906 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m14:34:57.173172 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m14:34:57.173699 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.174356 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m14:34:57.174625 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067256a0>]}
[0m14:34:57.174927 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.07s]
[0m14:34:57.175178 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m14:34:57.175496 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m14:34:57.175785 [info ] [Thread-2 (]: 3 of 3 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m14:34:57.176073 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m14:34:57.176245 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m14:34:57.178142 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.178933 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m14:34:57.180486 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.181041 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.181232 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m14:34:57.181402 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:34:57.181774 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.181940 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.182251 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m14:34:57.193386 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m14:34:57.194064 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.194249 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m14:34:57.194714 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.195301 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.195518 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m14:34:57.195905 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.197431 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.197601 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m14:34:57.197900 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.199167 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.199349 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m14:34:57.199689 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.200474 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:34:57.200637 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.200780 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m14:34:57.201369 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.202512 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m14:34:57.202679 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m14:34:57.203082 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m14:34:57.203668 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m14:34:57.203922 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c9e2de0-5409-4fd6-82c5-409b6c2789ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079145f0>]}
[0m14:34:57.204224 [info ] [Thread-2 (]: 3 of 3 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.03s]
[0m14:34:57.204467 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m14:34:57.205033 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.205186 [debug] [MainThread]: On master: BEGIN
[0m14:34:57.205310 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:34:57.205555 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.205688 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.205818 [debug] [MainThread]: Using duckdb connection "master"
[0m14:34:57.205936 [debug] [MainThread]: On master: COMMIT
[0m14:34:57.206130 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:34:57.206258 [debug] [MainThread]: On master: Close
[0m14:34:57.206415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:57.206532 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiments_assignments' was properly closed.
[0m14:34:57.206645 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m14:34:57.206754 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m14:34:57.206909 [info ] [MainThread]: 
[0m14:34:57.207059 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m14:34:57.207458 [debug] [MainThread]: Command end result
[0m14:34:57.223856 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:34:57.224913 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:34:57.227879 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:34:57.228046 [info ] [MainThread]: 
[0m14:34:57.228224 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:34:57.228358 [info ] [MainThread]: 
[0m14:34:57.228505 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:34:57.229832 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7288726, "process_in_blocks": "0", "process_kernel_time": 0.219692, "process_mem_max_rss": "185958400", "process_out_blocks": "0", "process_user_time": 1.182765}
[0m14:34:57.230055 [debug] [MainThread]: Command `dbt run` succeeded at 14:34:57.230016 after 0.73 seconds
[0m14:34:57.230221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107563230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107561b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107563ec0>]}
[0m14:34:57.230382 [debug] [MainThread]: Flushing usage events
[0m14:34:57.336502 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:10.961546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087159a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ee5940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ee5700>]}


============================== 14:35:10.965577 | f96468e7-43aa-4036-9fed-3ac07b0327db ==============================
[0m14:35:10.965577 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:35:10.965941 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'fail_fast': 'False', 'version_check': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'no_print': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'partial_parse': 'True', 'quiet': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'log_format': 'default', 'warn_error': 'None', 'introspect': 'True', 'write_json': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True'}
[0m14:35:11.138862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108aca870>]}
[0m14:35:11.167019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090d7fe0>]}
[0m14:35:11.169175 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:35:11.220647 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:35:11.287810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:11.288061 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:35:11.288202 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:11.310526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ebbad0>]}
[0m14:35:11.354641 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:11.356101 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:11.373605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10918c800>]}
[0m14:35:11.373898 [info ] [MainThread]: Found 8 models, 38 data tests, 3 sources, 472 macros
[0m14:35:11.374079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091b3f80>]}
[0m14:35:11.375881 [info ] [MainThread]: 
[0m14:35:11.376072 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:35:11.376215 [info ] [MainThread]: 
[0m14:35:11.376462 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:35:11.378748 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:35:11.406479 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:11.406701 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:35:11.406843 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:11.420717 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m14:35:11.420961 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:35:11.421122 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:35:11.438346 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m14:35:11.439508 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:35:11.440525 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:35:11.440701 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:35:11.441820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f96468e7-43aa-4036-9fed-3ac07b0327db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109170d10>]}
[0m14:35:11.442062 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.442199 [debug] [MainThread]: On master: BEGIN
[0m14:35:11.442327 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:35:11.442628 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.442766 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.442897 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.443020 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.443234 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.443362 [debug] [MainThread]: On master: Close
[0m14:35:11.444920 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.445119 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.445457 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.445321 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m14:35:11.445661 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.445824 [info ] [Thread-2 (]: 2 of 12 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m14:35:11.446005 [info ] [Thread-3 (]: 3 of 12 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m14:35:11.446247 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m14:35:11.446471 [info ] [Thread-4 (]: 4 of 12 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m14:35:11.446704 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:35:11.446903 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:35:11.447068 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.447262 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:35:11.447415 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.447567 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.460268 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.460519 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.463334 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.467604 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.470339 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.470709 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.480034 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.480449 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.480656 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.480842 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.482057 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.483301 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.485121 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.485343 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.485694 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m14:35:11.485902 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.486316 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.486518 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.486692 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.486875 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.487043 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m14:35:11.487204 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m14:35:11.487381 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m14:35:11.487559 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m14:35:11.487719 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:35:11.487900 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m14:35:11.488062 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:35:11.488205 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:35:11.488572 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.488781 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.488949 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m14:35:11.489109 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.489258 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m14:35:11.489438 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m14:35:11.489609 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m14:35:11.489779 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.490032 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.490979 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.491191 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.493215 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m14:35:11.493468 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m14:35:11.527279 [debug] [Thread-1 (]: SQL status: OK in 0.039 seconds
[0m14:35:11.527881 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m14:35:11.528353 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m14:35:11.529146 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m14:35:11.529840 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m14:35:11.530429 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m14:35:11.530861 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m14:35:11.531284 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m14:35:11.531686 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m14:35:11.532006 [info ] [Thread-3 (]: 3 of 12 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.09s]
[0m14:35:11.532185 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m14:35:11.532345 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m14:35:11.532494 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m14:35:11.532717 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m14:35:11.533020 [info ] [Thread-1 (]: 1 of 12 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.09s]
[0m14:35:11.533314 [info ] [Thread-4 (]: 4 of 12 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.09s]
[0m14:35:11.533593 [info ] [Thread-2 (]: 2 of 12 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.09s]
[0m14:35:11.533800 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.534026 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m14:35:11.534272 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m14:35:11.534480 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m14:35:11.534644 [info ] [Thread-3 (]: 5 of 12 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m14:35:11.534823 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.534983 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.535162 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.535337 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m14:35:11.535504 [info ] [Thread-1 (]: 6 of 12 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m14:35:11.535690 [info ] [Thread-4 (]: 7 of 12 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m14:35:11.535872 [info ] [Thread-2 (]: 8 of 12 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m14:35:11.536043 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.536216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m14:35:11.536382 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m14:35:11.536542 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m14:35:11.538871 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.539115 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.539319 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.539557 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.541637 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.543531 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.545316 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.545584 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.546813 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.547112 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.547292 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.547467 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.548597 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.549748 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.549968 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.551063 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.551311 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m14:35:11.551565 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:35:11.551904 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.552094 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.552268 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m14:35:11.552439 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.552613 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.552764 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m14:35:11.552914 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:35:11.553070 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m14:35:11.553219 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m14:35:11.553359 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.553583 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.553750 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:35:11.553902 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554232 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m14:35:11.554389 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554550 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.554715 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.554862 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m14:35:11.555007 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.555208 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m14:35:11.555404 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.555629 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.555788 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.556506 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m14:35:11.557361 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m14:35:11.557828 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m14:35:11.558257 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m14:35:11.558419 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m14:35:11.558586 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:35:11.558745 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m14:35:11.558897 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:35:11.559834 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m14:35:11.559213 [info ] [Thread-3 (]: 5 of 12 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.02s]
[0m14:35:11.560131 [info ] [Thread-2 (]: 8 of 12 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m14:35:11.560859 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m14:35:11.561252 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m14:35:11.561474 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m14:35:11.561701 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m14:35:11.562123 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m14:35:11.562273 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m14:35:11.562448 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.562641 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.562806 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m14:35:11.563047 [info ] [Thread-4 (]: 7 of 12 PASS not_null_fct_experiments_assignments_assigned_at .................. [[32mPASS[0m in 0.03s]
[0m14:35:11.563181 [info ] [Thread-3 (]: 9 of 12 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m14:35:11.563362 [info ] [Thread-2 (]: 10 of 12 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m14:35:11.563846 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m14:35:11.563659 [info ] [Thread-1 (]: 6 of 12 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:35:11.564067 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m14:35:11.564241 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m14:35:11.564409 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.564630 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m14:35:11.564789 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.564937 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.565268 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.565093 [info ] [Thread-4 (]: 11 of 12 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m14:35:11.567761 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.570072 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.570313 [info ] [Thread-1 (]: 12 of 12 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m14:35:11.570598 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m14:35:11.570948 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m14:35:11.571163 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.571331 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.571562 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.571695 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.575772 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.577639 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.578826 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.579943 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.580777 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.581094 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.582268 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.582444 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.582644 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.582852 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m14:35:11.584080 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.586261 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m14:35:11.586547 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:35:11.586844 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.587022 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:35:11.587354 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m14:35:11.587621 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.587780 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:35:11.587944 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588094 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588239 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m14:35:11.588463 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m14:35:11.588618 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m14:35:11.588771 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.588912 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:11.589077 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.589246 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:35:11.589401 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m14:35:11.589766 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:35:11.590035 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590194 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590347 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:35:11.590498 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m14:35:11.590774 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:35:11.591589 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m14:35:11.592643 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m14:35:11.593281 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m14:35:11.593444 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m14:35:11.593606 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:35:11.593997 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m14:35:11.594141 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m14:35:11.594873 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m14:35:11.595038 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m14:35:11.595666 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m14:35:11.595970 [info ] [Thread-3 (]: 9 of 12 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m14:35:11.596467 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m14:35:11.596744 [info ] [Thread-2 (]: 10 of 12 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m14:35:11.597215 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m14:35:11.597439 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m14:35:11.597610 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m14:35:11.597838 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m14:35:11.597991 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m14:35:11.598341 [info ] [Thread-4 (]: 11 of 12 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.03s]
[0m14:35:11.598827 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m14:35:11.598623 [info ] [Thread-1 (]: 12 of 12 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.03s]
[0m14:35:11.599179 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m14:35:11.599890 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.600049 [debug] [MainThread]: On master: BEGIN
[0m14:35:11.600175 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:35:11.600488 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.600621 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.600746 [debug] [MainThread]: Using duckdb connection "master"
[0m14:35:11.600859 [debug] [MainThread]: On master: COMMIT
[0m14:35:11.601050 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:35:11.601176 [debug] [MainThread]: On master: Close
[0m14:35:11.601340 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:11.601500 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m14:35:11.601628 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m14:35:11.601745 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b' was properly closed.
[0m14:35:11.601861 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m14:35:11.602002 [info ] [MainThread]: 
[0m14:35:11.602140 [info ] [MainThread]: Finished running 12 data tests in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m14:35:11.602981 [debug] [MainThread]: Command end result
[0m14:35:11.619311 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:35:11.620484 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:35:11.624054 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:35:11.624231 [info ] [MainThread]: 
[0m14:35:11.624405 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:35:11.624546 [info ] [MainThread]: 
[0m14:35:11.624697 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m14:35:11.626125 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.69964886, "process_in_blocks": "0", "process_kernel_time": 0.180843, "process_mem_max_rss": "152420352", "process_out_blocks": "0", "process_user_time": 1.166883}
[0m14:35:11.626362 [debug] [MainThread]: Command `dbt test` succeeded at 14:35:11.626320 after 0.70 seconds
[0m14:35:11.626538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cefc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109015d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109015940>]}
[0m14:35:11.626703 [debug] [MainThread]: Flushing usage events
[0m14:35:11.717931 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:04:37.568259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048df740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d6a0>]}


============================== 15:04:37.582986 | 631f7fc4-58b8-4416-8a9b-231ddc47653f ==============================
[0m15:04:37.582986 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:04:37.583402 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'empty': 'False', 'introspect': 'True', 'no_print': 'None', 'log_cache_events': 'False', 'version_check': 'True', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'partial_parse': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'write_json': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m15:04:37.755212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102647d10>]}
[0m15:04:37.783193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045bfef0>]}
[0m15:04:37.784477 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:04:37.835127 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:04:37.905436 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m15:04:37.905796 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:04:37.906085 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/experiments.yml
[0m15:04:38.103500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058a9df0>]}
[0m15:04:38.180318 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:04:38.182101 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:04:38.196785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055fcd40>]}
[0m15:04:38.197068 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:04:38.197244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105035c40>]}
[0m15:04:38.198874 [info ] [MainThread]: 
[0m15:04:38.199061 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:04:38.199200 [info ] [MainThread]: 
[0m15:04:38.199436 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:04:38.201654 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:04:38.231328 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:04:38.231570 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:04:38.231720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:04:38.250417 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:04:38.251396 [debug] [ThreadPool]: On list_analytics: Close
[0m15:04:38.251750 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:04:38.252004 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:04:38.255312 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.255601 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:04:38.255752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:04:38.256435 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:04:38.257053 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.257183 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:04:38.257395 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.257519 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.257642 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:04:38.257877 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.258250 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:04:38.258382 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:04:38.258498 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:04:38.258694 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.258821 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:04:38.259508 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:04:38.262879 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:04:38.263215 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:04:38.263462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:04:38.263852 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:04:38.263996 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:04:38.264135 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:04:38.276486 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:04:38.277349 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:04:38.278222 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:04:38.278379 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:04:38.279423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058fd280>]}
[0m15:04:38.279665 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.279805 [debug] [MainThread]: On master: BEGIN
[0m15:04:38.279926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:04:38.280243 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.280425 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.280577 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.280703 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.280921 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.281059 [debug] [MainThread]: On master: Close
[0m15:04:38.282479 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.282733 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:04:38.282950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:04:38.283116 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.286984 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.288396 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.304797 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.305918 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.306091 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:04:38.306240 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:04:38.306591 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.306736 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.306909 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:04:38.324274 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m15:04:38.327174 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.327387 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:04:38.327869 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.328490 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.328668 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:04:38.329090 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.332362 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.332558 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:04:38.334020 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.335429 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.335597 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:04:38.335877 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.343132 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:04:38.343357 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.343518 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:04:38.345695 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.348639 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:04:38.348856 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:04:38.349750 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.351011 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:04:38.352010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c4aa0>]}
[0m15:04:38.352344 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:04:38.352622 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:04:38.352995 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.353226 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:04:38.353462 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:04:38.353619 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.355401 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.356515 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.358059 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.358981 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.359195 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:04:38.359367 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:04:38.359885 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.360072 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.360297 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:04:38.369731 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:04:38.370550 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.370738 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:04:38.371230 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.371805 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.371967 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:04:38.372357 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.373918 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.374088 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:04:38.374381 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.375663 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.375822 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:04:38.376093 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.376844 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:04:38.377078 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.377256 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:04:38.378988 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.380057 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:04:38.380218 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:04:38.380578 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.381156 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:04:38.381414 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c2b620>]}
[0m15:04:38.381695 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:04:38.381936 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:04:38.382280 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:04:38.382496 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.382701 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:04:38.382948 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:04:38.383197 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:04:38.383383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:04:38.383551 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:04:38.383703 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.386369 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.388094 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.388616 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.390493 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.390833 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:04:38.392445 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.392708 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.392949 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:04:38.393125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:04:38.393407 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.393638 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:04:38.393845 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:04:38.394021 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.394243 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:04:38.394541 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:04:38.395751 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:04:38.395899 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.396189 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:04:38.399826 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:04:38.400142 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:04:38.400386 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:04:38.405704 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:04:38.406009 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:04:38.407709 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:04:38.408174 [debug] [Thread-2 (]: SQL status: OK in 0.012 seconds
[0m15:04:38.408445 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c30260>]}
[0m15:04:38.409043 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.411404 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:04:38.411216 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:04:38.411836 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:04:38.412024 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.412263 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:04:38.412930 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.413441 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:04:38.413868 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.415500 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.415675 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:04:38.415995 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.417312 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.417498 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:04:38.417796 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.418834 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:04:38.419080 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.419254 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:04:38.419941 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:04:38.421146 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:04:38.421314 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:04:38.421738 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:04:38.422336 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:04:38.422599 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '631f7fc4-58b8-4416-8a9b-231ddc47653f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b769f0>]}
[0m15:04:38.422875 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:04:38.423115 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:04:38.423659 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.423807 [debug] [MainThread]: On master: BEGIN
[0m15:04:38.423929 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:04:38.424163 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.424291 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.424415 [debug] [MainThread]: Using duckdb connection "master"
[0m15:04:38.424532 [debug] [MainThread]: On master: COMMIT
[0m15:04:38.424721 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:04:38.424845 [debug] [MainThread]: On master: Close
[0m15:04:38.425002 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:04:38.425117 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:04:38.425223 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:04:38.425330 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:04:38.425485 [info ] [MainThread]: 
[0m15:04:38.425634 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:04:38.426069 [debug] [MainThread]: Command end result
[0m15:04:38.442854 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:04:38.444042 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:04:38.447557 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:04:38.447747 [info ] [MainThread]: 
[0m15:04:38.447921 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:04:38.448063 [info ] [MainThread]: 
[0m15:04:38.448238 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:04:38.448410 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:04:38.448541 [info ] [MainThread]: 
[0m15:04:38.448690 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:04:38.448817 [info ] [MainThread]: 
[0m15:04:38.448954 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:04:38.450343 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9215141, "process_in_blocks": "0", "process_kernel_time": 0.223921, "process_mem_max_rss": "188858368", "process_out_blocks": "0", "process_user_time": 1.374879}
[0m15:04:38.450560 [debug] [MainThread]: Command `dbt run` failed at 15:04:38.450520 after 0.92 seconds
[0m15:04:38.450737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f9d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050c8e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c0f140>]}
[0m15:04:38.450899 [debug] [MainThread]: Flushing usage events
[0m15:04:38.571623 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:24.859384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023be690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e398b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e39640>]}


============================== 15:05:24.874126 | e0818f9d-4332-43f9-aecc-9272ccf4c6cd ==============================
[0m15:05:24.874126 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:24.874488 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None', 'log_format': 'default', 'printer_width': '80', 'version_check': 'True', 'introspect': 'True', 'debug': 'False', 'use_colors': 'True', 'no_print': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'empty': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'warn_error': 'None', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'write_json': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:05:25.039576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f1ae10>]}
[0m15:05:25.068318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103246cc0>]}
[0m15:05:25.069781 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:25.122747 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:05:25.193190 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:25.193434 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:05:25.193572 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:25.215887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd7020>]}
[0m15:05:25.261015 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:25.264771 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:25.278622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055121e0>]}
[0m15:05:25.278896 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:05:25.279064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f1b650>]}
[0m15:05:25.280747 [info ] [MainThread]: 
[0m15:05:25.280926 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:25.281066 [info ] [MainThread]: 
[0m15:05:25.281309 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:05:25.283459 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:05:25.313604 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:05:25.313866 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:05:25.314196 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:25.333567 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:05:25.334553 [debug] [ThreadPool]: On list_analytics: Close
[0m15:05:25.334909 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:05:25.335134 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:05:25.339105 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.339318 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:05:25.339463 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:25.340294 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:25.340954 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.341107 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:05:25.341359 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.341796 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.341966 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:05:25.342370 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.343053 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:25.343209 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:25.343337 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:25.343552 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.343686 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:05:25.344473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:05:25.347238 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:25.347550 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:05:25.347693 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:25.347977 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:25.348119 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:25.348255 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:05:25.359657 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:05:25.360424 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:05:25.361385 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:05:25.361536 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:05:25.362477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10538d310>]}
[0m15:05:25.362728 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.362864 [debug] [MainThread]: On master: BEGIN
[0m15:05:25.362983 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:25.363291 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.363480 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.363628 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.363753 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.363962 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.364092 [debug] [MainThread]: On master: Close
[0m15:05:25.365497 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.365743 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:05:25.365950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:05:25.366112 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.369886 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.370851 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.386422 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.387341 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.387516 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:05:25.387666 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:25.388022 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.388175 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.388355 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:05:25.406129 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m15:05:25.409173 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.409426 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:25.410008 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.410707 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.410981 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:25.411536 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.415633 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.416005 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:05:25.417116 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.418710 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.418913 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:05:25.419354 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.426862 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:25.427117 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.427277 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:25.429713 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:05:25.432589 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:25.432788 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:05:25.433490 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.434707 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:05:25.435675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502a660>]}
[0m15:05:25.436003 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:05:25.436272 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:05:25.436629 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.436910 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:05:25.437197 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:05:25.437367 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.439167 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.440199 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.443030 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.475948 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.476234 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:05:25.476405 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:05:25.476783 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.476945 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.477184 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:05:25.487084 [debug] [Thread-3 (]: SQL status: OK in 0.010 seconds
[0m15:05:25.487845 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.488081 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:25.488682 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.489442 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.490118 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:25.490679 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.496352 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.496586 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:05:25.496998 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.498469 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.498645 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:05:25.498940 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.499729 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:25.499890 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.500033 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:25.501699 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:05:25.502952 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:25.503146 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:05:25.503591 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.504236 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:05:25.504501 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059e9be0>]}
[0m15:05:25.504795 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.07s]
[0m15:05:25.505052 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:05:25.505410 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:05:25.505643 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.506020 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:05:25.506303 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:05:25.506612 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:05:25.506847 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:05:25.507018 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:05:25.507182 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.509328 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.511147 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.512257 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:05:25.512479 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.514503 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.516202 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.517108 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.517306 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.517475 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:05:25.517647 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:05:25.517812 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:05:25.517964 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:25.518349 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.518503 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.518655 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.518953 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:05:25.519253 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:25.520527 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:05:25.524615 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:05:25.525005 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:05:25.525245 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:05:25.530632 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m15:05:25.531341 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.531656 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:05:25.532468 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:05:25.532756 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:05:25.533491 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.534322 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:05:25.535073 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.535266 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105601c40>]}
[0m15:05:25.535480 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:05:25.535815 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:05:25.536172 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:25.536412 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:05:25.536595 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:25.538580 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.538756 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:05:25.539093 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.541541 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.541730 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:05:25.542050 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.542949 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:05:25.543156 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.543320 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:05:25.543946 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.546084 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:25.546309 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:05:25.546895 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:05:25.547579 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:05:25.547851 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0818f9d-4332-43f9-aecc-9272ccf4c6cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a2c4d0>]}
[0m15:05:25.548136 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:05:25.548387 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:05:25.548962 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.549123 [debug] [MainThread]: On master: BEGIN
[0m15:05:25.549251 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:05:25.549530 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.549659 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.549780 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:25.549896 [debug] [MainThread]: On master: COMMIT
[0m15:05:25.550093 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:25.550217 [debug] [MainThread]: On master: Close
[0m15:05:25.550379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:05:25.550938 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:05:25.551101 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:05:25.551270 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:05:25.551455 [info ] [MainThread]: 
[0m15:05:25.551678 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m15:05:25.552146 [debug] [MainThread]: Command end result
[0m15:05:25.574110 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:25.575350 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:25.578816 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:05:25.578999 [info ] [MainThread]: 
[0m15:05:25.579177 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:05:25.579317 [info ] [MainThread]: 
[0m15:05:25.579492 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:05:25.579662 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:05:25.579791 [info ] [MainThread]: 
[0m15:05:25.579942 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:05:25.580071 [info ] [MainThread]: 
[0m15:05:25.580209 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:05:25.581585 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7585153, "process_in_blocks": "0", "process_kernel_time": 0.214373, "process_mem_max_rss": "194756608", "process_out_blocks": "0", "process_user_time": 1.22754}
[0m15:05:25.581790 [debug] [MainThread]: Command `dbt run` failed at 15:05:25.581751 after 0.76 seconds
[0m15:05:25.581967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d19a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d34a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d2120>]}
[0m15:05:25.582130 [debug] [MainThread]: Flushing usage events
[0m15:05:25.679547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:59.328048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e51910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068dbc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106941640>]}


============================== 15:05:59.334961 | 624fcad9-0504-4ace-b1e5-67339e4074e6 ==============================
[0m15:05:59.334961 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:05:59.335349 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'fail_fast': 'False', 'introspect': 'True', 'no_print': 'None', 'printer_width': '80', 'debug': 'False', 'version_check': 'True', 'cache_selected_only': 'False'}
[0m15:05:59.537947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069c0350>]}
[0m15:05:59.569464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106283e60>]}
[0m15:05:59.570838 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:59.625110 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:05:59.694162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:59.694412 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:05:59.694602 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:59.717623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106be8560>]}
[0m15:05:59.761066 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:05:59.762453 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:05:59.774981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f9c290>]}
[0m15:05:59.775254 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:05:59.775427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069aa0f0>]}
[0m15:05:59.777097 [info ] [MainThread]: 
[0m15:05:59.777268 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:59.777399 [info ] [MainThread]: 
[0m15:05:59.777633 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:05:59.779916 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:05:59.808995 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:05:59.809228 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:05:59.809372 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:59.826949 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:05:59.827955 [debug] [ThreadPool]: On list_analytics: Close
[0m15:05:59.828327 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:05:59.828556 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:05:59.832260 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.832442 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:05:59.832575 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:59.833446 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:59.834092 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.834232 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:05:59.834461 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.834591 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.834717 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:05:59.835201 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.835578 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:59.835713 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:05:59.835827 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:05:59.836019 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.836145 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:05:59.836838 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:05:59.839448 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:59.839600 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:05:59.839718 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:59.839947 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:05:59.840071 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:05:59.840198 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:05:59.850965 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:05:59.851737 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:05:59.852718 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:05:59.852880 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:05:59.853875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a1e930>]}
[0m15:05:59.854103 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:59.854240 [debug] [MainThread]: On master: BEGIN
[0m15:05:59.854362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:59.854616 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:59.854746 [debug] [MainThread]: On master: COMMIT
[0m15:05:59.854868 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:59.854986 [debug] [MainThread]: On master: COMMIT
[0m15:05:59.855183 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:05:59.855304 [debug] [MainThread]: On master: Close
[0m15:05:59.856581 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.856828 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:05:59.857029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:05:59.857193 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.860904 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.861375 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.876771 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.877325 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.877496 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:05:59.877645 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:59.878056 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.878283 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.878538 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:05:59.896053 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m15:05:59.898834 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.899032 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:59.899528 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.900132 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.900297 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:05:59.900710 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.903762 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.903946 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:05:59.904734 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.906152 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.906325 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:05:59.906617 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.913513 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:59.913700 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.913855 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:05:59.915576 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:05:59.918226 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:05:59.918423 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:05:59.919238 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.920404 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:05:59.921328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074262d0>]}
[0m15:05:59.921645 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.06s]
[0m15:05:59.921914 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:05:59.922306 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.922603 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:05:59.922893 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:05:59.923066 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.924791 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.925205 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.927599 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.960283 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.960579 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:05:59.960749 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:05:59.961137 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.961309 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.961518 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:05:59.970698 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:05:59.971470 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.971671 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:59.972144 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.972750 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.972919 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:05:59.973348 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.974978 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.975156 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:05:59.975465 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.976793 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.976965 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:05:59.977238 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.978031 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:59.978216 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.978373 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:05:59.979572 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.980826 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:05:59.981003 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:05:59.981410 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:05:59.982008 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:05:59.982266 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073879e0>]}
[0m15:05:59.982553 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.06s]
[0m15:05:59.982814 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:05:59.983161 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:05:59.983353 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.983596 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:05:59.983891 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:05:59.984184 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:05:59.984431 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:05:59.984611 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:05:59.984785 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.986901 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.988660 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.991574 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:05:59.991762 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:05:59.993353 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.994971 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.995492 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.995808 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:05:59.996013 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:05:59.996187 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:05:59.996404 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:05:59.996711 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:59.997199 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.997402 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:05:59.997573 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:05:59.997878 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:05:59.998399 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:00.000174 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:00.004475 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquistion_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:00.004891 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:06:00.005143 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:06:00.008985 [debug] [Thread-2 (]: SQL status: OK in 0.010 seconds
[0m15:06:00.009649 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.009828 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:00.010427 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.011204 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:06:00.011911 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.012110 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:06:00.012301 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:00.013057 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:00.014714 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.014892 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:06:00.015726 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:06:00.015979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074d69c0>]}
[0m15:06:00.016153 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:00.016452 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:06:00.018861 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.019139 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:00.019321 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:06:00.019565 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^.
[0m15:06:00.020150 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.020918 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:00.021077 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.021224 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:00.021787 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.022876 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:00.023049 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:06:00.023493 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:00.024082 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:06:00.024331 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '624fcad9-0504-4ace-b1e5-67339e4074e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107424e30>]}
[0m15:06:00.024617 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:06:00.024862 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:06:00.025415 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:00.025564 [debug] [MainThread]: On master: BEGIN
[0m15:06:00.025685 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:06:00.025941 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:00.026071 [debug] [MainThread]: On master: COMMIT
[0m15:06:00.026192 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:00.026308 [debug] [MainThread]: On master: COMMIT
[0m15:06:00.026495 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:00.026618 [debug] [MainThread]: On master: Close
[0m15:06:00.026774 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:06:00.026889 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:06:00.026995 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:06:00.027102 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:06:00.027258 [info ] [MainThread]: 
[0m15:06:00.027402 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m15:06:00.027858 [debug] [MainThread]: Command end result
[0m15:06:00.044197 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:00.045304 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:00.048427 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:06:00.048610 [info ] [MainThread]: 
[0m15:06:00.048781 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:06:00.048923 [info ] [MainThread]: 
[0m15:06:00.049100 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:06:00.049271 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: Referenced column "acquistion_channel" not found in FROM clause!
  Candidate bindings: "acquisition_channel", "assigned_at_signup", "assigned_at", "country", "user_id"
  
  LINE 29:     group by acquistion_channel, experiment_variant 
                        ^
[0m15:06:00.049402 [info ] [MainThread]: 
[0m15:06:00.049549 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:00.049678 [info ] [MainThread]: 
[0m15:06:00.049815 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:06:00.051216 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7602901, "process_in_blocks": "0", "process_kernel_time": 0.206532, "process_mem_max_rss": "188563456", "process_out_blocks": "0", "process_user_time": 1.207264}
[0m15:06:00.051451 [debug] [MainThread]: Command `dbt run` failed at 15:06:00.051411 after 0.76 seconds
[0m15:06:00.051634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046356d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074e62a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074e42c0>]}
[0m15:06:00.051796 [debug] [MainThread]: Flushing usage events
[0m15:06:00.167628 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:06:41.859050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6ea20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f65850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f655e0>]}


============================== 15:06:41.876552 | 89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4 ==============================
[0m15:06:41.876552 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:06:41.876883 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'empty': 'False', 'introspect': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'use_experimental_parser': 'False', 'target_path': 'None', 'use_colors': 'True', 'log_cache_events': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select path:models/marts/experiments', 'fail_fast': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'no_print': 'None', 'cache_selected_only': 'False'}
[0m15:06:42.054714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fcc590>]}
[0m15:06:42.083315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071035c0>]}
[0m15:06:42.084761 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:06:42.137188 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:06:42.207679 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:06:42.208095 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:42.401054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075b1a30>]}
[0m15:06:42.480177 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:42.481735 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:42.495848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077849e0>]}
[0m15:06:42.496132 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:06:42.496315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078c9340>]}
[0m15:06:42.498005 [info ] [MainThread]: 
[0m15:06:42.498182 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:06:42.498321 [info ] [MainThread]: 
[0m15:06:42.498541 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:06:42.500745 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:06:42.529834 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:06:42.530075 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:06:42.530218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:06:42.547725 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:06:42.548668 [debug] [ThreadPool]: On list_analytics: Close
[0m15:06:42.549018 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:06:42.549270 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:06:42.552518 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.552687 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:06:42.552820 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:42.553544 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:06:42.554329 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.554594 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:06:42.554868 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.555006 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.555140 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:06:42.555523 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.555916 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:06:42.556052 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:06:42.556174 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:06:42.556380 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.556515 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:06:42.557211 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:06:42.559813 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:06:42.559974 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:06:42.560151 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:06:42.560388 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:06:42.560521 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:06:42.560653 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:06:42.570895 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:06:42.571822 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:06:42.572724 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:06:42.572883 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:06:42.573708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a23770>]}
[0m15:06:42.573935 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.574067 [debug] [MainThread]: On master: BEGIN
[0m15:06:42.574184 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:42.574424 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.574556 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.574679 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.574794 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.574979 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.575098 [debug] [MainThread]: On master: Close
[0m15:06:42.576470 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.576702 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:06:42.576897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:06:42.577056 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.580999 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.582029 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.597618 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.598669 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.598861 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:06:42.599017 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:42.599377 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.599537 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.599723 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:06:42.618388 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m15:06:42.621241 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.621439 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:06:42.621949 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.623361 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.623537 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:06:42.623962 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.627069 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.627254 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:06:42.628134 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.629638 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.629826 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:06:42.630141 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.637291 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:06:42.637514 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.637681 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:06:42.639992 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:06:42.643118 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:06:42.643354 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:06:42.644059 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.645262 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:06:42.646254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104612db0>]}
[0m15:06:42.646567 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:06:42.646835 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:06:42.647218 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.647482 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:06:42.647724 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:06:42.647888 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.649582 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.650416 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.651923 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.652530 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.652699 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:06:42.652900 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:06:42.653290 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.653448 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.653681 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:06:42.662871 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:06:42.663558 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.663735 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:06:42.664213 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.664803 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.665029 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:06:42.665447 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.666969 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.667139 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:06:42.667435 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.668732 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.668898 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:06:42.669168 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.669924 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:06:42.670200 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.670344 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:06:42.671939 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.673019 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:06:42.673170 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:06:42.673550 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.674208 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:06:42.674474 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd73e0>]}
[0m15:06:42.674760 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:06:42.675016 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:06:42.675354 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:06:42.675561 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.675795 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:06:42.676249 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:06:42.675997 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:06:42.676454 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:06:42.676634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:06:42.678547 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.678784 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.681622 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.682333 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.684250 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.684591 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:06:42.686128 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.686436 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.686611 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:06:42.686763 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:42.687090 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.687258 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:06:42.687414 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.687553 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:06:42.687700 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:06:42.688040 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:42.689236 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:06:42.689385 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.689660 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:06:42.694918 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) > 1.96
            )
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:06:42.695387 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m15:06:42.695721 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: ROLLBACK
[0m15:06:42.700962 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_experiment_results_by_channel'
[0m15:06:42.701196 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:06:42.701380 [debug] [Thread-2 (]: SQL status: OK in 0.011 seconds
[0m15:06:42.701992 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.702172 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:42.703515 [debug] [Thread-1 (]: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^
[0m15:06:42.703802 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.703989 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c071a0>]}
[0m15:06:42.704622 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.705104 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:06:42.704951 [error] [Thread-1 (]: 4 of 4 ERROR creating sql table model main.rpt_experiment_results_by_channel ... [[31mERROR[0m in 0.03s]
[0m15:06:42.705476 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:06:42.705730 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_experiment_results_by_channel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^.
[0m15:06:42.706283 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:06:42.708119 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.708324 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:06:42.708701 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.710083 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.713907 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:06:42.714528 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.715425 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:42.715600 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.715746 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:06:42.716328 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.717463 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:06:42.717634 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:06:42.718035 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:06:42.718622 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:06:42.718871 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89b8eb3e-8878-4bf8-b57b-6ae9c5c508c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c17020>]}
[0m15:06:42.719144 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.04s]
[0m15:06:42.719385 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:06:42.719938 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.720088 [debug] [MainThread]: On master: BEGIN
[0m15:06:42.720212 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:06:42.720452 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.720582 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.720705 [debug] [MainThread]: Using duckdb connection "master"
[0m15:06:42.720819 [debug] [MainThread]: On master: COMMIT
[0m15:06:42.721004 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:06:42.721129 [debug] [MainThread]: On master: Close
[0m15:06:42.721291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:06:42.721407 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:06:42.721511 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:06:42.721613 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:06:42.721765 [info ] [MainThread]: 
[0m15:06:42.721902 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m15:06:42.722361 [debug] [MainThread]: Command end result
[0m15:06:42.738961 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:06:42.740061 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:06:42.743290 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:06:42.743447 [info ] [MainThread]: 
[0m15:06:42.743609 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:06:42.743746 [info ] [MainThread]: 
[0m15:06:42.743921 [error] [MainThread]: [31mFailure in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)[0m
[0m15:06:42.744097 [error] [MainThread]:   Runtime Error in model rpt_experiment_results_by_channel (models/marts/experiments/rpt_experiment_results_by_channel.sql)
  Binder Error: No function matches the given name and argument types 'abs(BOOLEAN)'. You might need to add explicit type casts.
  	Candidate functions:
  	abs(TINYINT) -> TINYINT
  	abs(SMALLINT) -> SMALLINT
  	abs(INTEGER) -> INTEGER
  	abs(BIGINT) -> BIGINT
  	abs(HUGEINT) -> HUGEINT
  	abs(FLOAT) -> FLOAT
  	abs(DOUBLE) -> DOUBLE
  	abs(DECIMAL) -> DECIMAL
  	abs(UTINYINT) -> UTINYINT
  	abs(USMALLINT) -> USMALLINT
  	abs(UINTEGER) -> UINTEGER
  	abs(UBIGINT) -> UBIGINT
  	abs(UHUGEINT) -> UHUGEINT
  
  
  LINE 136:             when abs(
                             ^
[0m15:06:42.744240 [info ] [MainThread]: 
[0m15:06:42.744387 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:06:42.744518 [info ] [MainThread]: 
[0m15:06:42.744656 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m15:06:42.746063 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9248138, "process_in_blocks": "0", "process_kernel_time": 0.219528, "process_mem_max_rss": "191545344", "process_out_blocks": "0", "process_user_time": 1.368118}
[0m15:06:42.746275 [debug] [MainThread]: Command `dbt run` failed at 15:06:42.746236 after 0.93 seconds
[0m15:06:42.746446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d6ea20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f655e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b5f950>]}
[0m15:06:42.746604 [debug] [MainThread]: Flushing usage events
[0m15:06:42.862855 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:14.481433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102026630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a699d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a69760>]}


============================== 15:07:14.494850 | 4acb4e33-df64-4d6d-9366-61a8597187a7 ==============================
[0m15:07:14.494850 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:07:14.495252 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/experiments', 'version_check': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'log_format': 'default', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'introspect': 'True', 'printer_width': '80', 'use_colors': 'True', 'target_path': 'None', 'debug': 'False', 'no_print': 'None', 'write_json': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt'}
[0m15:07:14.662898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041f5880>]}
[0m15:07:14.691639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102875d00>]}
[0m15:07:14.693065 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:14.746332 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:07:14.815028 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:07:14.815459 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/experiments/rpt_experiment_results_by_channel.sql
[0m15:07:15.007936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c94470>]}
[0m15:07:15.082952 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:15.084364 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:15.098528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aea360>]}
[0m15:07:15.098828 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:07:15.099003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c65670>]}
[0m15:07:15.100678 [info ] [MainThread]: 
[0m15:07:15.100861 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:07:15.101003 [info ] [MainThread]: 
[0m15:07:15.101236 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:07:15.103419 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m15:07:15.132269 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m15:07:15.132512 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m15:07:15.132678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:07:15.151311 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:07:15.152275 [debug] [ThreadPool]: On list_analytics: Close
[0m15:07:15.152633 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m15:07:15.152864 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m15:07:15.156155 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.156326 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m15:07:15.156457 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:15.157277 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:07:15.157984 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.158140 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m15:07:15.158378 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.158507 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.158632 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m15:07:15.159056 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.159485 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:07:15.159626 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m15:07:15.159749 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m15:07:15.159956 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.160088 [debug] [ThreadPool]: On create_analytics_main: Close
[0m15:07:15.160772 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m15:07:15.163496 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:15.163654 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:07:15.163841 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:15.164091 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:07:15.164218 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:15.164347 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:07:15.174887 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m15:07:15.175906 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:07:15.176871 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:07:15.177040 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:07:15.177907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aea000>]}
[0m15:07:15.178151 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.178292 [debug] [MainThread]: On master: BEGIN
[0m15:07:15.178416 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:15.178664 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.178793 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.178918 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.179036 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.179238 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.179362 [debug] [MainThread]: On master: Close
[0m15:07:15.180635 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.180871 [info ] [Thread-1 (]: 1 of 4 START sql table model main.fct_experiments_assignments .................. [RUN]
[0m15:07:15.181069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_experiments_assignments)
[0m15:07:15.181227 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.185052 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.185743 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.201496 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.202232 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.202416 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: BEGIN
[0m15:07:15.202568 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:15.202926 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.203069 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.203245 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiments_assignments__dbt_tmp"
  
    as (
      

with events as (
    select * from "analytics"."main"."stg_events"
),

users as (
    select * from "analytics"."main"."dim_users"
),

--get the first time each user was assigned to an experiment variant
--we use the earliest event with a non-null experiment_variant
first_assignment as (
    select
        user_id,
        experiment_variant,
        min(event_at) as assigned_at
    from events
    where experiment_variant is not null
        and event_at is not null
    group by user_id, experiment_variant
),

--in a case where a user was assigned to multiple variants (shouldn't happen but lets be safe),
--take the earliest assignment
deduplicated_assignments as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        row_number() over (partition by user_id order by assigned_at) as rn 
    from first_assignment
),

final as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,

        --user attributes for segmentation
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        u.user_lifecycle_stage,

        --was the user assigned on their signup day?
        a.assigned_at = u.signed_up_at as assigned_at_signup
    from deduplicated_assignments a 
    left join users u on a.user_id = u.user_id 
    where a.rn = 1
)

select * from final
    );
  
  
[0m15:07:15.223041 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m15:07:15.225904 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.226112 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:07:15.226629 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.228071 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.228255 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiments_assignments'
  
[0m15:07:15.228699 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.231993 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.232206 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments" rename to "fct_experiments_assignments__dbt_backup"
[0m15:07:15.235267 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:07:15.236753 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.236928 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */
alter table "analytics"."main"."fct_experiments_assignments__dbt_tmp" rename to "fct_experiments_assignments"
[0m15:07:15.237218 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.244787 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:07:15.245039 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.245202 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: COMMIT
[0m15:07:15.247273 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:07:15.250157 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_experiments_assignments"
[0m15:07:15.250356 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiments_assignments"} */

      drop table if exists "analytics"."main"."fct_experiments_assignments__dbt_backup" cascade
    
[0m15:07:15.251521 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.252714 [debug] [Thread-1 (]: On model.saas_analytics.fct_experiments_assignments: Close
[0m15:07:15.253805 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102112ea0>]}
[0m15:07:15.254119 [info ] [Thread-1 (]: 1 of 4 OK created sql table model main.fct_experiments_assignments ............. [[32mOK[0m in 0.07s]
[0m15:07:15.254383 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_experiments_assignments
[0m15:07:15.254732 [debug] [Thread-3 (]: Began running node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.254992 [info ] [Thread-3 (]: 2 of 4 START sql table model main.fct_experiment_conversions ................... [RUN]
[0m15:07:15.255284 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.fct_experiment_conversions'
[0m15:07:15.255460 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.257079 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.257493 [debug] [Thread-3 (]: Began executing node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.259026 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.259477 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.259649 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: BEGIN
[0m15:07:15.259799 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:07:15.260352 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.260564 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.260783 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

  
    
    

    create  table
      "analytics"."main"."fct_experiment_conversions__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

events as (
    select * from "analytics"."main"."stg_events"
),

--defining conversion events we care about
conversion_events as (
    select 
        user_id,
        event_name,
        event_at 
    from events 
    where event_name in (
        'onboarding_completed',
        'feature_a_used',
        'feature_b_used',
        'upgrade',
        'cancel'
    )
),

--join conversion to assignments
--only count conversions that happened AFTER assignment
conversions_post_assignments as (
    select
        a.user_id,
        a.experiment_variant,
        a.assigned_at,
        c.event_name as conversion_event,
        c.event_at as converted_at,
        datediff('day', a.assigned_at, c.event_at) as days_to_conversion,
        datediff('hour', a.assigned_at, c.event_at) as hours_to_conversion

    from assignments a 
    inner join conversion_events c 
        on a.user_id = c.user_id 
        and c.event_at >= a.assigned_at --conversion must happen after assignment

),

--for each user-variant-conversion_event combo, take the FIRST conversion
first_conversions as (
    select 
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion,
        row_number() over(
            partition by user_id, experiment_variant, conversion_event
            order by converted_at 
        ) as conversion_rn 
    from conversions_post_assignments
),

final as (
    select
        user_id,
        experiment_variant,
        assigned_at,
        conversion_event,
        converted_at,
        days_to_conversion,
        hours_to_conversion
    from first_conversions
    where conversion_rn = 1
)

select * from final
    );
  
  
[0m15:07:15.270230 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m15:07:15.270945 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.271129 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:07:15.271665 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.272340 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.272592 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_experiment_conversions'
  
[0m15:07:15.273026 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.274588 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.274758 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions" rename to "fct_experiment_conversions__dbt_backup"
[0m15:07:15.275057 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.276375 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.276548 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */
alter table "analytics"."main"."fct_experiment_conversions__dbt_tmp" rename to "fct_experiment_conversions"
[0m15:07:15.276838 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.277602 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:07:15.277761 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.277906 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: COMMIT
[0m15:07:15.279049 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.280200 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.fct_experiment_conversions"
[0m15:07:15.280374 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_experiment_conversions"} */

      drop table if exists "analytics"."main"."fct_experiment_conversions__dbt_backup" cascade
    
[0m15:07:15.280767 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.281357 [debug] [Thread-3 (]: On model.saas_analytics.fct_experiment_conversions: Close
[0m15:07:15.281608 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111037ec0>]}
[0m15:07:15.281883 [info ] [Thread-3 (]: 2 of 4 OK created sql table model main.fct_experiment_conversions .............. [[32mOK[0m in 0.03s]
[0m15:07:15.282129 [debug] [Thread-3 (]: Finished running node model.saas_analytics.fct_experiment_conversions
[0m15:07:15.282450 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_experiment_results
[0m15:07:15.282635 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.282880 [info ] [Thread-2 (]: 3 of 4 START sql table model main.rpt_experiment_results ....................... [RUN]
[0m15:07:15.283124 [info ] [Thread-1 (]: 4 of 4 START sql table model main.rpt_experiment_results_by_channel ............ [RUN]
[0m15:07:15.283369 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_experiment_results'
[0m15:07:15.283552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.saas_analytics.fct_experiments_assignments, now model.saas_analytics.rpt_experiment_results_by_channel)
[0m15:07:15.283718 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_experiment_results
[0m15:07:15.283874 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.285786 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.288487 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.289033 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.289209 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_experiment_results
[0m15:07:15.291210 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.292822 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.293376 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.293577 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.293741 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: BEGIN
[0m15:07:15.293904 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: BEGIN
[0m15:07:15.294055 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:07:15.294200 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:15.294566 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.294724 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.294877 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.295171 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results__dbt_tmp"
  
    as (
      

with assignments as (

    select * from "analytics"."main"."fct_experiments_assignments"

),

conversions as (

    select * from "analytics"."main"."fct_experiment_conversions"

),

-- Count total users assigned to each variant
variant_totals as (

    select
        experiment_variant,
        count(distinct user_id) as total_users

    from assignments
    group by experiment_variant

),

-- Count conversions per variant per conversion event
variant_conversions as (

    select
        experiment_variant,
        conversion_event,
        count(distinct user_id) as converted_users,
        avg(days_to_conversion) as avg_days_to_conversion,
        median(days_to_conversion) as median_days_to_conversion

    from conversions
    group by experiment_variant, conversion_event

),

-- Calculate conversion rates
conversion_rates as (

    select
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion,
        vc.median_days_to_conversion

    from variant_totals vt
    cross join (select distinct conversion_event from variant_conversions) events
    left join variant_conversions vc
        on vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event

),

-- Pivot to get control vs treatment side-by-side for each metric
results_pivoted as (

    select
        conversion_event,

        -- Variant A (assuming this is control)
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,
        max(case when experiment_variant = 'A' then avg_days_to_conversion end) as a_avg_days_to_conversion,

        -- Variant B (assuming this is treatment)
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate,
        max(case when experiment_variant = 'B' then avg_days_to_conversion end) as b_avg_days_to_conversion

    from conversion_rates
    group by conversion_event

),

-- Calculate statistical metrics
final as (

    select
        conversion_event,

        -- Variant A stats
        a_total_users,
        a_converted_users,
        a_conversion_rate,
        a_avg_days_to_conversion,

        -- Variant B stats
        b_total_users,
        b_converted_users,
        b_conversion_rate,
        b_avg_days_to_conversion,

        -- Lift calculation
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case 
            when a_conversion_rate > 0 
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null
        end as relative_lift_pct,

        -- Standard error for each variant (for confidence intervals)
        sqrt(a_conversion_rate * (1 - a_conversion_rate) / a_total_users) as a_standard_error,
        sqrt(b_conversion_rate * (1 - b_conversion_rate) / b_total_users) as b_standard_error,

        -- Pooled standard error (for z-test)
        sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as pooled_standard_error,

        -- Z-score (test statistic)
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        -- 95% confidence interval for B - A difference
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        -- Statistical significance (at 95% confidence level, z > 1.96)
        case
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                )
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        -- Which variant won
        case
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner

    from results_pivoted

)

select * from final
    );
  
  
[0m15:07:15.295516 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.296914 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp"
  
    as (
      

with assignments as (
    select * from "analytics"."main"."fct_experiments_assignments"
),

conversions as (
    select * from "analytics"."main"."fct_experiment_conversions"
),

--count total users assigned to each variant by channel
variant_totals_by_channel as (
    select 
        acquisition_channel,
        experiment_variant,
        count(distinct user_id) as total_users 
    from assignments
    where acquisition_channel is not null --exclude users with unknown channel
    group by acquisition_channel, experiment_variant 
),

--count conversions per variant per channel per conversion event
variant_conversions_by_channel as (
    select 
        a.acquisition_channel,
        c.experiment_variant,
        c.conversion_event,
        count(distinct c.user_id) as converted_users,
        avg(c.days_to_conversion) as avg_days_to_conversion 
    from conversions c 
    inner join assignments a 
        on c.user_id = a.user_id 
    where a.acquisition_channel is not null 
    group by a.acquisition_channel, c.experiment_variant, c.conversion_event 
),

--calculate conversion rates by channel
conversion_rates_by_channel as (
    select
        vt.acquisition_channel,
        vt.experiment_variant,
        vc.conversion_event,
        vt.total_users,
        coalesce(vc.converted_users, 0) as converted_users,
        coalesce(vc.converted_users, 0)::float / vt.total_users as conversion_rate,
        vc.avg_days_to_conversion
    from variant_totals_by_channel vt 
    cross join (
        select distinct conversion_event 
        from variant_conversions_by_channel
    ) events
    left join variant_conversions_by_channel vc 
        on vt.acquisition_channel = vc.acquisition_channel
        and vt.experiment_variant = vc.experiment_variant
        and events.conversion_event = vc.conversion_event 
),

--pivot to get A vs. B side-by-side for each channel-metric combo
results_pivoted as (
    select
        acquisition_channel,
        conversion_event,

        --variant A
        max(case when experiment_variant = 'A' then total_users end) as a_total_users,
        max(case when experiment_variant = 'A' then converted_users end) as a_converted_users,
        max(case when experiment_variant = 'A' then conversion_rate end) as a_conversion_rate,

        --variant B
        max(case when experiment_variant = 'B' then total_users end) as b_total_users,
        max(case when experiment_variant = 'B' then converted_users end) as b_converted_users,
        max(case when experiment_variant = 'B' then conversion_rate end) as b_conversion_rate
    from conversion_rates_by_channel 
    group by acquisition_channel, conversion_event 
),

--calculate statistical metrics
final as (
    select 
        acquisition_channel,
        conversion_event,

        --sample sizes
        a_total_users,
        b_total_users,
        a_total_users + b_total_users as total_users,

        --conversion rates
        a_conversion_rate,
        b_conversion_rate,

        --lift
        (b_conversion_rate - a_conversion_rate) as absolute_lift,
        case
            when a_conversion_rate > 0
            then ((b_conversion_rate - a_conversion_rate) / a_conversion_rate) * 100
            else null 
        end as relative_lift_pct,

        --z-score
        case
            when sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) + 
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            ) > 0 
            then (b_conversion_rate - a_conversion_rate) / sqrt(
                (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
            )
            else null
        end as z_score,

        --95% confidence interval
        (b_conversion_rate - a_conversion_rate) - 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_lower,

        (b_conversion_rate - a_conversion_rate) + 1.96 * sqrt(
            (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
            (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
        ) as ci_upper,

        --statistical significance
        case 
            when abs(
                (b_conversion_rate - a_conversion_rate) / sqrt(
                    (a_conversion_rate * (1 - a_conversion_rate) / a_total_users) +
                    (b_conversion_rate * (1 - b_conversion_rate) / b_total_users)
                ) 
            ) > 1.96
            then true
            else false
        end as is_statistically_significant,

        --winner
        case 
            when b_conversion_rate > a_conversion_rate then 'B'
            when a_conversion_rate > b_conversion_rate then 'A'
            else 'Tie'
        end as winner
    from results_pivoted
    where a_total_users >= 30 and b_total_users >= 30
)

select * from final
order by acquisition_channel, conversion_event
    );
  
  
[0m15:07:15.311283 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m15:07:15.311551 [debug] [Thread-2 (]: SQL status: OK in 0.016 seconds
[0m15:07:15.315967 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.316533 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.316731 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */
alter table "analytics"."main"."rpt_experiment_results_by_channel__dbt_tmp" rename to "rpt_experiment_results_by_channel"
[0m15:07:15.316913 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:07:15.317407 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.318123 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: COMMIT
[0m15:07:15.318289 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:15.318453 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.319053 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.319212 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: COMMIT
[0m15:07:15.319368 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_experiment_results'
  
[0m15:07:15.319877 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.321018 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results_by_channel"
[0m15:07:15.321197 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results_by_channel"} */

      drop table if exists "analytics"."main"."rpt_experiment_results_by_channel__dbt_backup" cascade
    
[0m15:07:15.321448 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:07:15.321636 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.322225 [debug] [Thread-1 (]: On model.saas_analytics.rpt_experiment_results_by_channel: Close
[0m15:07:15.322498 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11102ce30>]}
[0m15:07:15.324264 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.322797 [info ] [Thread-1 (]: 4 of 4 OK created sql table model main.rpt_experiment_results_by_channel ....... [[32mOK[0m in 0.04s]
[0m15:07:15.324486 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results" rename to "rpt_experiment_results__dbt_backup"
[0m15:07:15.324732 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_experiment_results_by_channel
[0m15:07:15.325086 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.326545 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.326992 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */
alter table "analytics"."main"."rpt_experiment_results__dbt_tmp" rename to "rpt_experiment_results"
[0m15:07:15.327293 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.328291 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:07:15.328445 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.328582 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: COMMIT
[0m15:07:15.329077 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.330294 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_experiment_results"
[0m15:07:15.330464 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_experiment_results"} */

      drop table if exists "analytics"."main"."rpt_experiment_results__dbt_backup" cascade
    
[0m15:07:15.330989 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m15:07:15.331600 [debug] [Thread-2 (]: On model.saas_analytics.rpt_experiment_results: Close
[0m15:07:15.331852 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4acb4e33-df64-4d6d-9366-61a8597187a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f77e00>]}
[0m15:07:15.332116 [info ] [Thread-2 (]: 3 of 4 OK created sql table model main.rpt_experiment_results .................. [[32mOK[0m in 0.05s]
[0m15:07:15.332351 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_experiment_results
[0m15:07:15.332917 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.333064 [debug] [MainThread]: On master: BEGIN
[0m15:07:15.333182 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:07:15.333429 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.333559 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.333681 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:15.333792 [debug] [MainThread]: On master: COMMIT
[0m15:07:15.333975 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:15.334098 [debug] [MainThread]: On master: Close
[0m15:07:15.334253 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:07:15.334369 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results_by_channel' was properly closed.
[0m15:07:15.334478 [debug] [MainThread]: Connection 'model.saas_analytics.fct_experiment_conversions' was properly closed.
[0m15:07:15.334588 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_experiment_results' was properly closed.
[0m15:07:15.334744 [info ] [MainThread]: 
[0m15:07:15.334885 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m15:07:15.335318 [debug] [MainThread]: Command end result
[0m15:07:15.355002 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:15.356091 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:15.359160 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:07:15.359328 [info ] [MainThread]: 
[0m15:07:15.359511 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:15.359653 [info ] [MainThread]: 
[0m15:07:15.359808 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m15:07:15.361125 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9408884, "process_in_blocks": "0", "process_kernel_time": 0.218551, "process_mem_max_rss": "199000064", "process_out_blocks": "0", "process_user_time": 1.357449}
[0m15:07:15.361336 [debug] [MainThread]: Command `dbt run` succeeded at 15:07:15.361298 after 0.94 seconds
[0m15:07:15.361508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d49250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a69a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d11610>]}
[0m15:07:15.361674 [debug] [MainThread]: Flushing usage events
[0m15:07:15.476653 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:33.564998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12943da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a5b59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a5b5670>]}


============================== 15:07:33.570425 | 7fb88111-30fb-4a14-983d-1d14e119028c ==============================
[0m15:07:33.570425 [info ] [MainThread]: Running with dbt=1.11.4
[0m15:07:33.570830 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'empty': 'None', 'write_json': 'True', 'version_check': 'True', 'log_format': 'default', 'debug': 'False', 'invocation_command': 'dbt test --select path:models/marts/experiments', 'introspect': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'indirect_selection': 'eager', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'fail_fast': 'False', 'quiet': 'False', 'no_print': 'None', 'target_path': 'None'}
[0m15:07:33.739940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a54dfa0>]}
[0m15:07:33.767684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6522d0>]}
[0m15:07:33.769648 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:33.820941 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m15:07:33.890978 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:07:33.891234 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m15:07:33.891375 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:07:33.913782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b141010>]}
[0m15:07:33.959968 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:33.961277 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:33.979523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b7f44a0>]}
[0m15:07:33.979797 [info ] [MainThread]: Found 9 models, 40 data tests, 3 sources, 472 macros
[0m15:07:33.979958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b72ca10>]}
[0m15:07:33.981822 [info ] [MainThread]: 
[0m15:07:33.982006 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:07:33.982146 [info ] [MainThread]: 
[0m15:07:33.982388 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:07:33.984671 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m15:07:34.016576 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:34.016805 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m15:07:34.016941 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:07:34.030365 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:07:34.030576 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m15:07:34.030731 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m15:07:34.048434 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m15:07:34.049524 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m15:07:34.050517 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m15:07:34.050685 [debug] [ThreadPool]: On list_analytics_main: Close
[0m15:07:34.051603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7fb88111-30fb-4a14-983d-1d14e119028c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b7140e0>]}
[0m15:07:34.051828 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.051956 [debug] [MainThread]: On master: BEGIN
[0m15:07:34.052075 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:34.052351 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.052483 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.052608 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.052728 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.052927 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.053051 [debug] [MainThread]: On master: Close
[0m15:07:34.054618 [debug] [Thread-1 (]: Began running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.054818 [debug] [Thread-2 (]: Began running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.055175 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.055343 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.055038 [info ] [Thread-1 (]: 1 of 14 START test accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [RUN]
[0m15:07:34.055558 [info ] [Thread-2 (]: 2 of 14 START test accepted_values_fct_experiments_assignments_experiment_variant__A__B  [RUN]
[0m15:07:34.055714 [info ] [Thread-3 (]: 3 of 14 START test not_null_fct_experiment_conversions_conversion_event ........ [RUN]
[0m15:07:34.055937 [info ] [Thread-4 (]: 4 of 14 START test not_null_fct_experiment_conversions_converted_at ............ [RUN]
[0m15:07:34.056182 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338)
[0m15:07:34.056407 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m15:07:34.056612 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m15:07:34.056809 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m15:07:34.056974 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.057133 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.057282 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.057429 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.067104 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.069801 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.073590 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.075357 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.077028 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.083616 [debug] [Thread-1 (]: Began executing node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.085490 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.085690 [debug] [Thread-2 (]: Began executing node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.087528 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.087699 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.120735 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.121978 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.122418 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.122596 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: BEGIN
[0m15:07:34.122816 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.123036 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:07:34.123247 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: BEGIN
[0m15:07:34.123438 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.123619 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.123911 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.124088 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: BEGIN
[0m15:07:34.124249 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.124396 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: BEGIN
[0m15:07:34.124608 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:07:34.124770 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"
[0m15:07:34.124925 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.125059 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:07:34.125270 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select converted_at
from "analytics"."main"."fct_experiment_conversions"
where converted_at is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.125442 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"
[0m15:07:34.125595 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.125900 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        conversion_event as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiment_conversions"
    group by conversion_event

)

select *
from all_values
where value_field not in (
    'onboarding_completed','feature_a_used','feature_b_used','upgrade','cancel'
)



  
  
      
    ) dbt_internal_test
[0m15:07:34.126082 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.126233 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"
[0m15:07:34.126466 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"
[0m15:07:34.126645 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        experiment_variant as value_field,
        count(*) as n_records

    from "analytics"."main"."fct_experiments_assignments"
    group by experiment_variant

)

select *
from all_values
where value_field not in (
    'A','B'
)



  
  
      
    ) dbt_internal_test
[0m15:07:34.126847 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."fct_experiment_conversions"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.127037 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.129413 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: ROLLBACK
[0m15:07:34.129643 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.129820 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.130232 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17'
[0m15:07:34.130387 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.131112 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: ROLLBACK
[0m15:07:34.131285 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17: Close
[0m15:07:34.131932 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: ROLLBACK
[0m15:07:34.132575 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: ROLLBACK
[0m15:07:34.132995 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065'
[0m15:07:34.133718 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338'
[0m15:07:34.133315 [info ] [Thread-4 (]: 4 of 14 PASS not_null_fct_experiment_conversions_converted_at .................. [[32mPASS[0m in 0.08s]
[0m15:07:34.134186 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103'
[0m15:07:34.134346 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065: Close
[0m15:07:34.134519 [debug] [Thread-1 (]: On test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338: Close
[0m15:07:34.134754 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17
[0m15:07:34.134908 [debug] [Thread-2 (]: On test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103: Close
[0m15:07:34.135170 [info ] [Thread-3 (]: 3 of 14 PASS not_null_fct_experiment_conversions_conversion_event .............. [[32mPASS[0m in 0.08s]
[0m15:07:34.135582 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.135377 [info ] [Thread-1 (]: 1 of 14 PASS accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel  [[32mPASS[0m in 0.08s]
[0m15:07:34.136090 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065
[0m15:07:34.135895 [info ] [Thread-2 (]: 2 of 14 PASS accepted_values_fct_experiments_assignments_experiment_variant__A__B  [[32mPASS[0m in 0.08s]
[0m15:07:34.136303 [info ] [Thread-4 (]: 5 of 14 START test not_null_fct_experiment_conversions_experiment_variant ...... [RUN]
[0m15:07:34.136538 [debug] [Thread-1 (]: Finished running node test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338
[0m15:07:34.136712 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.136943 [debug] [Thread-2 (]: Finished running node test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103
[0m15:07:34.137127 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_converted_at.aac8bf7f17, now test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b)
[0m15:07:34.137307 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.137722 [info ] [Thread-3 (]: 6 of 14 START test not_null_fct_experiment_conversions_user_id ................. [RUN]
[0m15:07:34.137950 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.138130 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.138295 [info ] [Thread-1 (]: 7 of 14 START test not_null_fct_experiments_assignments_assigned_at ............ [RUN]
[0m15:07:34.138532 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_conversion_event.a159e50065, now test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7)
[0m15:07:34.138720 [info ] [Thread-2 (]: 8 of 14 START test not_null_fct_experiments_assignments_experiment_variant ..... [RUN]
[0m15:07:34.141211 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.141530 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiment_conversions_conversion_event__onboarding_completed__feature_a_used__feature_b_used__upgrade__cancel.a005551338, now test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484)
[0m15:07:34.141887 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.142070 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.accepted_values_fct_experiments_assignments_experiment_variant__A__B.0685673103, now test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267)
[0m15:07:34.142290 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.144239 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.144453 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.146293 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.148730 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.148989 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.150323 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.150575 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.151767 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.151976 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.153122 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.153327 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.154359 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.154578 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.154749 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: BEGIN
[0m15:07:34.154946 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.155248 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.155439 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.155600 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: BEGIN
[0m15:07:34.155766 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.155923 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.156076 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: BEGIN
[0m15:07:34.156222 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:07:34.156363 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: BEGIN
[0m15:07:34.156508 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"
[0m15:07:34.156648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.156845 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.157014 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiment_conversions"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.157181 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157541 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157727 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.157887 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"
[0m15:07:34.158051 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"
[0m15:07:34.158202 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.158344 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"
[0m15:07:34.158517 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select experiment_variant
from "analytics"."main"."fct_experiments_assignments"
where experiment_variant is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.158697 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select assigned_at
from "analytics"."main"."fct_experiments_assignments"
where assigned_at is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.158907 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiment_conversions"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.159624 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: ROLLBACK
[0m15:07:34.160293 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b'
[0m15:07:34.160474 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b: Close
[0m15:07:34.160635 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.160783 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.161186 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.161046 [info ] [Thread-4 (]: 5 of 14 PASS not_null_fct_experiment_conversions_experiment_variant ............ [[32mPASS[0m in 0.02s]
[0m15:07:34.161922 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: ROLLBACK
[0m15:07:34.162709 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: ROLLBACK
[0m15:07:34.163350 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: ROLLBACK
[0m15:07:34.163611 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b
[0m15:07:34.164072 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484'
[0m15:07:34.164476 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7'
[0m15:07:34.164886 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267'
[0m15:07:34.165076 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.165262 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484: Close
[0m15:07:34.165423 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7: Close
[0m15:07:34.165575 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267: Close
[0m15:07:34.165736 [info ] [Thread-4 (]: 9 of 14 START test not_null_fct_experiments_assignments_user_id ................ [RUN]
[0m15:07:34.166042 [info ] [Thread-1 (]: 7 of 14 PASS not_null_fct_experiments_assignments_assigned_at .................. [[32mPASS[0m in 0.02s]
[0m15:07:34.166262 [info ] [Thread-3 (]: 6 of 14 PASS not_null_fct_experiment_conversions_user_id ....................... [[32mPASS[0m in 0.03s]
[0m15:07:34.166561 [info ] [Thread-2 (]: 8 of 14 PASS not_null_fct_experiments_assignments_experiment_variant ........... [[32mPASS[0m in 0.02s]
[0m15:07:34.166770 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_experiment_variant.8f901aa82b, now test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b)
[0m15:07:34.167004 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484
[0m15:07:34.167220 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7
[0m15:07:34.167428 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267
[0m15:07:34.167576 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.167741 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.167916 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.168100 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.171085 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.171284 [info ] [Thread-1 (]: 10 of 14 START test not_null_rpt_experiment_results_by_channel_acquisition_channel  [RUN]
[0m15:07:34.171485 [info ] [Thread-3 (]: 11 of 14 START test not_null_rpt_experiment_results_by_channel_conversion_event  [RUN]
[0m15:07:34.171707 [info ] [Thread-2 (]: 12 of 14 START test not_null_rpt_experiment_results_conversion_event ........... [RUN]
[0m15:07:34.171971 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_assigned_at.f059282484, now test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525)
[0m15:07:34.172139 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiment_conversions_user_id.8566f17af7, now test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7)
[0m15:07:34.172295 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_experiment_variant.a6d9c79267, now test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400)
[0m15:07:34.172449 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.172631 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.172801 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.174719 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.176514 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.178269 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.178514 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.179816 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.180163 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.181510 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.181778 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.181975 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.183097 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.184284 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.184488 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.184700 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.184903 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: BEGIN
[0m15:07:34.185079 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: BEGIN
[0m15:07:34.185273 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.185431 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.185601 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.185904 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.186134 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: BEGIN
[0m15:07:34.186305 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.186464 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: BEGIN
[0m15:07:34.186617 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.186758 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:34.186917 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"
[0m15:07:34.187064 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:07:34.187213 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"
[0m15:07:34.187424 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results_by_channel"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.187629 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.187800 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_experiments_assignments"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.187969 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.188194 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"
[0m15:07:34.188418 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"
[0m15:07:34.188613 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select acquisition_channel
from "analytics"."main"."rpt_experiment_results_by_channel"
where acquisition_channel is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.188816 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select conversion_event
from "analytics"."main"."rpt_experiment_results"
where conversion_event is null



  
  
      
    ) dbt_internal_test
[0m15:07:34.188980 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.189131 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.190249 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: ROLLBACK
[0m15:07:34.190888 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: ROLLBACK
[0m15:07:34.191394 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b'
[0m15:07:34.191576 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.191738 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m15:07:34.192173 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7'
[0m15:07:34.195710 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b: Close
[0m15:07:34.196486 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: ROLLBACK
[0m15:07:34.196665 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7: Close
[0m15:07:34.197307 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: ROLLBACK
[0m15:07:34.198081 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525'
[0m15:07:34.197663 [info ] [Thread-4 (]: 9 of 14 PASS not_null_fct_experiments_assignments_user_id ...................... [[32mPASS[0m in 0.03s]
[0m15:07:34.198806 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400'
[0m15:07:34.198977 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525: Close
[0m15:07:34.198340 [info ] [Thread-3 (]: 11 of 14 PASS not_null_rpt_experiment_results_by_channel_conversion_event ...... [[32mPASS[0m in 0.03s]
[0m15:07:34.199254 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b
[0m15:07:34.199413 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400: Close
[0m15:07:34.199856 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7
[0m15:07:34.199663 [info ] [Thread-1 (]: 10 of 14 PASS not_null_rpt_experiment_results_by_channel_acquisition_channel ... [[32mPASS[0m in 0.03s]
[0m15:07:34.200088 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.200483 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.200347 [info ] [Thread-2 (]: 12 of 14 PASS not_null_rpt_experiment_results_conversion_event ................. [[32mPASS[0m in 0.03s]
[0m15:07:34.200755 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525
[0m15:07:34.200925 [info ] [Thread-4 (]: 13 of 14 START test unique_fct_experiments_assignments_user_id ................. [RUN]
[0m15:07:34.201100 [info ] [Thread-3 (]: 14 of 14 START test unique_rpt_experiment_results_conversion_event ............. [RUN]
[0m15:07:34.201338 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400
[0m15:07:34.201551 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_experiments_assignments_user_id.f4b330633b, now test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b)
[0m15:07:34.201733 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_rpt_experiment_results_by_channel_conversion_event.c3940f8ef7, now test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c)
[0m15:07:34.201927 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.202084 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.205445 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.207310 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.207996 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.210264 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.210466 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.211583 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.211870 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.212039 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: BEGIN
[0m15:07:34.212198 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:07:34.212507 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.212693 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: BEGIN
[0m15:07:34.212860 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:07:34.213027 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.213245 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"
[0m15:07:34.213463 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    conversion_event as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_experiment_results"
where conversion_event is not null
group by conversion_event
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:07:34.213632 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m15:07:34.213865 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"
[0m15:07:34.214029 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."fct_experiments_assignments"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:07:34.216673 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.216967 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m15:07:34.217774 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: ROLLBACK
[0m15:07:34.218468 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: ROLLBACK
[0m15:07:34.218917 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c'
[0m15:07:34.219343 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b'
[0m15:07:34.219496 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c: Close
[0m15:07:34.219673 [debug] [Thread-4 (]: On test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b: Close
[0m15:07:34.220005 [info ] [Thread-3 (]: 14 of 14 PASS unique_rpt_experiment_results_conversion_event ................... [[32mPASS[0m in 0.02s]
[0m15:07:34.220479 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c
[0m15:07:34.220221 [info ] [Thread-4 (]: 13 of 14 PASS unique_fct_experiments_assignments_user_id ....................... [[32mPASS[0m in 0.02s]
[0m15:07:34.220793 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b
[0m15:07:34.221370 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.221510 [debug] [MainThread]: On master: BEGIN
[0m15:07:34.221625 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:07:34.221919 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.222046 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.222167 [debug] [MainThread]: Using duckdb connection "master"
[0m15:07:34.222278 [debug] [MainThread]: On master: COMMIT
[0m15:07:34.222470 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:07:34.222589 [debug] [MainThread]: On master: Close
[0m15:07:34.222744 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:07:34.222851 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_by_channel_acquisition_channel.b176c3a525' was properly closed.
[0m15:07:34.222954 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_experiment_results_conversion_event.f15bb18400' was properly closed.
[0m15:07:34.223058 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_experiment_results_conversion_event.c037287a9c' was properly closed.
[0m15:07:34.223157 [debug] [MainThread]: Connection 'test.saas_analytics.unique_fct_experiments_assignments_user_id.d4c225633b' was properly closed.
[0m15:07:34.223283 [info ] [MainThread]: 
[0m15:07:34.223412 [info ] [MainThread]: Finished running 14 data tests in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m15:07:34.224289 [debug] [MainThread]: Command end result
[0m15:07:34.240364 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m15:07:34.241538 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m15:07:34.244869 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m15:07:34.245042 [info ] [MainThread]: 
[0m15:07:34.245221 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:34.245364 [info ] [MainThread]: 
[0m15:07:34.245512 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=14
[0m15:07:34.246970 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.7206431, "process_in_blocks": "0", "process_kernel_time": 0.194477, "process_mem_max_rss": "151863296", "process_out_blocks": "0", "process_user_time": 1.183287}
[0m15:07:34.247225 [debug] [MainThread]: Command `dbt test` succeeded at 15:07:34.247180 after 0.72 seconds
[0m15:07:34.247407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1293f7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a470920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12b140920>]}
[0m15:07:34.247580 [debug] [MainThread]: Flushing usage events
[0m15:07:34.359190 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:04.526766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104439160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ab5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ab5730>]}


============================== 14:50:04.547659 | 8d9b1cc6-1507-44e6-ac4c-1b00b7da0e0d ==============================
[0m14:50:04.547659 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:50:04.548171 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'printer_width': '80', 'no_print': 'None', 'use_experimental_parser': 'False', 'target_path': 'None', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'partial_parse': 'True', 'use_colors': 'True', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'write_json': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select path:models/marts/retention'}
[0m14:50:04.798759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d9b1cc6-1507-44e6-ac4c-1b00b7da0e0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b34770>]}
[0m14:50:04.851318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d9b1cc6-1507-44e6-ac4c-1b00b7da0e0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b8d2b0>]}
[0m14:50:04.853193 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:50:04.933142 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:50:05.024143 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: marts/retention/retention.yml - Runtime Error
    Syntax error near line 66
    ------------------------------
    63 |         description: Total number of users in this cohort.
    64 | 
    65 |       - name: week_0_pct
    66 |         description: % of cohort active in week 0 (signup week).
    67 | 
    68 |       - name: week_1_pct
    69 |         description: % of cohort active in week 1.
    
    Raw Error:
    ------------------------------
    while scanning for the next token
    found character that cannot start any token
      in "<unicode string>", line 66, column 22
[0m14:50:05.026127 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.564153, "process_in_blocks": "0", "process_kernel_time": 0.326596, "process_mem_max_rss": "132120576", "process_out_blocks": "0", "process_user_time": 1.644043}
[0m14:50:05.026535 [debug] [MainThread]: Command `dbt run` failed at 14:50:05.026455 after 0.56 seconds
[0m14:50:05.026848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ab4fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b8e600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e7a180>]}
[0m14:50:05.027137 [debug] [MainThread]: Flushing usage events
[0m14:50:05.170518 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:43.696855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104afe3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ab5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ab57c0>]}


============================== 14:50:43.702661 | 5824d2d4-9c20-483d-a623-5c38aa1eab40 ==============================
[0m14:50:43.702661 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:50:43.703186 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_colors': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'invocation_command': 'dbt run --select path:models/marts/retention', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'partial_parse': 'True', 'fail_fast': 'False', 'introspect': 'True', 'printer_width': '80', 'target_path': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'log_format': 'default', 'no_print': 'None', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m14:50:43.931806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5824d2d4-9c20-483d-a623-5c38aa1eab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086e5d30>]}
[0m14:50:43.978300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5824d2d4-9c20-483d-a623-5c38aa1eab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bd8050>]}
[0m14:50:43.980983 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:50:44.061961 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:50:44.151298 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading saas_analytics: marts/retention/retention.yml - Runtime Error
    Syntax error near line 66
    ------------------------------
    63 |         description: Total number of users in this cohort.
    64 | 
    65 |       - name: week_0_pct
    66 |         description: % of cohort active in week 0 (signup week).
    67 | 
    68 |       - name: week_1_pct
    69 |         description: % of cohort active in week 1.
    
    Raw Error:
    ------------------------------
    while scanning for the next token
    found character that cannot start any token
      in "<unicode string>", line 66, column 22
[0m14:50:44.153357 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.51773924, "process_in_blocks": "0", "process_kernel_time": 0.238015, "process_mem_max_rss": "131661824", "process_out_blocks": "0", "process_user_time": 1.369763}
[0m14:50:44.153796 [debug] [MainThread]: Command `dbt run` failed at 14:50:44.153716 after 0.52 seconds
[0m14:50:44.154121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ab5850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7e180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081706b0>]}
[0m14:50:44.154410 [debug] [MainThread]: Flushing usage events
[0m14:50:44.283768 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:52:43.279756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106048650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121b5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121b55b0>]}


============================== 14:52:43.285514 | baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a ==============================
[0m14:52:43.285514 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:52:43.285986 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'invocation_command': 'dbt run --select path:models/marts/retention', 'debug': 'False', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'version_check': 'True', 'printer_width': '80', 'no_print': 'None', 'introspect': 'True', 'empty': 'False', 'use_experimental_parser': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'partial_parse': 'True', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'write_json': 'True'}
[0m14:52:43.512942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c4b0e0>]}
[0m14:52:43.558425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bb2e10>]}
[0m14:52:43.560926 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:52:43.641801 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:52:43.752842 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m14:52:43.753406 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/retention/rpt_retention_curves.sql
[0m14:52:43.753676 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/retention/fct_user_activity_by_date.sql
[0m14:52:43.754026 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/retention/retention.yml
[0m14:52:43.754256 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/retention/rpt_retention_cohorts.sql
[0m14:52:44.123533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d1cef0>]}
[0m14:52:44.196012 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:52:44.198485 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:52:44.218672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d02c60>]}
[0m14:52:44.219080 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:52:44.219351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105582210>]}
[0m14:52:44.222195 [info ] [MainThread]: 
[0m14:52:44.222489 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:52:44.222705 [info ] [MainThread]: 
[0m14:52:44.223045 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:52:44.226580 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:52:44.282255 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:52:44.282612 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:52:44.282848 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:52:44.306275 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m14:52:44.307637 [debug] [ThreadPool]: On list_analytics: Close
[0m14:52:44.308139 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:52:44.308468 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:52:44.313674 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:52:44.313964 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:52:44.314194 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:52:44.315315 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:52:44.316296 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:52:44.316522 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:52:44.316874 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:52:44.317065 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:52:44.317250 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:52:44.317612 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:52:44.318179 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:52:44.318388 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:52:44.318574 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:52:44.318928 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:52:44.319220 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:52:44.320433 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:52:44.324654 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:52:44.324908 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:52:44.325197 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:52:44.325588 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:52:44.325789 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:52:44.325990 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:52:44.340994 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:52:44.342201 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:52:44.343507 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:52:44.345482 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:52:44.346925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c4c6b0>]}
[0m14:52:44.347289 [debug] [MainThread]: Using duckdb connection "master"
[0m14:52:44.347506 [debug] [MainThread]: On master: BEGIN
[0m14:52:44.347696 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:52:44.348102 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:52:44.348544 [debug] [MainThread]: On master: COMMIT
[0m14:52:44.348736 [debug] [MainThread]: Using duckdb connection "master"
[0m14:52:44.348969 [debug] [MainThread]: On master: COMMIT
[0m14:52:44.349453 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:52:44.349647 [debug] [MainThread]: On master: Close
[0m14:52:44.351671 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_user_activity_by_date
[0m14:52:44.352133 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_user_activity_by_date .................... [RUN]
[0m14:52:44.352510 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_user_activity_by_date)
[0m14:52:44.353098 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_user_activity_by_date
[0m14:52:44.359952 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:52:44.361039 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_user_activity_by_date
[0m14:52:44.385029 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:52:44.385916 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:52:44.386204 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: BEGIN
[0m14:52:44.386450 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:52:44.386974 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:52:44.387211 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:52:44.387498 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_date a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:52:44.392790 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_date a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:52:44.393172 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:52:44.393529 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: ROLLBACK
[0m14:52:44.398964 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_user_activity_by_date'
[0m14:52:44.399251 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: Close
[0m14:52:44.401103 [debug] [Thread-1 (]: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Catalog Error: Table with name user_activity_date does not exist!
  Did you mean "pg_attribute"?
  
  LINE 55:     from user_activity_date a 
                    ^
[0m14:52:44.402492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'baa8e8f3-249b-4c82-93c9-d9e5a08a8e6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112dd9c10>]}
[0m14:52:44.402980 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_user_activity_by_date ........... [[31mERROR[0m in 0.05s]
[0m14:52:44.403401 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_user_activity_by_date
[0m14:52:44.403743 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_user_activity_by_date' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Catalog Error: Table with name user_activity_date does not exist!
  Did you mean "pg_attribute"?
  
  LINE 55:     from user_activity_date a 
                    ^.
[0m14:52:44.404853 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_retention_cohorts
[0m14:52:44.405133 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_retention_curves
[0m14:52:44.405424 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.rpt_retention_cohorts ................................ [[33mSKIP[0m]
[0m14:52:44.405821 [info ] [Thread-4 (]: 3 of 3 SKIP relation main.rpt_retention_curves ................................. [[33mSKIP[0m]
[0m14:52:44.406177 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_retention_cohorts
[0m14:52:44.406460 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_retention_curves
[0m14:52:44.407266 [debug] [MainThread]: Using duckdb connection "master"
[0m14:52:44.407502 [debug] [MainThread]: On master: BEGIN
[0m14:52:44.407698 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:52:44.408150 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:52:44.408353 [debug] [MainThread]: On master: COMMIT
[0m14:52:44.408550 [debug] [MainThread]: Using duckdb connection "master"
[0m14:52:44.408739 [debug] [MainThread]: On master: COMMIT
[0m14:52:44.409168 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:52:44.409382 [debug] [MainThread]: On master: Close
[0m14:52:44.409654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:52:44.409845 [debug] [MainThread]: Connection 'model.saas_analytics.fct_user_activity_by_date' was properly closed.
[0m14:52:44.410081 [info ] [MainThread]: 
[0m14:52:44.410307 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m14:52:44.410798 [debug] [MainThread]: Command end result
[0m14:52:44.443917 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:52:44.445487 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:52:44.450188 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:52:44.450461 [info ] [MainThread]: 
[0m14:52:44.450729 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:52:44.450944 [info ] [MainThread]: 
[0m14:52:44.451222 [error] [MainThread]: [31mFailure in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)[0m
[0m14:52:44.451492 [error] [MainThread]:   Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Catalog Error: Table with name user_activity_date does not exist!
  Did you mean "pg_attribute"?
  
  LINE 55:     from user_activity_date a 
                    ^
[0m14:52:44.451694 [info ] [MainThread]: 
[0m14:52:44.451926 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/retention/fct_user_activity_by_date.sql
[0m14:52:44.452118 [info ] [MainThread]: 
[0m14:52:44.452331 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:52:44.454153 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2298179, "process_in_blocks": "0", "process_kernel_time": 0.291838, "process_mem_max_rss": "151306240", "process_out_blocks": "0", "process_user_time": 1.978011}
[0m14:52:44.454543 [debug] [MainThread]: Command `dbt run` failed at 14:52:44.454471 after 1.23 seconds
[0m14:52:44.454836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107128680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121b5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121b56d0>]}
[0m14:52:44.455116 [debug] [MainThread]: Flushing usage events
[0m14:52:44.592780 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:54:03.444848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e8fc80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b5790>]}


============================== 14:54:03.467146 | 511d6dae-f453-4b26-b2c3-90ea21a35fe1 ==============================
[0m14:54:03.467146 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:54:03.467807 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'log_format': 'default', 'version_check': 'True', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'empty': 'False', 'write_json': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'partial_parse': 'True', 'no_print': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'use_experimental_parser': 'False', 'target_path': 'None', 'printer_width': '80', 'invocation_command': 'dbt run --select path:models/marts/retention', 'debug': 'False'}
[0m14:54:03.721717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1124a5970>]}
[0m14:54:03.772249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11204e3f0>]}
[0m14:54:03.776369 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:54:03.872640 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:54:04.000399 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:54:04.001081 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/retention/fct_user_activity_by_date.sql
[0m14:54:04.351673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e34fb0>]}
[0m14:54:04.466754 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:54:04.468656 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:54:04.488096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d10800>]}
[0m14:54:04.488514 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:54:04.488782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112d214f0>]}
[0m14:54:04.491593 [info ] [MainThread]: 
[0m14:54:04.491883 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:54:04.492099 [info ] [MainThread]: 
[0m14:54:04.492436 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:54:04.496097 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:54:04.538809 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:54:04.539160 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:54:04.539532 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:04.565672 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m14:54:04.566999 [debug] [ThreadPool]: On list_analytics: Close
[0m14:54:04.567500 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:54:04.567825 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:54:04.572935 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:04.573208 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:54:04.573421 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:54:04.574343 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:54:04.575465 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:04.575673 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:54:04.575999 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:04.576182 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:04.576364 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:54:04.576721 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:04.577293 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:54:04.577498 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:04.577679 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:54:04.577982 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:04.578170 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:54:04.579275 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:54:04.583464 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:54:04.583741 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:54:04.584039 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:54:04.584444 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:04.584642 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:54:04.584842 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:54:04.600413 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:54:04.601746 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:54:04.603412 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:54:04.603658 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:54:04.605308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ed7710>]}
[0m14:54:04.605661 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:04.605869 [debug] [MainThread]: On master: BEGIN
[0m14:54:04.606063 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:54:04.606501 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:04.606703 [debug] [MainThread]: On master: COMMIT
[0m14:54:04.606959 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:04.607144 [debug] [MainThread]: On master: COMMIT
[0m14:54:04.607493 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:04.607762 [debug] [MainThread]: On master: Close
[0m14:54:04.609722 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_user_activity_by_date
[0m14:54:04.610128 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_user_activity_by_date .................... [RUN]
[0m14:54:04.610448 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_user_activity_by_date)
[0m14:54:04.610695 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_user_activity_by_date
[0m14:54:04.618092 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:04.619221 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_user_activity_by_date
[0m14:54:04.645676 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:04.646497 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:04.647211 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: BEGIN
[0m14:54:04.648431 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:04.649138 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:54:04.649417 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:04.649726 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:54:04.655735 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:54:04.656157 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:54:04.656520 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: ROLLBACK
[0m14:54:04.662408 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_user_activity_by_date'
[0m14:54:04.662752 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: Close
[0m14:54:04.665007 [debug] [Thread-1 (]: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 34:         a.activity_date,
                   ^
[0m14:54:04.666538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '511d6dae-f453-4b26-b2c3-90ea21a35fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1130df6e0>]}
[0m14:54:04.667039 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_user_activity_by_date ........... [[31mERROR[0m in 0.05s]
[0m14:54:04.667460 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_user_activity_by_date
[0m14:54:04.667843 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_user_activity_by_date' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 34:         a.activity_date,
                   ^.
[0m14:54:04.668959 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_retention_cohorts
[0m14:54:04.669275 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_retention_curves
[0m14:54:04.669604 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.rpt_retention_cohorts ................................ [[33mSKIP[0m]
[0m14:54:04.669972 [info ] [Thread-4 (]: 3 of 3 SKIP relation main.rpt_retention_curves ................................. [[33mSKIP[0m]
[0m14:54:04.670283 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_retention_cohorts
[0m14:54:04.670550 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_retention_curves
[0m14:54:04.671390 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:04.671603 [debug] [MainThread]: On master: BEGIN
[0m14:54:04.671790 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:54:04.672298 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:04.672567 [debug] [MainThread]: On master: COMMIT
[0m14:54:04.672781 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:04.672970 [debug] [MainThread]: On master: COMMIT
[0m14:54:04.673367 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:04.673594 [debug] [MainThread]: On master: Close
[0m14:54:04.673862 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:54:04.674048 [debug] [MainThread]: Connection 'model.saas_analytics.fct_user_activity_by_date' was properly closed.
[0m14:54:04.674356 [info ] [MainThread]: 
[0m14:54:04.674608 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m14:54:04.675111 [debug] [MainThread]: Command end result
[0m14:54:04.709928 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:54:04.711544 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:54:04.715824 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:54:04.716100 [info ] [MainThread]: 
[0m14:54:04.716379 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:54:04.716602 [info ] [MainThread]: 
[0m14:54:04.716873 [error] [MainThread]: [31mFailure in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)[0m
[0m14:54:04.717136 [error] [MainThread]:   Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 34:         a.activity_date,
                   ^
[0m14:54:04.717331 [info ] [MainThread]: 
[0m14:54:04.717560 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/retention/fct_user_activity_by_date.sql
[0m14:54:04.717755 [info ] [MainThread]: 
[0m14:54:04.717972 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:54:04.719808 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3363935, "process_in_blocks": "0", "process_kernel_time": 0.309939, "process_mem_max_rss": "148733952", "process_out_blocks": "0", "process_user_time": 2.068536}
[0m14:54:04.720185 [debug] [MainThread]: Command `dbt run` failed at 14:54:04.720117 after 1.34 seconds
[0m14:54:04.720468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110795610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060e3dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131571a0>]}
[0m14:54:04.720742 [debug] [MainThread]: Flushing usage events
[0m14:54:04.868107 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:54:47.584113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123af6d20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123fb5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123fb5700>]}


============================== 14:54:47.589832 | 3148a0fd-0429-452b-8f1a-bbd0679352ff ==============================
[0m14:54:47.589832 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:54:47.590324 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'partial_parse': 'True', 'debug': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_colors': 'True', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'printer_width': '80', 'version_check': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select path:models/marts/retention', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m14:54:47.827161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070ebe30>]}
[0m14:54:47.873697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124252ed0>]}
[0m14:54:47.875702 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:54:47.956332 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:54:48.066239 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:54:48.066941 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/retention/fct_user_activity_by_date.sql
[0m14:54:48.382591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138206b10>]}
[0m14:54:48.495550 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:54:48.497462 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:54:48.516605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138011af0>]}
[0m14:54:48.517031 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:54:48.517299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1380226f0>]}
[0m14:54:48.520065 [info ] [MainThread]: 
[0m14:54:48.520346 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:54:48.520547 [info ] [MainThread]: 
[0m14:54:48.520876 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:54:48.524785 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:54:48.568946 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:54:48.569309 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:54:48.569552 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:48.596188 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m14:54:48.597557 [debug] [ThreadPool]: On list_analytics: Close
[0m14:54:48.598098 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:54:48.598430 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:54:48.603559 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:48.603828 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:54:48.604038 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:54:48.605096 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:54:48.606051 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:48.606275 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:54:48.606616 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:48.606801 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:48.606986 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:54:48.607567 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:48.608135 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:54:48.608344 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:54:48.608529 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:54:48.608835 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:48.609023 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:54:48.610155 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:54:48.614293 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:54:48.614546 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:54:48.614827 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:54:48.615195 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:54:48.615383 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:54:48.615579 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:54:48.630703 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:54:48.631909 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:54:48.633183 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:54:48.633428 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:54:48.635093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12398fcb0>]}
[0m14:54:48.635434 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:48.635839 [debug] [MainThread]: On master: BEGIN
[0m14:54:48.636070 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:54:48.636534 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:48.636749 [debug] [MainThread]: On master: COMMIT
[0m14:54:48.636945 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:48.637130 [debug] [MainThread]: On master: COMMIT
[0m14:54:48.637478 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:48.637723 [debug] [MainThread]: On master: Close
[0m14:54:48.639573 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_user_activity_by_date
[0m14:54:48.639948 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_user_activity_by_date .................... [RUN]
[0m14:54:48.640268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_user_activity_by_date)
[0m14:54:48.640631 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_user_activity_by_date
[0m14:54:48.647787 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:48.648554 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_user_activity_by_date
[0m14:54:48.673688 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:48.674473 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:48.674760 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: BEGIN
[0m14:54:48.674999 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:48.675509 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:54:48.675742 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:54:48.676030 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.user_activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:54:48.681001 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.user_activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:54:48.681443 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:54:48.681815 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: ROLLBACK
[0m14:54:48.687289 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_user_activity_by_date'
[0m14:54:48.687613 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: Close
[0m14:54:48.689507 [debug] [Thread-1 (]: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 48:         datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,
                                                   ^
[0m14:54:48.690870 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3148a0fd-0429-452b-8f1a-bbd0679352ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12272f0b0>]}
[0m14:54:48.691360 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_user_activity_by_date ........... [[31mERROR[0m in 0.05s]
[0m14:54:48.691789 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_user_activity_by_date
[0m14:54:48.692132 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_user_activity_by_date' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 48:         datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,
                                                   ^.
[0m14:54:48.693095 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_retention_cohorts
[0m14:54:48.693381 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_retention_curves
[0m14:54:48.693652 [info ] [Thread-3 (]: 2 of 3 SKIP relation main.rpt_retention_cohorts ................................ [[33mSKIP[0m]
[0m14:54:48.694018 [info ] [Thread-4 (]: 3 of 3 SKIP relation main.rpt_retention_curves ................................. [[33mSKIP[0m]
[0m14:54:48.694371 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_retention_cohorts
[0m14:54:48.694647 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_retention_curves
[0m14:54:48.695457 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:48.695670 [debug] [MainThread]: On master: BEGIN
[0m14:54:48.695855 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:54:48.696320 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:48.696520 [debug] [MainThread]: On master: COMMIT
[0m14:54:48.696706 [debug] [MainThread]: Using duckdb connection "master"
[0m14:54:48.696885 [debug] [MainThread]: On master: COMMIT
[0m14:54:48.697228 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:54:48.697468 [debug] [MainThread]: On master: Close
[0m14:54:48.697744 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:54:48.697954 [debug] [MainThread]: Connection 'model.saas_analytics.fct_user_activity_by_date' was properly closed.
[0m14:54:48.698454 [info ] [MainThread]: 
[0m14:54:48.699098 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m14:54:48.699604 [debug] [MainThread]: Command end result
[0m14:54:48.729847 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:54:48.735144 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:54:48.739376 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:54:48.739636 [info ] [MainThread]: 
[0m14:54:48.739904 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:54:48.740125 [info ] [MainThread]: 
[0m14:54:48.740400 [error] [MainThread]: [31mFailure in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)[0m
[0m14:54:48.740669 [error] [MainThread]:   Runtime Error in model fct_user_activity_by_date (models/marts/retention/fct_user_activity_by_date.sql)
  Binder Error: Values list "a" does not have a column named "activity_date"
  
  LINE 48:         datediff('day', u.signed_up_at, a.activity_date) as days_since_signup,
                                                   ^
[0m14:54:48.740875 [info ] [MainThread]: 
[0m14:54:48.741109 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/retention/fct_user_activity_by_date.sql
[0m14:54:48.741309 [info ] [MainThread]: 
[0m14:54:48.741525 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=2 NO-OP=0 TOTAL=3
[0m14:54:48.743303 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2160723, "process_in_blocks": "0", "process_kernel_time": 0.287133, "process_mem_max_rss": "151666688", "process_out_blocks": "0", "process_user_time": 2.034812}
[0m14:54:48.743696 [debug] [MainThread]: Command `dbt run` failed at 14:54:48.743626 after 1.22 seconds
[0m14:54:48.743991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123fb57c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056bbad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122795280>]}
[0m14:54:48.744277 [debug] [MainThread]: Flushing usage events
[0m14:54:48.880833 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:13.485330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077ffb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107865940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107865700>]}


============================== 14:55:13.495596 | 83534d34-733b-47f6-b732-5487f03012ab ==============================
[0m14:55:13.495596 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:55:13.496147 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'cache_selected_only': 'False', 'partial_parse': 'True', 'log_format': 'default', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'empty': 'False', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select path:models/marts/retention', 'use_experimental_parser': 'False'}
[0m14:55:13.734304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d3fb0>]}
[0m14:55:13.782165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107969f10>]}
[0m14:55:13.784113 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:55:13.867582 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:55:13.978994 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:55:13.979651 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/retention/fct_user_activity_by_date.sql
[0m14:55:14.297464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081b6c00>]}
[0m14:55:14.412582 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:14.414536 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:14.433894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080b6360>]}
[0m14:55:14.434313 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:55:14.434584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080943b0>]}
[0m14:55:14.437463 [info ] [MainThread]: 
[0m14:55:14.437757 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:55:14.437968 [info ] [MainThread]: 
[0m14:55:14.438306 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:55:14.441925 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:55:14.483731 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:55:14.484098 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:55:14.484340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:14.509110 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m14:55:14.510465 [debug] [ThreadPool]: On list_analytics: Close
[0m14:55:14.511020 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:55:14.511370 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:55:14.516586 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:14.516885 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:55:14.517107 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:55:14.518176 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:55:14.519186 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:14.519419 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:55:14.519769 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:14.519958 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:14.520151 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:55:14.520631 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:14.521210 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:55:14.521425 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:14.521609 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:55:14.521938 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:14.522137 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:55:14.523285 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:55:14.527439 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:14.527696 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:55:14.527985 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:55:14.528410 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:14.528678 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:14.528912 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:55:14.544649 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:55:14.545814 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:55:14.547070 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:55:14.547302 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:55:14.548751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081e63f0>]}
[0m14:55:14.549133 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:14.549359 [debug] [MainThread]: On master: BEGIN
[0m14:55:14.549557 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:14.549974 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:14.550175 [debug] [MainThread]: On master: COMMIT
[0m14:55:14.550362 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:14.550541 [debug] [MainThread]: On master: COMMIT
[0m14:55:14.550847 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:14.551036 [debug] [MainThread]: On master: Close
[0m14:55:14.552996 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_user_activity_by_date
[0m14:55:14.553379 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_user_activity_by_date .................... [RUN]
[0m14:55:14.553694 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_user_activity_by_date)
[0m14:55:14.553940 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_user_activity_by_date
[0m14:55:14.561075 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.561937 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_user_activity_by_date
[0m14:55:14.586894 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.587767 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.588078 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: BEGIN
[0m14:55:14.588331 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:14.588871 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:14.589111 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.589627 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.user_activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.user_activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.user_activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.user_activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:55:14.610260 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m14:55:14.615513 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.615859 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */
alter table "analytics"."main"."fct_user_activity_by_date__dbt_tmp" rename to "fct_user_activity_by_date"
[0m14:55:14.616837 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:14.626638 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: COMMIT
[0m14:55:14.626952 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.627206 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: COMMIT
[0m14:55:14.630017 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:55:14.634275 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:14.634602 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

      drop table if exists "analytics"."main"."fct_user_activity_by_date__dbt_backup" cascade
    
[0m14:55:14.635625 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:14.637487 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: Close
[0m14:55:14.639099 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10850cef0>]}
[0m14:55:14.639601 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_user_activity_by_date ............... [[32mOK[0m in 0.08s]
[0m14:55:14.640018 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_user_activity_by_date
[0m14:55:14.640559 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_retention_cohorts
[0m14:55:14.640839 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_retention_curves
[0m14:55:14.641206 [info ] [Thread-3 (]: 2 of 3 START sql table model main.rpt_retention_cohorts ........................ [RUN]
[0m14:55:14.641631 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_retention_curves ......................... [RUN]
[0m14:55:14.642045 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_retention_cohorts'
[0m14:55:14.642392 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_retention_curves'
[0m14:55:14.642669 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_retention_cohorts
[0m14:55:14.642926 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_retention_curves
[0m14:55:14.648710 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.652103 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_retention_curves"
[0m14:55:14.653927 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_retention_cohorts
[0m14:55:14.658944 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.659421 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_retention_curves
[0m14:55:14.663384 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_retention_curves"
[0m14:55:14.663844 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.664129 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: BEGIN
[0m14:55:14.664441 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:55:14.664970 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:14.665245 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: BEGIN
[0m14:55:14.665495 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:55:14.665734 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:55:14.665981 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.666423 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

  
    
    

    create  table
      "analytics"."main"."rpt_retention_cohorts__dbt_tmp"
  
    as (
      

with activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

--count cohort sizes (total users who signed up in each cohort week)
cohort_sizes as (
    select 
        cohort_week,
        count(distinct user_id) as cohort_size 
    from activity 
    where weeks_since_cohort = 0
    group by cohort_week 
),

--count how many users from each cohort were active in each subsequent week
cohort_activity as (
    select 
        cohort_week,
        weeks_since_cohort,
        count(distinct user_id) as active_users
    from activity 
    group by cohort_week, weeks_since_cohort
),

--calculate retention rates
retention_rates as (
    select
        cs.cohort_week,
        cs.cohort_size,
        ca.weeks_since_cohort,
        ca.active_users,
        ca.active_users::float / cs.cohort_size as retention_rate,
        ca.active_users::float / cs.cohort_size * 100 as retention_pct 
    from cohort_sizes cs
    inner join cohort_activity ca 
        on cs.cohort_week = ca.cohort_week
),

--pivot to wide format for classic cohort table
-- show week 0 through week 12 (adjust as needed)
pivoted as (
    select
        cohort_week,
        cohort_size,
        max(case when weeks_since_cohort = 0 then retention_pct end) as week_0_pct,
        max(case when weeks_since_cohort = 1 then retention_pct end) as week_1_pct,
        max(case when weeks_since_cohort = 2 then retention_pct end) as week_2_pct,
        max(case when weeks_since_cohort = 3 then retention_pct end) as week_3_pct,
        max(case when weeks_since_cohort = 4 then retention_pct end) as week_4_pct,
        max(case when weeks_since_cohort = 5 then retention_pct end) as week_5_pct,
        max(case when weeks_since_cohort = 6 then retention_pct end) as week_6_pct,
        max(case when weeks_since_cohort = 7 then retention_pct end) as week_7_pct,
        max(case when weeks_since_cohort = 8 then retention_pct end) as week_8_pct,
        max(case when weeks_since_cohort = 9 then retention_pct end) as week_9_pct,
        max(case when weeks_since_cohort = 10 then retention_pct end) as week_10_pct,
        max(case when weeks_since_cohort = 11 then retention_pct end) as week_11_pct,
        max(case when weeks_since_cohort = 12 then retention_pct end) as week_12_pct
    from retention_rates 
    group by cohort_week, cohort_size 

)

select * from pivoted 
order by cohort_week desc
    );
  
  
[0m14:55:14.667164 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:55:14.667447 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:14.667743 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_curves"} */

  
    
    

    create  table
      "analytics"."main"."rpt_retention_curves__dbt_tmp"
  
    as (
      

with activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

--count cohort sizes
cohort_sizes as (
    select 
        cohort_week,
        count(distinct user_id) as cohort_size 
    from activity 
    where weeks_since_cohort = 0
    group by cohort_week
),

--count active users by cohort and week
cohort_activity as (
    select 
        cohort_week,
        weeks_since_cohort,
        count(distinct user_id) as active_users 
    from activity
    group by cohort_week, weeks_since_cohort
),

--calculate retention rates
final as (
    select 
        cs.cohort_week,
        ca.weeks_since_cohort,
        cs.cohort_size,
        ca.active_users,
        ca.active_users::float / cs.cohort_size as retention_rate,
        ca.active_users::float / cs.cohort_size * 100 as retention_pct,

        --label for charting
        cs.cohort_week::varchar || ' Cohort' as cohort_label
    from cohort_sizes cs 
    inner join cohort_activity ca 
        cs.cohort_week =  ca.cohort_week
)

select * from final
order by cohort_week desc, weeks_since_cohort
    );
  
  
[0m14:55:14.672278 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_curves"} */

  
    
    

    create  table
      "analytics"."main"."rpt_retention_curves__dbt_tmp"
  
    as (
      

with activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

--count cohort sizes
cohort_sizes as (
    select 
        cohort_week,
        count(distinct user_id) as cohort_size 
    from activity 
    where weeks_since_cohort = 0
    group by cohort_week
),

--count active users by cohort and week
cohort_activity as (
    select 
        cohort_week,
        weeks_since_cohort,
        count(distinct user_id) as active_users 
    from activity
    group by cohort_week, weeks_since_cohort
),

--calculate retention rates
final as (
    select 
        cs.cohort_week,
        ca.weeks_since_cohort,
        cs.cohort_size,
        ca.active_users,
        ca.active_users::float / cs.cohort_size as retention_rate,
        ca.active_users::float / cs.cohort_size * 100 as retention_pct,

        --label for charting
        cs.cohort_week::varchar || ' Cohort' as cohort_label
    from cohort_sizes cs 
    inner join cohort_activity ca 
        cs.cohort_week =  ca.cohort_week
)

select * from final
order by cohort_week desc, weeks_since_cohort
    );
  
  
[0m14:55:14.672683 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m14:55:14.673038 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: ROLLBACK
[0m14:55:14.675996 [debug] [Thread-3 (]: SQL status: OK in 0.009 seconds
[0m14:55:14.678542 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.678842 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */
alter table "analytics"."main"."rpt_retention_cohorts__dbt_tmp" rename to "rpt_retention_cohorts"
[0m14:55:14.679481 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:55:14.680740 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: COMMIT
[0m14:55:14.681082 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.684065 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: COMMIT
[0m14:55:14.684911 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:55:14.687133 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:14.687440 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

      drop table if exists "analytics"."main"."rpt_retention_cohorts__dbt_backup" cascade
    
[0m14:55:14.688005 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:55:14.689009 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: Close
[0m14:55:14.690176 [debug] [Thread-4 (]: Failed to rollback 'model.saas_analytics.rpt_retention_curves'
[0m14:55:14.690631 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: Close
[0m14:55:14.690876 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10855a5d0>]}
[0m14:55:14.692082 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.rpt_retention_cohorts ................... [[32mOK[0m in 0.05s]
[0m14:55:14.692543 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_retention_cohorts
[0m14:55:14.693798 [debug] [Thread-4 (]: Runtime Error in model rpt_retention_curves (models/marts/retention/rpt_retention_curves.sql)
  Parser Error: syntax error at or near "cs"
  
  LINE 51:         cs.cohort_week =  ca.cohort_week
                   ^
[0m14:55:14.694122 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83534d34-733b-47f6-b732-5487f03012ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ade930>]}
[0m14:55:14.694729 [error] [Thread-4 (]: 3 of 3 ERROR creating sql table model main.rpt_retention_curves ................ [[31mERROR[0m in 0.05s]
[0m14:55:14.695133 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_retention_curves
[0m14:55:14.695479 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_retention_curves' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_retention_curves (models/marts/retention/rpt_retention_curves.sql)
  Parser Error: syntax error at or near "cs"
  
  LINE 51:         cs.cohort_week =  ca.cohort_week
                   ^.
[0m14:55:14.696852 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:14.697143 [debug] [MainThread]: On master: BEGIN
[0m14:55:14.697341 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:55:14.698484 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m14:55:14.698776 [debug] [MainThread]: On master: COMMIT
[0m14:55:14.699073 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:14.699279 [debug] [MainThread]: On master: COMMIT
[0m14:55:14.699650 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:14.699852 [debug] [MainThread]: On master: Close
[0m14:55:14.700132 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:14.700323 [debug] [MainThread]: Connection 'model.saas_analytics.fct_user_activity_by_date' was properly closed.
[0m14:55:14.700495 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_retention_cohorts' was properly closed.
[0m14:55:14.700657 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_retention_curves' was properly closed.
[0m14:55:14.700899 [info ] [MainThread]: 
[0m14:55:14.701120 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m14:55:14.701724 [debug] [MainThread]: Command end result
[0m14:55:14.729127 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:14.730720 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:14.735993 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:55:14.736292 [info ] [MainThread]: 
[0m14:55:14.736558 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:55:14.736783 [info ] [MainThread]: 
[0m14:55:14.737054 [error] [MainThread]: [31mFailure in model rpt_retention_curves (models/marts/retention/rpt_retention_curves.sql)[0m
[0m14:55:14.737306 [error] [MainThread]:   Runtime Error in model rpt_retention_curves (models/marts/retention/rpt_retention_curves.sql)
  Parser Error: syntax error at or near "cs"
  
  LINE 51:         cs.cohort_week =  ca.cohort_week
                   ^
[0m14:55:14.737499 [info ] [MainThread]: 
[0m14:55:14.737723 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/retention/rpt_retention_curves.sql
[0m14:55:14.737917 [info ] [MainThread]: 
[0m14:55:14.738133 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:55:14.739904 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3107475, "process_in_blocks": "0", "process_kernel_time": 0.293158, "process_mem_max_rss": "178225152", "process_out_blocks": "0", "process_user_time": 2.078165}
[0m14:55:14.740223 [debug] [MainThread]: Command `dbt run` failed at 14:55:14.740165 after 1.31 seconds
[0m14:55:14.740486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107865820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a6c00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f53440>]}
[0m14:55:14.740736 [debug] [MainThread]: Flushing usage events
[0m14:55:14.870993 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:40.439224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104aa76b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051659d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105165760>]}


============================== 14:55:40.445506 | 4343f942-998f-451d-b055-0b989c33f5e8 ==============================
[0m14:55:40.445506 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:55:40.446012 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'invocation_command': 'dbt run --select path:models/marts/retention', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'introspect': 'True', 'write_json': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'use_colors': 'True', 'no_print': 'None', 'warn_error': 'None', 'printer_width': '80', 'target_path': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_format': 'default', 'use_experimental_parser': 'False', 'static_parser': 'True', 'version_check': 'True', 'empty': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False'}
[0m14:55:40.676472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051cea80>]}
[0m14:55:40.724492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105294a40>]}
[0m14:55:40.726537 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:55:40.807192 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:55:40.917968 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:55:40.918658 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/retention/rpt_retention_curves.sql
[0m14:55:41.236441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105743dd0>]}
[0m14:55:41.349997 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:41.352028 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:41.370657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059cb860>]}
[0m14:55:41.371062 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:55:41.371334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059c6690>]}
[0m14:55:41.374123 [info ] [MainThread]: 
[0m14:55:41.374405 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:55:41.374619 [info ] [MainThread]: 
[0m14:55:41.374957 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:55:41.378691 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m14:55:41.419239 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m14:55:41.419601 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m14:55:41.420426 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:41.444821 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m14:55:41.446157 [debug] [ThreadPool]: On list_analytics: Close
[0m14:55:41.446662 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m14:55:41.447101 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m14:55:41.452287 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:41.452568 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m14:55:41.452789 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:55:41.453857 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m14:55:41.454795 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:41.455016 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m14:55:41.455360 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:41.455549 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:41.455736 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m14:55:41.456330 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:41.456890 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:55:41.457098 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m14:55:41.457281 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m14:55:41.457584 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:41.457774 [debug] [ThreadPool]: On create_analytics_main: Close
[0m14:55:41.458879 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m14:55:41.463015 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:41.463264 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:55:41.463549 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:55:41.463924 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m14:55:41.464120 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:41.464320 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:55:41.479404 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m14:55:41.480576 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:55:41.481857 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:55:41.482081 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:55:41.483603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ae42c0>]}
[0m14:55:41.483947 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:41.484148 [debug] [MainThread]: On master: BEGIN
[0m14:55:41.484336 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:41.484737 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:41.484935 [debug] [MainThread]: On master: COMMIT
[0m14:55:41.485124 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:41.485302 [debug] [MainThread]: On master: COMMIT
[0m14:55:41.485601 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:41.485789 [debug] [MainThread]: On master: Close
[0m14:55:41.487475 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_user_activity_by_date
[0m14:55:41.487839 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_user_activity_by_date .................... [RUN]
[0m14:55:41.488151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_user_activity_by_date)
[0m14:55:41.488399 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_user_activity_by_date
[0m14:55:41.495732 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.496470 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_user_activity_by_date
[0m14:55:41.521237 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.522033 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.522326 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: BEGIN
[0m14:55:41.522572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:41.523098 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.523336 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.523629 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

  
    
    

    create  table
      "analytics"."main"."fct_user_activity_by_date__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--getting all dates a user was active (meaning they fired an event)
user_activity_dates as (
    select distinct 
        user_id,
        event_at as user_activity_date 
    from events 
    where event_at is not null 
),

--join to user dimension to get cohort info
final as (
    select 
        a.user_id,
        a.user_activity_date,

        --cohort definition: week user signed up
        date_trunc('week', u.signed_up_at) as cohort_week,

        --cohort definition: month user signed up 
        date_trunc('month', u.signed_up_at) as cohort_month,

        --user attributes for segmentation
        u.acquisition_channel,
        u.country,
        u.signed_up_at,

        --time since signup
        datediff('day', u.signed_up_at, a.user_activity_date) as days_since_signup,

        --which week since signup (0 = signup week, 1 = 1 week, etc.)
        floor(datediff('day', date_trunc('week', u.signed_up_at), date_trunc('week', a.user_activity_date)) /7) as weeks_since_cohort,

        --which month since signup
        datediff('month', date_trunc('month', u.signed_up_at), date_trunc('month', a.user_activity_date)) as months_since_cohort
    from user_activity_dates a 
    inner join users u 
        on a.user_id = u.user_id 

)

select * from final
    );
  
  
[0m14:55:41.543515 [debug] [Thread-1 (]: SQL status: OK in 0.020 seconds
[0m14:55:41.548052 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.548387 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_user_activity_by_date'
  
[0m14:55:41.549141 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.550111 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.550375 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_user_activity_by_date'
  
[0m14:55:41.551234 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.556383 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.556694 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */
alter table "analytics"."main"."fct_user_activity_by_date" rename to "fct_user_activity_by_date__dbt_backup"
[0m14:55:41.557725 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.559841 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.560107 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */
alter table "analytics"."main"."fct_user_activity_by_date__dbt_tmp" rename to "fct_user_activity_by_date"
[0m14:55:41.560556 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.571693 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: COMMIT
[0m14:55:41.571999 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.572246 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: COMMIT
[0m14:55:41.575390 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m14:55:41.579764 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_user_activity_by_date"
[0m14:55:41.580093 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_user_activity_by_date"} */

      drop table if exists "analytics"."main"."fct_user_activity_by_date__dbt_backup" cascade
    
[0m14:55:41.581624 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.583626 [debug] [Thread-1 (]: On model.saas_analytics.fct_user_activity_by_date: Close
[0m14:55:41.587045 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043dec60>]}
[0m14:55:41.587591 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_user_activity_by_date ............... [[32mOK[0m in 0.10s]
[0m14:55:41.588035 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_user_activity_by_date
[0m14:55:41.588589 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_retention_cohorts
[0m14:55:41.588871 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_retention_curves
[0m14:55:41.589261 [info ] [Thread-3 (]: 2 of 3 START sql table model main.rpt_retention_cohorts ........................ [RUN]
[0m14:55:41.589679 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_retention_curves ......................... [RUN]
[0m14:55:41.590101 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_retention_cohorts'
[0m14:55:41.590803 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_retention_curves'
[0m14:55:41.591121 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_retention_cohorts
[0m14:55:41.591412 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_retention_curves
[0m14:55:41.594281 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.596769 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.597731 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_retention_cohorts
[0m14:55:41.598517 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_retention_curves
[0m14:55:41.602795 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.605258 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.606046 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.606369 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.606666 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: BEGIN
[0m14:55:41.606957 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: BEGIN
[0m14:55:41.607213 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:55:41.607486 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:55:41.608094 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.609116 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.609488 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

  
    
    

    create  table
      "analytics"."main"."rpt_retention_cohorts__dbt_tmp"
  
    as (
      

with activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

--count cohort sizes (total users who signed up in each cohort week)
cohort_sizes as (
    select 
        cohort_week,
        count(distinct user_id) as cohort_size 
    from activity 
    where weeks_since_cohort = 0
    group by cohort_week 
),

--count how many users from each cohort were active in each subsequent week
cohort_activity as (
    select 
        cohort_week,
        weeks_since_cohort,
        count(distinct user_id) as active_users
    from activity 
    group by cohort_week, weeks_since_cohort
),

--calculate retention rates
retention_rates as (
    select
        cs.cohort_week,
        cs.cohort_size,
        ca.weeks_since_cohort,
        ca.active_users,
        ca.active_users::float / cs.cohort_size as retention_rate,
        ca.active_users::float / cs.cohort_size * 100 as retention_pct 
    from cohort_sizes cs
    inner join cohort_activity ca 
        on cs.cohort_week = ca.cohort_week
),

--pivot to wide format for classic cohort table
-- show week 0 through week 12 (adjust as needed)
pivoted as (
    select
        cohort_week,
        cohort_size,
        max(case when weeks_since_cohort = 0 then retention_pct end) as week_0_pct,
        max(case when weeks_since_cohort = 1 then retention_pct end) as week_1_pct,
        max(case when weeks_since_cohort = 2 then retention_pct end) as week_2_pct,
        max(case when weeks_since_cohort = 3 then retention_pct end) as week_3_pct,
        max(case when weeks_since_cohort = 4 then retention_pct end) as week_4_pct,
        max(case when weeks_since_cohort = 5 then retention_pct end) as week_5_pct,
        max(case when weeks_since_cohort = 6 then retention_pct end) as week_6_pct,
        max(case when weeks_since_cohort = 7 then retention_pct end) as week_7_pct,
        max(case when weeks_since_cohort = 8 then retention_pct end) as week_8_pct,
        max(case when weeks_since_cohort = 9 then retention_pct end) as week_9_pct,
        max(case when weeks_since_cohort = 10 then retention_pct end) as week_10_pct,
        max(case when weeks_since_cohort = 11 then retention_pct end) as week_11_pct,
        max(case when weeks_since_cohort = 12 then retention_pct end) as week_12_pct
    from retention_rates 
    group by cohort_week, cohort_size 

)

select * from pivoted 
order by cohort_week desc
    );
  
  
[0m14:55:41.609846 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:55:41.610510 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.610811 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_curves"} */

  
    
    

    create  table
      "analytics"."main"."rpt_retention_curves__dbt_tmp"
  
    as (
      

with activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

--count cohort sizes
cohort_sizes as (
    select 
        cohort_week,
        count(distinct user_id) as cohort_size 
    from activity 
    where weeks_since_cohort = 0
    group by cohort_week
),

--count active users by cohort and week
cohort_activity as (
    select 
        cohort_week,
        weeks_since_cohort,
        count(distinct user_id) as active_users 
    from activity
    group by cohort_week, weeks_since_cohort
),

--calculate retention rates
final as (
    select 
        cs.cohort_week,
        ca.weeks_since_cohort,
        cs.cohort_size,
        ca.active_users,
        ca.active_users::float / cs.cohort_size as retention_rate,
        ca.active_users::float / cs.cohort_size * 100 as retention_pct,

        --label for charting
        cs.cohort_week::varchar || ' Cohort' as cohort_label
    from cohort_sizes cs 
    inner join cohort_activity ca 
        on cs.cohort_week =  ca.cohort_week
)

select * from final
order by cohort_week desc, weeks_since_cohort
    );
  
  
[0m14:55:41.629317 [debug] [Thread-4 (]: SQL status: OK in 0.018 seconds
[0m14:55:41.629706 [debug] [Thread-3 (]: SQL status: OK in 0.020 seconds
[0m14:55:41.632290 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.633127 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.633450 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_curves"} */
alter table "analytics"."main"."rpt_retention_curves__dbt_tmp" rename to "rpt_retention_curves"
[0m14:55:41.633769 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_retention_cohorts'
  
[0m14:55:41.634411 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.635647 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: COMMIT
[0m14:55:41.635914 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:55:41.636172 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.637170 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.637431 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: COMMIT
[0m14:55:41.637698 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_retention_cohorts'
  
[0m14:55:41.638416 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.640241 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_retention_curves"
[0m14:55:41.640501 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:55:41.640753 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_curves"} */

      drop table if exists "analytics"."main"."rpt_retention_curves__dbt_backup" cascade
    
[0m14:55:41.643280 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.643547 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */
alter table "analytics"."main"."rpt_retention_cohorts" rename to "rpt_retention_cohorts__dbt_backup"
[0m14:55:41.643793 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m14:55:41.644779 [debug] [Thread-4 (]: On model.saas_analytics.rpt_retention_curves: Close
[0m14:55:41.645183 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e5a510>]}
[0m14:55:41.645448 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.648252 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.645966 [info ] [Thread-4 (]: 3 of 3 OK created sql table model main.rpt_retention_curves .................... [[32mOK[0m in 0.05s]
[0m14:55:41.648608 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */
alter table "analytics"."main"."rpt_retention_cohorts__dbt_tmp" rename to "rpt_retention_cohorts"
[0m14:55:41.648989 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_retention_curves
[0m14:55:41.649538 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.650752 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: COMMIT
[0m14:55:41.651012 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.651245 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: COMMIT
[0m14:55:41.651886 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m14:55:41.653736 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_retention_cohorts"
[0m14:55:41.654043 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_retention_cohorts"} */

      drop table if exists "analytics"."main"."rpt_retention_cohorts__dbt_backup" cascade
    
[0m14:55:41.655379 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m14:55:41.656485 [debug] [Thread-3 (]: On model.saas_analytics.rpt_retention_cohorts: Close
[0m14:55:41.656909 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4343f942-998f-451d-b055-0b989c33f5e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e5c380>]}
[0m14:55:41.657371 [info ] [Thread-3 (]: 2 of 3 OK created sql table model main.rpt_retention_cohorts ................... [[32mOK[0m in 0.07s]
[0m14:55:41.657762 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_retention_cohorts
[0m14:55:41.658933 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:41.659208 [debug] [MainThread]: On master: BEGIN
[0m14:55:41.659414 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:55:41.659876 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:41.660082 [debug] [MainThread]: On master: COMMIT
[0m14:55:41.660279 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:41.660472 [debug] [MainThread]: On master: COMMIT
[0m14:55:41.660783 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:41.660982 [debug] [MainThread]: On master: Close
[0m14:55:41.661239 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:41.661434 [debug] [MainThread]: Connection 'model.saas_analytics.fct_user_activity_by_date' was properly closed.
[0m14:55:41.661606 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_retention_cohorts' was properly closed.
[0m14:55:41.661787 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_retention_curves' was properly closed.
[0m14:55:41.662033 [info ] [MainThread]: 
[0m14:55:41.662264 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m14:55:41.662874 [debug] [MainThread]: Command end result
[0m14:55:41.693553 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:41.695122 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:41.699534 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:55:41.699811 [info ] [MainThread]: 
[0m14:55:41.700094 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:55:41.700316 [info ] [MainThread]: 
[0m14:55:41.700549 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:55:41.702065 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.3169501, "process_in_blocks": "0", "process_kernel_time": 0.293168, "process_mem_max_rss": "186466304", "process_out_blocks": "0", "process_user_time": 2.121388}
[0m14:55:41.702443 [debug] [MainThread]: Command `dbt run` succeeded at 14:55:41.702372 after 1.32 seconds
[0m14:55:41.702717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105165790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104411520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d3f260>]}
[0m14:55:41.702984 [debug] [MainThread]: Flushing usage events
[0m14:55:41.825883 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:53.793710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104535910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092b5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092b5850>]}


============================== 14:55:53.800137 | 8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f ==============================
[0m14:55:53.800137 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:55:53.800640 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'introspect': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'log_format': 'default', 'target_path': 'None', 'use_colors': 'True', 'fail_fast': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'empty': 'None', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'static_parser': 'True', 'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt test --select path:models/marts/retention'}
[0m14:55:54.055622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093bc470>]}
[0m14:55:54.101645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046c7b00>]}
[0m14:55:54.104326 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:55:54.186819 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:55:54.294435 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:54.294815 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m14:55:54.295039 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:54.331223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109782de0>]}
[0m14:55:54.405963 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:54.408223 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:54.434985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6285c0>]}
[0m14:55:54.435421 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:55:54.435692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109249b80>]}
[0m14:55:54.438791 [info ] [MainThread]: 
[0m14:55:54.439064 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:55:54.439264 [info ] [MainThread]: 
[0m14:55:54.439610 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:55:54.443739 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:55:54.487562 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:54.487922 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:55:54.488152 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:54.510435 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m14:55:54.510787 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:55:54.511039 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:55:54.532550 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m14:55:54.533946 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:55:54.535284 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:55:54.535554 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:55:54.537442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a3d0c58-d20d-4fc3-9b97-80b9bd3e986f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109782c30>]}
[0m14:55:54.537798 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:54.538008 [debug] [MainThread]: On master: BEGIN
[0m14:55:54.538207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:54.538627 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:54.538832 [debug] [MainThread]: On master: COMMIT
[0m14:55:54.539022 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:54.539205 [debug] [MainThread]: On master: COMMIT
[0m14:55:54.539503 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:54.539693 [debug] [MainThread]: On master: Close
[0m14:55:54.541758 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c
[0m14:55:54.542040 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:55:54.542541 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:55:54.542336 [info ] [Thread-1 (]: 1 of 8 START test not_null_fct_user_activity_by_date_activity_date ............. [RUN]
[0m14:55:54.542866 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:55:54.543121 [info ] [Thread-2 (]: 2 of 8 START test not_null_fct_user_activity_by_date_cohort_month .............. [RUN]
[0m14:55:54.543395 [info ] [Thread-3 (]: 3 of 8 START test not_null_fct_user_activity_by_date_cohort_week ............... [RUN]
[0m14:55:54.543717 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c)
[0m14:55:54.543973 [info ] [Thread-4 (]: 4 of 8 START test not_null_fct_user_activity_by_date_user_id ................... [RUN]
[0m14:55:54.544317 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834'
[0m14:55:54.544633 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6'
[0m14:55:54.544880 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c
[0m14:55:54.545173 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049'
[0m14:55:54.545402 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:55:54.545627 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:55:54.559597 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"
[0m14:55:54.559948 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:55:54.602312 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:55:54.605384 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:55:54.608363 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:55:54.609164 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:55:54.609485 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c
[0m14:55:54.609748 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:55:54.616283 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:55:54.624439 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"
[0m14:55:54.624767 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:55:54.626472 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:55:54.628151 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:55:54.628902 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:55:54.629199 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: BEGIN
[0m14:55:54.629509 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:55:54.629849 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"
[0m14:55:54.630202 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:55:54.630614 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:55:54.630880 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c: BEGIN
[0m14:55:54.631155 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.631405 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: BEGIN
[0m14:55:54.631645 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: BEGIN
[0m14:55:54.631882 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:54.632129 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:55:54.632360 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:55:54.632587 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:55:54.632925 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_month
from "analytics"."main"."fct_user_activity_by_date"
where cohort_month is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.633256 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:55:54.633587 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:55:54.633953 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"
[0m14:55:54.634179 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.634412 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:55:54.634682 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select activity_date
from "analytics"."main"."fct_user_activity_by_date"
where activity_date is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.634958 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:55:54.635232 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_user_activity_by_date"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.635506 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.635803 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."fct_user_activity_by_date"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.639077 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: ROLLBACK
[0m14:55:54.639444 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m14:55:54.639716 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m14:55:54.640179 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select activity_date
from "analytics"."main"."fct_user_activity_by_date"
where activity_date is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.640857 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834'
[0m14:55:54.641959 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: ROLLBACK
[0m14:55:54.642851 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: ROLLBACK
[0m14:55:54.643116 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m14:55:54.643363 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: Close
[0m14:55:54.644028 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6'
[0m14:55:54.644750 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049'
[0m14:55:54.645108 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c: ROLLBACK
[0m14:55:54.645448 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: Close
[0m14:55:54.646140 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: Close
[0m14:55:54.645910 [info ] [Thread-2 (]: 2 of 8 PASS not_null_fct_user_activity_by_date_cohort_month .................... [[32mPASS[0m in 0.10s]
[0m14:55:54.647736 [info ] [Thread-3 (]: 3 of 8 PASS not_null_fct_user_activity_by_date_cohort_week ..................... [[32mPASS[0m in 0.10s]
[0m14:55:54.648222 [info ] [Thread-4 (]: 4 of 8 PASS not_null_fct_user_activity_by_date_user_id ......................... [[32mPASS[0m in 0.10s]
[0m14:55:54.652566 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:55:54.653141 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:55:54.653637 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:55:54.654053 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:55:54.654499 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:55:54.654842 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:55:54.655408 [info ] [Thread-2 (]: 5 of 8 START test not_null_rpt_retention_cohorts_cohort_week ................... [RUN]
[0m14:55:54.655835 [info ] [Thread-3 (]: 6 of 8 START test not_null_rpt_retention_curves_cohort_week .................... [RUN]
[0m14:55:54.657136 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834, now test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312)
[0m14:55:54.656211 [info ] [Thread-4 (]: 7 of 8 START test not_null_rpt_retention_curves_weeks_since_cohort ............. [RUN]
[0m14:55:54.657781 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6, now test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e)
[0m14:55:54.658244 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:55:54.658616 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049, now test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806)
[0m14:55:54.658956 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:55:54.662638 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:55:54.663038 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:55:54.666634 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:55:54.670463 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:55:54.671642 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c'
[0m14:55:54.672202 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c: Close
[0m14:55:54.672517 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:55:54.676981 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:55:54.677508 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:55:54.679339 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:55:54.679809 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:55:54.681872 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:55:54.683026 [debug] [Thread-1 (]: Runtime Error in test not_null_fct_user_activity_by_date_activity_date (models/marts/retention/retention.yml)
  Binder Error: Referenced column "activity_date" not found in FROM clause!
  Candidate bindings: "user_activity_date", "acquisition_channel", "cohort_week", "days_since_signup", "country"
  
  LINE 18: where activity_date is null
                 ^
[0m14:55:54.683486 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:55:54.683908 [error] [Thread-1 (]: 1 of 8 ERROR not_null_fct_user_activity_by_date_activity_date .................. [[31mERROR[0m in 0.14s]
[0m14:55:54.684251 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:55:54.684556 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: BEGIN
[0m14:55:54.684937 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c
[0m14:55:54.685226 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:55:54.685472 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: BEGIN
[0m14:55:54.685716 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:55:54.685994 [debug] [Thread-1 (]: Began running node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:55:54.686318 [debug] [Thread-7 (]: Marking all children of 'test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c' to be skipped because of status 'error'.  Reason: Runtime Error in test not_null_fct_user_activity_by_date_activity_date (models/marts/retention/retention.yml)
  Binder Error: Referenced column "activity_date" not found in FROM clause!
  Candidate bindings: "user_activity_date", "acquisition_channel", "cohort_week", "days_since_signup", "country"
  
  LINE 18: where activity_date is null
                 ^.
[0m14:55:54.686574 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: BEGIN
[0m14:55:54.686806 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:55:54.687229 [info ] [Thread-1 (]: 8 of 8 START test unique_rpt_retention_cohorts_cohort_week ..................... [RUN]
[0m14:55:54.688018 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.688243 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:55:54.688611 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_activity_date.f650c8d36c, now test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3)
[0m14:55:54.688930 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:55:54.689228 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.689596 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:55:54.689885 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."rpt_retention_cohorts"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.690153 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.690382 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:55:54.695335 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:55:54.695892 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:55:54.696191 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."rpt_retention_curves"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.696544 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:55:54.696835 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select weeks_since_cohort
from "analytics"."main"."rpt_retention_curves"
where weeks_since_cohort is null



  
  
      
    ) dbt_internal_test
[0m14:55:54.698508 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: ROLLBACK
[0m14:55:54.698872 [debug] [Thread-1 (]: Began executing node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:55:54.699342 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.700112 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312'
[0m14:55:54.702310 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:55:54.702769 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m14:55:54.703065 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: Close
[0m14:55:54.704172 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: ROLLBACK
[0m14:55:54.705228 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: ROLLBACK
[0m14:55:54.705696 [info ] [Thread-2 (]: 5 of 8 PASS not_null_rpt_retention_cohorts_cohort_week ......................... [[32mPASS[0m in 0.05s]
[0m14:55:54.706138 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:55:54.706793 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e'
[0m14:55:54.707488 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806'
[0m14:55:54.707854 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:55:54.708119 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: BEGIN
[0m14:55:54.708355 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: Close
[0m14:55:54.708595 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: Close
[0m14:55:54.708919 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:54.709307 [info ] [Thread-3 (]: 6 of 8 PASS not_null_rpt_retention_curves_cohort_week .......................... [[32mPASS[0m in 0.05s]
[0m14:55:54.709708 [info ] [Thread-4 (]: 7 of 8 PASS not_null_rpt_retention_curves_weeks_since_cohort ................... [[32mPASS[0m in 0.05s]
[0m14:55:54.710227 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:55:54.710559 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:55:54.710801 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:55:54.711116 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:55:54.711422 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    cohort_week as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_retention_cohorts"
where cohort_week is not null
group by cohort_week
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:55:54.715324 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m14:55:54.716603 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: ROLLBACK
[0m14:55:54.717379 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3'
[0m14:55:54.717801 [debug] [Thread-1 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: Close
[0m14:55:54.718326 [info ] [Thread-1 (]: 8 of 8 PASS unique_rpt_retention_cohorts_cohort_week ........................... [[32mPASS[0m in 0.03s]
[0m14:55:54.718732 [debug] [Thread-1 (]: Finished running node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:55:54.719623 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:54.719862 [debug] [MainThread]: On master: BEGIN
[0m14:55:54.720059 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:55:54.720512 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:54.720710 [debug] [MainThread]: On master: COMMIT
[0m14:55:54.720895 [debug] [MainThread]: Using duckdb connection "master"
[0m14:55:54.721077 [debug] [MainThread]: On master: COMMIT
[0m14:55:54.721962 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:55:54.722255 [debug] [MainThread]: On master: Close
[0m14:55:54.722517 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:54.722706 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3' was properly closed.
[0m14:55:54.722882 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312' was properly closed.
[0m14:55:54.723056 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e' was properly closed.
[0m14:55:54.723219 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806' was properly closed.
[0m14:55:54.723734 [info ] [MainThread]: 
[0m14:55:54.723961 [info ] [MainThread]: Finished running 8 data tests in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m14:55:54.725137 [debug] [MainThread]: Command end result
[0m14:55:54.753166 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:55:54.754712 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:55:54.759751 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:55:54.760009 [info ] [MainThread]: 
[0m14:55:54.760275 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:55:54.760494 [info ] [MainThread]: 
[0m14:55:54.760766 [error] [MainThread]: [31mFailure in test not_null_fct_user_activity_by_date_activity_date (models/marts/retention/retention.yml)[0m
[0m14:55:54.761025 [error] [MainThread]:   Runtime Error in test not_null_fct_user_activity_by_date_activity_date (models/marts/retention/retention.yml)
  Binder Error: Referenced column "activity_date" not found in FROM clause!
  Candidate bindings: "user_activity_date", "acquisition_channel", "cohort_week", "days_since_signup", "country"
  
  LINE 18: where activity_date is null
                 ^
[0m14:55:54.761223 [info ] [MainThread]: 
[0m14:55:54.761449 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/retention/retention.yml/not_null_fct_user_activity_by_date_activity_date.sql
[0m14:55:54.761641 [info ] [MainThread]: 
[0m14:55:54.761852 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=8
[0m14:55:54.763664 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 1.0245788, "process_in_blocks": "0", "process_kernel_time": 0.275491, "process_mem_max_rss": "148553728", "process_out_blocks": "0", "process_user_time": 1.790192}
[0m14:55:54.763970 [debug] [MainThread]: Command `dbt test` failed at 14:55:54.763914 after 1.02 seconds
[0m14:55:54.764229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a70e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a73b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104535550>]}
[0m14:55:54.764481 [debug] [MainThread]: Flushing usage events
[0m14:55:54.887125 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:56:44.486222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12595f6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271b5940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271b5700>]}


============================== 14:56:44.509324 | ccf435a0-76a3-416f-acf0-254c8b7fbe35 ==============================
[0m14:56:44.509324 [info ] [MainThread]: Running with dbt=1.11.4
[0m14:56:44.509835 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'empty': 'None', 'target_path': 'None', 'printer_width': '80', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt test --select path:models/marts/retention', 'log_format': 'default', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'quiet': 'False', 'debug': 'False', 'write_json': 'True', 'use_colors': 'True', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'static_parser': 'True', 'warn_error': 'None'}
[0m14:56:44.750240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125fe3770>]}
[0m14:56:44.796874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12733c050>]}
[0m14:56:44.799524 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m14:56:44.887967 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m14:56:45.017935 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:56:45.018773 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/retention/retention.yml
[0m14:56:45.380866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1381003b0>]}
[0m14:56:45.452320 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:56:45.454266 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:56:45.481062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127df12e0>]}
[0m14:56:45.481484 [info ] [MainThread]: Found 12 models, 48 data tests, 3 sources, 472 macros
[0m14:56:45.481752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127c5f560>]}
[0m14:56:45.484668 [info ] [MainThread]: 
[0m14:56:45.484957 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:56:45.485173 [info ] [MainThread]: 
[0m14:56:45.485506 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m14:56:45.489451 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m14:56:45.530803 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:56:45.531165 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m14:56:45.531385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:45.551588 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m14:56:45.551930 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m14:56:45.552172 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m14:56:45.574322 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m14:56:45.575904 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m14:56:45.577225 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m14:56:45.577488 [debug] [ThreadPool]: On list_analytics_main: Close
[0m14:56:45.579334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ccf435a0-76a3-416f-acf0-254c8b7fbe35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1380780b0>]}
[0m14:56:45.579680 [debug] [MainThread]: Using duckdb connection "master"
[0m14:56:45.579889 [debug] [MainThread]: On master: BEGIN
[0m14:56:45.580078 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:56:45.580508 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:56:45.580749 [debug] [MainThread]: On master: COMMIT
[0m14:56:45.580959 [debug] [MainThread]: Using duckdb connection "master"
[0m14:56:45.581151 [debug] [MainThread]: On master: COMMIT
[0m14:56:45.581489 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:56:45.581690 [debug] [MainThread]: On master: Close
[0m14:56:45.583970 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:56:45.584272 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:56:45.584772 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff
[0m14:56:45.585014 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:56:45.584561 [info ] [Thread-1 (]: 1 of 8 START test not_null_fct_user_activity_by_date_cohort_month .............. [RUN]
[0m14:56:45.585318 [info ] [Thread-2 (]: 2 of 8 START test not_null_fct_user_activity_by_date_cohort_week ............... [RUN]
[0m14:56:45.585610 [info ] [Thread-3 (]: 3 of 8 START test not_null_fct_user_activity_by_date_user_activity_date ........ [RUN]
[0m14:56:45.585879 [info ] [Thread-4 (]: 4 of 8 START test not_null_fct_user_activity_by_date_user_id ................... [RUN]
[0m14:56:45.586186 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834)
[0m14:56:45.586527 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6'
[0m14:56:45.586830 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff'
[0m14:56:45.587142 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049'
[0m14:56:45.587386 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:56:45.587623 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:56:45.587855 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff
[0m14:56:45.588083 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:56:45.600878 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:56:45.603895 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:56:45.606655 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff"
[0m14:56:45.609547 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:56:45.610606 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff
[0m14:56:45.610932 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:56:45.611194 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:56:45.625913 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:56:45.626886 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff"
[0m14:56:45.627196 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:56:45.629880 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:56:45.631889 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:56:45.632431 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff"
[0m14:56:45.632766 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:56:45.633041 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff: BEGIN
[0m14:56:45.633340 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: BEGIN
[0m14:56:45.633584 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:56:45.633932 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:56:45.634175 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:56:45.634385 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:56:45.634760 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: BEGIN
[0m14:56:45.635008 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: BEGIN
[0m14:56:45.635271 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.635585 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:45.635819 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:56:45.636049 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.636264 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff"
[0m14:56:45.636675 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"
[0m14:56:45.636935 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.637172 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_activity_date
from "analytics"."main"."fct_user_activity_by_date"
where user_activity_date is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.637412 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.637646 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."fct_user_activity_by_date"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.637901 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"
[0m14:56:45.638237 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"
[0m14:56:45.638598 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_month
from "analytics"."main"."fct_user_activity_by_date"
where cohort_month is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.638875 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_user_activity_by_date"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.639679 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.639915 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.640203 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.640416 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.643474 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: ROLLBACK
[0m14:56:45.644799 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: ROLLBACK
[0m14:56:45.645715 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff: ROLLBACK
[0m14:56:45.646496 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6'
[0m14:56:45.647518 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: ROLLBACK
[0m14:56:45.648208 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834'
[0m14:56:45.648886 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff'
[0m14:56:45.649129 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6: Close
[0m14:56:45.649775 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049'
[0m14:56:45.650002 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834: Close
[0m14:56:45.650238 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff: Close
[0m14:56:45.650496 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049: Close
[0m14:56:45.650896 [info ] [Thread-2 (]: 2 of 8 PASS not_null_fct_user_activity_by_date_cohort_week ..................... [[32mPASS[0m in 0.06s]
[0m14:56:45.651303 [info ] [Thread-1 (]: 1 of 8 PASS not_null_fct_user_activity_by_date_cohort_month .................... [[32mPASS[0m in 0.07s]
[0m14:56:45.651698 [info ] [Thread-3 (]: 3 of 8 PASS not_null_fct_user_activity_by_date_user_activity_date .............. [[32mPASS[0m in 0.06s]
[0m14:56:45.652023 [info ] [Thread-4 (]: 4 of 8 PASS not_null_fct_user_activity_by_date_user_id ......................... [[32mPASS[0m in 0.06s]
[0m14:56:45.652400 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6
[0m14:56:45.652731 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834
[0m14:56:45.653040 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff
[0m14:56:45.653337 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049
[0m14:56:45.653596 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:56:45.653893 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:56:45.654174 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:56:45.654444 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:56:45.654717 [info ] [Thread-2 (]: 5 of 8 START test not_null_rpt_retention_cohorts_cohort_week ................... [RUN]
[0m14:56:45.654982 [info ] [Thread-1 (]: 6 of 8 START test not_null_rpt_retention_curves_cohort_week .................... [RUN]
[0m14:56:45.655232 [info ] [Thread-3 (]: 7 of 8 START test not_null_rpt_retention_curves_weeks_since_cohort ............. [RUN]
[0m14:56:45.655477 [info ] [Thread-4 (]: 8 of 8 START test unique_rpt_retention_cohorts_cohort_week ..................... [RUN]
[0m14:56:45.655793 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_cohort_week.801aa128a6, now test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312)
[0m14:56:45.656087 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_cohort_month.aba5a71834, now test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e)
[0m14:56:45.656337 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_user_activity_date.5eae157bff, now test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806)
[0m14:56:45.656580 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_user_activity_by_date_user_id.69a3792049, now test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3)
[0m14:56:45.656822 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:56:45.657097 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:56:45.657336 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:56:45.657558 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:56:45.661019 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:56:45.664206 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:56:45.667315 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:56:45.671991 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:56:45.672772 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:56:45.674634 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:56:45.674918 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:56:45.676753 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:56:45.677058 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:56:45.677317 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:56:45.679173 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:56:45.681030 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:56:45.681368 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:56:45.681803 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:56:45.682109 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: BEGIN
[0m14:56:45.682371 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: BEGIN
[0m14:56:45.682660 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:56:45.682908 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:56:45.683141 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:45.683394 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:56:45.683620 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: BEGIN
[0m14:56:45.684139 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: BEGIN
[0m14:56:45.684386 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:56:45.684632 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.684865 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.685108 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:56:45.685426 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"
[0m14:56:45.685669 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"
[0m14:56:45.685926 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.686255 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."rpt_retention_cohorts"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.686534 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select cohort_week
from "analytics"."main"."rpt_retention_curves"
where cohort_week is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.686778 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.686991 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"
[0m14:56:45.687441 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"
[0m14:56:45.687718 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select weeks_since_cohort
from "analytics"."main"."rpt_retention_curves"
where weeks_since_cohort is null



  
  
      
    ) dbt_internal_test
[0m14:56:45.688005 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    cohort_week as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_retention_cohorts"
where cohort_week is not null
group by cohort_week
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m14:56:45.688256 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.688477 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m14:56:45.690019 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: ROLLBACK
[0m14:56:45.690335 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m14:56:45.691172 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e'
[0m14:56:45.692213 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: ROLLBACK
[0m14:56:45.693134 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: ROLLBACK
[0m14:56:45.693383 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e: Close
[0m14:56:45.693647 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m14:56:45.695607 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312'
[0m14:56:45.696291 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806'
[0m14:56:45.697034 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312: Close
[0m14:56:45.697968 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: ROLLBACK
[0m14:56:45.696789 [info ] [Thread-1 (]: 6 of 8 PASS not_null_rpt_retention_curves_cohort_week .......................... [[32mPASS[0m in 0.04s]
[0m14:56:45.698333 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806: Close
[0m14:56:45.698741 [info ] [Thread-2 (]: 5 of 8 PASS not_null_rpt_retention_cohorts_cohort_week ......................... [[32mPASS[0m in 0.04s]
[0m14:56:45.699433 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3'
[0m14:56:45.699787 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e
[0m14:56:45.700473 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312
[0m14:56:45.700176 [info ] [Thread-3 (]: 7 of 8 PASS not_null_rpt_retention_curves_weeks_since_cohort ................... [[32mPASS[0m in 0.04s]
[0m14:56:45.700782 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3: Close
[0m14:56:45.701225 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806
[0m14:56:45.701588 [info ] [Thread-4 (]: 8 of 8 PASS unique_rpt_retention_cohorts_cohort_week ........................... [[32mPASS[0m in 0.04s]
[0m14:56:45.702005 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3
[0m14:56:45.702875 [debug] [MainThread]: Using duckdb connection "master"
[0m14:56:45.703102 [debug] [MainThread]: On master: BEGIN
[0m14:56:45.703297 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:56:45.703991 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m14:56:45.704249 [debug] [MainThread]: On master: COMMIT
[0m14:56:45.704471 [debug] [MainThread]: Using duckdb connection "master"
[0m14:56:45.704720 [debug] [MainThread]: On master: COMMIT
[0m14:56:45.705205 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m14:56:45.705434 [debug] [MainThread]: On master: Close
[0m14:56:45.705731 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:56:45.705925 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_curves_cohort_week.65261d6a8e' was properly closed.
[0m14:56:45.706104 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_cohorts_cohort_week.f94f3a5312' was properly closed.
[0m14:56:45.706278 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_retention_curves_weeks_since_cohort.906e60e806' was properly closed.
[0m14:56:45.706444 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_retention_cohorts_cohort_week.e3e5feebb3' was properly closed.
[0m14:56:45.706670 [info ] [MainThread]: 
[0m14:56:45.706887 [info ] [MainThread]: Finished running 8 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m14:56:45.707859 [debug] [MainThread]: Command end result
[0m14:56:45.737022 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m14:56:45.738577 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m14:56:45.743754 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m14:56:45.744030 [info ] [MainThread]: 
[0m14:56:45.744310 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:56:45.744531 [info ] [MainThread]: 
[0m14:56:45.744765 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m14:56:45.746678 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.3176711, "process_in_blocks": "0", "process_kernel_time": 0.304506, "process_mem_max_rss": "151502848", "process_out_blocks": "0", "process_user_time": 2.05293}
[0m14:56:45.747066 [debug] [MainThread]: Command `dbt test` succeeded at 14:56:45.746997 after 1.32 seconds
[0m14:56:45.747435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271b5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125996e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1380570e0>]}
[0m14:56:45.747707 [debug] [MainThread]: Flushing usage events
[0m14:56:45.889274 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:29:11.309175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10663f260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e8da60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e8d7f0>]}


============================== 13:29:11.327967 | 97d24dc7-ab15-43ed-91f9-9cb169feb331 ==============================
[0m13:29:11.327967 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:29:11.328520 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'empty': 'False', 'write_json': 'True', 'version_check': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'introspect': 'True', 'printer_width': '80', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'partial_parse': 'True', 'use_colors': 'True', 'debug': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'log_cache_events': 'False'}
[0m13:29:11.742039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fb8b00>]}
[0m13:29:11.838545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d483e0>]}
[0m13:29:11.841288 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:29:12.106845 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:29:12.406119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m13:29:12.408209 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:29:12.408793 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:29:12.409256 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/conversion/conversion.yml
[0m13:29:12.896238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107708800>]}
[0m13:29:12.983364 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:29:12.985711 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:29:13.010835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076dba40>]}
[0m13:29:13.011334 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:29:13.011644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107566ea0>]}
[0m13:29:13.015341 [info ] [MainThread]: 
[0m13:29:13.015732 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:29:13.016035 [info ] [MainThread]: 
[0m13:29:13.016736 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:29:13.022625 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:29:13.083996 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:29:13.084416 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:29:13.084690 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:13.114567 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m13:29:13.116004 [debug] [ThreadPool]: On list_analytics: Close
[0m13:29:13.116644 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:29:13.117049 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:29:13.122496 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:13.122791 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:29:13.123015 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:29:13.123933 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:29:13.125071 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:13.125301 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:29:13.125664 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:13.125860 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:13.126049 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:29:13.126412 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:13.126991 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:29:13.127201 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:13.127387 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:29:13.127694 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:13.127885 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:29:13.129127 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:29:13.134224 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:29:13.134539 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:29:13.134848 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:29:13.135366 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:13.135671 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:29:13.135924 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:29:13.151052 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m13:29:13.152317 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:29:13.153524 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:29:13.153799 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:29:13.155622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107567320>]}
[0m13:29:13.155968 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:13.156178 [debug] [MainThread]: On master: BEGIN
[0m13:29:13.156369 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:29:13.156828 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:13.157059 [debug] [MainThread]: On master: COMMIT
[0m13:29:13.157265 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:13.157453 [debug] [MainThread]: On master: COMMIT
[0m13:29:13.157828 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:13.158111 [debug] [MainThread]: On master: Close
[0m13:29:13.160340 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:13.160632 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:13.161034 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:29:13.161451 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:29:13.161804 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:29:13.162172 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:29:13.162432 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:13.162681 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:13.169385 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:13.172383 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:13.173180 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:13.173461 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:13.234113 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:13.238550 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:13.239533 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:13.239897 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:13.240204 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:29:13.240476 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:29:13.240746 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:29:13.241089 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:29:13.241788 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:29:13.242044 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:29:13.242310 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:13.242567 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:13.242980 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:29:13.243515 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type 
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:29:13.248895 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type 
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:29:13.249618 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:29:13.250048 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:29:13.250334 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m13:29:13.250728 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:29:13.251059 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: ROLLBACK
[0m13:29:13.259019 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:29:13.260186 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:29:13.260480 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:29:13.260766 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:29:13.263561 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 29:         on e.user_id u.user_id 
                                ^
[0m13:29:13.264762 [debug] [Thread-2 (]: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:29:13.266801 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b413a0>]}
[0m13:29:13.267074 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97d24dc7-ab15-43ed-91f9-9cb169feb331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106107140>]}
[0m13:29:13.267552 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.rpt_upgrade_analysis ................ [[31mERROR[0m in 0.10s]
[0m13:29:13.268409 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:13.268070 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.10s]
[0m13:29:13.268846 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_upgrade_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^.
[0m13:29:13.269244 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:13.270184 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 29:         on e.user_id u.user_id 
                                ^.
[0m13:29:13.271077 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:13.271329 [debug] [MainThread]: On master: BEGIN
[0m13:29:13.271533 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:29:13.272042 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:13.272284 [debug] [MainThread]: On master: COMMIT
[0m13:29:13.272493 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:13.272681 [debug] [MainThread]: On master: COMMIT
[0m13:29:13.273048 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:13.273266 [debug] [MainThread]: On master: Close
[0m13:29:13.273658 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:29:13.273852 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:29:13.274105 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:29:13.274351 [info ] [MainThread]: 
[0m13:29:13.274612 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m13:29:13.275247 [debug] [MainThread]: Command end result
[0m13:29:13.305012 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:29:13.306593 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:29:13.310976 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:29:13.311251 [info ] [MainThread]: 
[0m13:29:13.311528 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:29:13.311751 [info ] [MainThread]: 
[0m13:29:13.312029 [error] [MainThread]: [31mFailure in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)[0m
[0m13:29:13.312291 [error] [MainThread]:   Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:29:13.312486 [info ] [MainThread]: 
[0m13:29:13.312719 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:29:13.312918 [info ] [MainThread]: 
[0m13:29:13.313156 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:29:13.313395 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 29:         on e.user_id u.user_id 
                                ^
[0m13:29:13.313582 [info ] [MainThread]: 
[0m13:29:13.313803 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:29:13.313987 [info ] [MainThread]: 
[0m13:29:13.314195 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m13:29:13.315839 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.0658875, "process_in_blocks": "0", "process_kernel_time": 0.422254, "process_mem_max_rss": "150126592", "process_out_blocks": "0", "process_user_time": 2.342553}
[0m13:29:13.316217 [debug] [MainThread]: Command `dbt run` failed at 13:29:13.316146 after 2.07 seconds
[0m13:29:13.316496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e8d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061307a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061316d0>]}
[0m13:29:13.316762 [debug] [MainThread]: Flushing usage events
[0m13:29:13.491891 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:29:43.604560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a7cbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048989b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105621670>]}


============================== 13:29:43.613384 | b51e4dc1-9ac8-41c7-bde7-47eae1552ce4 ==============================
[0m13:29:43.613384 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:29:43.613882 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'debug': 'False', 'printer_width': '80', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'use_colors': 'True', 'quiet': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'write_json': 'True', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'cache_selected_only': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'log_format': 'default', 'warn_error': 'None'}
[0m13:29:43.844620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058124b0>]}
[0m13:29:43.916792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057beea0>]}
[0m13:29:43.919751 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:29:44.010592 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:29:44.124165 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:29:44.124835 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:29:44.380536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ec2420>]}
[0m13:29:44.496268 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:29:44.498438 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:29:44.517682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e756d0>]}
[0m13:29:44.518088 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:29:44.518357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f390d0>]}
[0m13:29:44.521340 [info ] [MainThread]: 
[0m13:29:44.521618 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:29:44.521827 [info ] [MainThread]: 
[0m13:29:44.522167 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:29:44.525612 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:29:44.567758 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:29:44.568112 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:29:44.568346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:29:44.592053 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m13:29:44.593224 [debug] [ThreadPool]: On list_analytics: Close
[0m13:29:44.593736 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:29:44.594072 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:29:44.599206 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:44.599479 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:29:44.599696 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:29:44.600692 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:29:44.601667 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:44.601885 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:29:44.602219 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:44.602536 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:44.602773 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:29:44.603209 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:44.603864 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:29:44.604094 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:29:44.604296 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:29:44.604638 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:44.604844 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:29:44.606086 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:29:44.610274 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:29:44.610622 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:29:44.610828 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:29:44.611206 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:29:44.611411 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:29:44.611612 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:29:44.625197 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m13:29:44.626412 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:29:44.627712 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:29:44.627961 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:29:44.629718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f38680>]}
[0m13:29:44.630057 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:44.630263 [debug] [MainThread]: On master: BEGIN
[0m13:29:44.630453 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:29:44.630837 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:44.631033 [debug] [MainThread]: On master: COMMIT
[0m13:29:44.631221 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:44.631400 [debug] [MainThread]: On master: COMMIT
[0m13:29:44.631695 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:44.631883 [debug] [MainThread]: On master: Close
[0m13:29:44.633640 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:44.633920 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:44.634276 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:29:44.634635 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:29:44.634941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:29:44.635274 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:29:44.635519 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:44.635761 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:44.642145 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:44.649176 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:44.650075 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:44.650376 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:44.681433 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:44.685693 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:44.686661 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:44.687007 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:44.687275 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:29:44.687526 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:29:44.687756 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:29:44.687977 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:29:44.688542 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:29:44.688774 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:29:44.689009 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:29:44.689408 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:29:44.690130 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:29:44.691555 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type 
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:29:44.695267 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type 
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:29:44.695971 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:29:44.696390 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:29:44.696688 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m13:29:44.697105 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:29:44.697412 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: ROLLBACK
[0m13:29:44.704678 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:29:44.705030 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:29:44.706122 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:29:44.706448 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:29:44.709186 [debug] [Thread-2 (]: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:29:44.710243 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "used_feature_a"
  
  LINE 75:         used_feature_a as completed_event,
                   ^
[0m13:29:44.711788 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10627c740>]}
[0m13:29:44.712065 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b51e4dc1-9ac8-41c7-bde7-47eae1552ce4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106285880>]}
[0m13:29:44.712526 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.rpt_upgrade_analysis ................ [[31mERROR[0m in 0.08s]
[0m13:29:44.713023 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.08s]
[0m13:29:44.713456 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:29:44.713806 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:29:44.714564 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_upgrade_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^.
[0m13:29:44.715428 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "used_feature_a"
  
  LINE 75:         used_feature_a as completed_event,
                   ^.
[0m13:29:44.716282 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:44.716514 [debug] [MainThread]: On master: BEGIN
[0m13:29:44.716709 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:29:44.717173 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:44.717373 [debug] [MainThread]: On master: COMMIT
[0m13:29:44.717569 [debug] [MainThread]: Using duckdb connection "master"
[0m13:29:44.717753 [debug] [MainThread]: On master: COMMIT
[0m13:29:44.718102 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:29:44.718295 [debug] [MainThread]: On master: Close
[0m13:29:44.718913 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:29:44.719192 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:29:44.719424 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:29:44.719789 [info ] [MainThread]: 
[0m13:29:44.720014 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m13:29:44.720602 [debug] [MainThread]: Command end result
[0m13:29:44.752992 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:29:44.754665 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:29:44.759070 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:29:44.759330 [info ] [MainThread]: 
[0m13:29:44.759606 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:29:44.759832 [info ] [MainThread]: 
[0m13:29:44.760100 [error] [MainThread]: [31mFailure in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)[0m
[0m13:29:44.760364 [error] [MainThread]:   Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:29:44.760558 [info ] [MainThread]: 
[0m13:29:44.760784 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:29:44.760978 [info ] [MainThread]: 
[0m13:29:44.761216 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:29:44.761453 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Parser Error: syntax error at or near "used_feature_a"
  
  LINE 75:         used_feature_a as completed_event,
                   ^
[0m13:29:44.761634 [info ] [MainThread]: 
[0m13:29:44.761852 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:29:44.762033 [info ] [MainThread]: 
[0m13:29:44.762246 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m13:29:44.764009 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2154337, "process_in_blocks": "0", "process_kernel_time": 0.297985, "process_mem_max_rss": "150323200", "process_out_blocks": "0", "process_user_time": 1.985921}
[0m13:29:44.764398 [debug] [MainThread]: Command `dbt run` failed at 13:29:44.764326 after 1.22 seconds
[0m13:29:44.764689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a7cbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e74560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105621760>]}
[0m13:29:44.764962 [debug] [MainThread]: Flushing usage events
[0m13:29:44.885273 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:31:44.988474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108caafc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091d16d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091d1490>]}


============================== 13:31:45.010025 | 07517ffb-40b4-430b-81e9-6dc158b0a3e6 ==============================
[0m13:31:45.010025 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:31:45.010585 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'quiet': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'empty': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'no_print': 'None', 'partial_parse': 'True', 'fail_fast': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'version_check': 'True', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m13:31:45.260743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109238530>]}
[0m13:31:45.307854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b12b70>]}
[0m13:31:45.309832 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:31:45.395313 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:31:45.512339 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:31:45.512996 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:31:45.767231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10994b290>]}
[0m13:31:45.882434 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:31:45.884666 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:31:45.904151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a34860>]}
[0m13:31:45.904548 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:31:45.904813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a62480>]}
[0m13:31:45.907817 [info ] [MainThread]: 
[0m13:31:45.908081 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:31:45.908279 [info ] [MainThread]: 
[0m13:31:45.908606 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:31:45.912108 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:31:45.955030 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:31:45.955393 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:31:45.955775 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:31:45.986414 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m13:31:45.987606 [debug] [ThreadPool]: On list_analytics: Close
[0m13:31:45.988129 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:31:45.988469 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:31:45.993651 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:31:45.993926 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:31:45.994141 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:31:45.995260 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:31:45.996449 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:31:45.996686 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:31:45.997043 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:31:45.997243 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:31:45.997443 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:31:45.997830 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:31:45.998427 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:31:45.998637 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:31:45.998843 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:31:45.999154 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:31:45.999345 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:31:46.000533 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:31:46.004714 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:31:46.005059 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:31:46.005257 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:31:46.005631 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:31:46.005822 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:31:46.006018 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:31:46.020571 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m13:31:46.021860 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:31:46.023172 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:31:46.023434 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:31:46.025279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108471550>]}
[0m13:31:46.025694 [debug] [MainThread]: Using duckdb connection "master"
[0m13:31:46.025932 [debug] [MainThread]: On master: BEGIN
[0m13:31:46.026135 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:31:46.026564 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:31:46.026770 [debug] [MainThread]: On master: COMMIT
[0m13:31:46.026961 [debug] [MainThread]: Using duckdb connection "master"
[0m13:31:46.027141 [debug] [MainThread]: On master: COMMIT
[0m13:31:46.027444 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:31:46.027633 [debug] [MainThread]: On master: Close
[0m13:31:46.029653 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:31:46.029959 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:31:46.030374 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:31:46.030784 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:31:46.031131 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:31:46.031491 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:31:46.031756 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:31:46.032010 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:31:46.038372 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:31:46.042405 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:31:46.044024 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:31:46.044307 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:31:46.075584 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:31:46.075951 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:31:46.076941 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:31:46.077257 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:31:46.077522 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:31:46.077774 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:31:46.078012 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:31:46.078234 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:31:46.078830 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:31:46.079125 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:31:46.079367 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:31:46.079612 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:31:46.079985 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:31:46.080488 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:31:46.085251 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at. e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:31:46.085932 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:31:46.086343 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:31:46.086628 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m13:31:46.086982 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:31:46.087309 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: ROLLBACK
[0m13:31:46.094786 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:31:46.095929 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:31:46.096223 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:31:46.096505 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:31:46.099242 [debug] [Thread-2 (]: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:31:46.100298 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: Cannot extract field 'e' from expression "signed_up_at" because it is not a struct, union, map, or json
[0m13:31:46.101819 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a56540>]}
[0m13:31:46.102068 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07517ffb-40b4-430b-81e9-6dc158b0a3e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e25a00>]}
[0m13:31:46.102549 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.rpt_upgrade_analysis ................ [[31mERROR[0m in 0.07s]
[0m13:31:46.102977 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.07s]
[0m13:31:46.103424 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:31:46.103773 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:31:46.104105 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_upgrade_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^.
[0m13:31:46.104912 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: Cannot extract field 'e' from expression "signed_up_at" because it is not a struct, union, map, or json.
[0m13:31:46.105744 [debug] [MainThread]: Using duckdb connection "master"
[0m13:31:46.105965 [debug] [MainThread]: On master: BEGIN
[0m13:31:46.106154 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:31:46.106675 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:31:46.106937 [debug] [MainThread]: On master: COMMIT
[0m13:31:46.107149 [debug] [MainThread]: Using duckdb connection "master"
[0m13:31:46.107344 [debug] [MainThread]: On master: COMMIT
[0m13:31:46.107679 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:31:46.107875 [debug] [MainThread]: On master: Close
[0m13:31:46.108132 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:31:46.108327 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:31:46.108498 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:31:46.108744 [info ] [MainThread]: 
[0m13:31:46.109032 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m13:31:46.109632 [debug] [MainThread]: Command end result
[0m13:31:46.138901 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:31:46.140519 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:31:46.145256 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:31:46.145518 [info ] [MainThread]: 
[0m13:31:46.145786 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:31:46.146014 [info ] [MainThread]: 
[0m13:31:46.146283 [error] [MainThread]: [31mFailure in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)[0m
[0m13:31:46.146532 [error] [MainThread]:   Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:31:46.146726 [info ] [MainThread]: 
[0m13:31:46.146952 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:31:46.147149 [info ] [MainThread]: 
[0m13:31:46.147380 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:31:46.147616 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: Cannot extract field 'e' from expression "signed_up_at" because it is not a struct, union, map, or json
[0m13:31:46.147800 [info ] [MainThread]: 
[0m13:31:46.148016 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:31:46.148192 [info ] [MainThread]: 
[0m13:31:46.148402 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m13:31:46.150179 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2175759, "process_in_blocks": "0", "process_kernel_time": 0.301212, "process_mem_max_rss": "151879680", "process_out_blocks": "0", "process_user_time": 1.942472}
[0m13:31:46.150503 [debug] [MainThread]: Command `dbt run` failed at 13:31:46.150442 after 1.22 seconds
[0m13:31:46.150770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091d14c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108caafc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092594f0>]}
[0m13:31:46.151018 [debug] [MainThread]: Flushing usage events
[0m13:31:46.291999 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:31.895824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106844c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069ee420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107065550>]}


============================== 13:32:31.903305 | a90ac50e-f586-48f4-86ce-451d032db69e ==============================
[0m13:32:31.903305 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:32:31.903811 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'use_colors': 'True', 'no_print': 'None', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'static_parser': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'write_json': 'True', 'fail_fast': 'False', 'introspect': 'True', 'version_check': 'True', 'warn_error': 'None', 'target_path': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'empty': 'False'}
[0m13:32:32.151196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10703ac90>]}
[0m13:32:32.198976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106712660>]}
[0m13:32:32.201132 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:32:32.285913 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:32:32.397530 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:32:32.398373 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:32:32.656048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070915e0>]}
[0m13:32:32.774412 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:32:32.776572 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:32:32.796029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078b7410>]}
[0m13:32:32.796471 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:32:32.796746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a1b230>]}
[0m13:32:32.799794 [info ] [MainThread]: 
[0m13:32:32.800082 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:32:32.800291 [info ] [MainThread]: 
[0m13:32:32.800625 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:32:32.805114 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:32:32.848349 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:32:32.848717 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:32:32.848957 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:32:32.874908 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m13:32:32.876268 [debug] [ThreadPool]: On list_analytics: Close
[0m13:32:32.876788 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:32:32.877120 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:32:32.882353 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:32:32.882643 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:32:32.882860 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:32:32.883833 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:32:32.884837 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:32:32.885047 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:32:32.885388 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:32:32.885581 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:32:32.885770 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:32:32.886274 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:32:32.886980 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:32:32.887232 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:32:32.887442 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:32:32.887785 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:32:32.887996 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:32:32.889298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:32:32.893577 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:32:32.893990 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:32:32.894226 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:32:32.894680 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:32:32.894883 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:32:32.895089 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:32:32.910339 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m13:32:32.911668 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:32:32.912986 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:32:32.913258 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:32:32.915077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078cb740>]}
[0m13:32:32.915419 [debug] [MainThread]: Using duckdb connection "master"
[0m13:32:32.915626 [debug] [MainThread]: On master: BEGIN
[0m13:32:32.915814 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:32:32.916235 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:32:32.916429 [debug] [MainThread]: On master: COMMIT
[0m13:32:32.916615 [debug] [MainThread]: Using duckdb connection "master"
[0m13:32:32.916796 [debug] [MainThread]: On master: COMMIT
[0m13:32:32.917098 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:32:32.917285 [debug] [MainThread]: On master: Close
[0m13:32:32.919207 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:32:32.919508 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:32:32.919877 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:32:32.920291 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:32:32.920653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:32:32.921004 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:32:32.921268 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:32:32.921518 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:32:32.928298 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:32:32.933166 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:32:32.934290 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:32:32.934994 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:32:32.975655 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:32:32.976139 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:32:32.977170 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:32:32.977827 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:32:32.978319 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:32:32.978661 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:32:32.978965 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:32:32.979270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:32:32.979895 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:32:32.980187 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:32:32.980457 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:32:32.980882 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:32:32.981315 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:32:32.982406 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:32:32.990502 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:32:32.991305 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:32:32.991792 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:32:32.992095 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m13:32:32.992529 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:32:32.992915 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: ROLLBACK
[0m13:32:33.001261 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:32:33.002587 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:32:33.002922 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:32:33.003233 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:32:33.006464 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:32:33.009043 [debug] [Thread-2 (]: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:32:33.009408 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062de6f0>]}
[0m13:32:33.011254 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a90ac50e-f586-48f4-86ce-451d032db69e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c32c90>]}
[0m13:32:33.011914 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.09s]
[0m13:32:33.012876 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:32:33.012477 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.rpt_upgrade_analysis ................ [[31mERROR[0m in 0.09s]
[0m13:32:33.013367 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?.
[0m13:32:33.013765 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:32:33.014670 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_upgrade_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^.
[0m13:32:33.015642 [debug] [MainThread]: Using duckdb connection "master"
[0m13:32:33.015920 [debug] [MainThread]: On master: BEGIN
[0m13:32:33.016146 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:32:33.016687 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m13:32:33.016922 [debug] [MainThread]: On master: COMMIT
[0m13:32:33.017141 [debug] [MainThread]: Using duckdb connection "master"
[0m13:32:33.017352 [debug] [MainThread]: On master: COMMIT
[0m13:32:33.017712 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:32:33.018042 [debug] [MainThread]: On master: Close
[0m13:32:33.018458 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:32:33.018704 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:32:33.018916 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:32:33.019333 [info ] [MainThread]: 
[0m13:32:33.019843 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m13:32:33.020634 [debug] [MainThread]: Command end result
[0m13:32:33.057744 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:32:33.059578 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:32:33.064602 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:32:33.064905 [info ] [MainThread]: 
[0m13:32:33.065211 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:32:33.065463 [info ] [MainThread]: 
[0m13:32:33.065765 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:32:33.066057 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:32:33.066306 [info ] [MainThread]: 
[0m13:32:33.066575 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:32:33.066803 [info ] [MainThread]: 
[0m13:32:33.067069 [error] [MainThread]: [31mFailure in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)[0m
[0m13:32:33.067338 [error] [MainThread]:   Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "user_id"
  
  LINE 110:         on ut.user_id = ep.user_id 
                                    ^
[0m13:32:33.067546 [info ] [MainThread]: 
[0m13:32:33.067789 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:32:33.067996 [info ] [MainThread]: 
[0m13:32:33.068239 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m13:32:33.070336 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2311904, "process_in_blocks": "0", "process_kernel_time": 0.296015, "process_mem_max_rss": "153059328", "process_out_blocks": "0", "process_user_time": 1.992903}
[0m13:32:33.071053 [debug] [MainThread]: Command `dbt run` failed at 13:32:33.070960 after 1.23 seconds
[0m13:32:33.071413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070656a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107065760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070656d0>]}
[0m13:32:33.071739 [debug] [MainThread]: Flushing usage events
[0m13:32:33.200318 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:33:58.101863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10718fbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b58e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b56a0>]}


============================== 13:33:58.108199 | 4b7af71d-6526-4d42-a7de-84b18c713614 ==============================
[0m13:33:58.108199 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:33:58.108689 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/hazeldonaldson/.dbt', 'target_path': 'None', 'log_cache_events': 'False', 'write_json': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'no_print': 'None', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'version_check': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'introspect': 'True', 'cache_selected_only': 'False', 'use_experimental_parser': 'False'}
[0m13:33:58.356841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10771ea80>]}
[0m13:33:58.406615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5340>]}
[0m13:33:58.409333 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:33:58.522412 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:33:58.659128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:33:58.660955 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:33:59.273841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107efec30>]}
[0m13:33:59.359739 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:33:59.362085 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:33:59.385305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d686e0>]}
[0m13:33:59.385802 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:33:59.386220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c38320>]}
[0m13:33:59.389857 [info ] [MainThread]: 
[0m13:33:59.390258 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:33:59.390629 [info ] [MainThread]: 
[0m13:33:59.391317 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:33:59.396562 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:33:59.443984 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:33:59.444355 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:33:59.444593 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:59.477822 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m13:33:59.479219 [debug] [ThreadPool]: On list_analytics: Close
[0m13:33:59.479750 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:33:59.480092 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:33:59.485337 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:33:59.485657 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:33:59.485878 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:59.487229 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:33:59.488512 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:33:59.488792 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:33:59.489203 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:33:59.489416 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:33:59.489612 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:33:59.490004 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:33:59.490614 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:33:59.490832 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:33:59.491022 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:33:59.491386 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:33:59.491660 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:33:59.492998 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:33:59.497316 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:33:59.497605 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:33:59.497911 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:33:59.498385 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:33:59.498589 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:33:59.498793 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:33:59.513950 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m13:33:59.515320 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:33:59.516671 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:33:59.516940 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:33:59.518795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e37590>]}
[0m13:33:59.519195 [debug] [MainThread]: Using duckdb connection "master"
[0m13:33:59.519457 [debug] [MainThread]: On master: BEGIN
[0m13:33:59.519676 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:33:59.520175 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:33:59.520393 [debug] [MainThread]: On master: COMMIT
[0m13:33:59.520592 [debug] [MainThread]: Using duckdb connection "master"
[0m13:33:59.520778 [debug] [MainThread]: On master: COMMIT
[0m13:33:59.521085 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:33:59.521281 [debug] [MainThread]: On master: Close
[0m13:33:59.523274 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:33:59.523572 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:33:59.523950 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:33:59.524629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:33:59.524351 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:33:59.524968 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:33:59.525308 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:33:59.532239 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:33:59.532619 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:33:59.536695 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:33:59.537509 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:33:59.537815 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:33:59.576671 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:33:59.579682 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:33:59.580574 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:33:59.580906 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:33:59.581194 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:33:59.581456 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:33:59.581996 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:33:59.582295 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:33:59.582899 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:33:59.583152 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:33:59.583395 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:33:59.583778 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:33:59.584152 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:33:59.585277 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user_id,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:33:59.590586 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:33:59.591321 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user_id,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:33:59.591752 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:33:59.592018 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m13:33:59.592401 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:33:59.592731 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: ROLLBACK
[0m13:33:59.600411 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:33:59.601610 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:33:59.601907 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:33:59.602250 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:33:59.605151 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:33:59.606209 [debug] [Thread-2 (]: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "feature_a_used"
  
  LINE 100:         coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
                             ^
[0m13:33:59.607831 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108377230>]}
[0m13:33:59.608099 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b7af71d-6526-4d42-a7de-84b18c713614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108392990>]}
[0m13:33:59.608604 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.08s]
[0m13:33:59.609441 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:33:59.609099 [error] [Thread-2 (]: 2 of 2 ERROR creating sql table model main.rpt_upgrade_analysis ................ [[31mERROR[0m in 0.08s]
[0m13:33:59.609912 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?.
[0m13:33:59.610271 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:33:59.611072 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_upgrade_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "feature_a_used"
  
  LINE 100:         coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
                             ^.
[0m13:33:59.611942 [debug] [MainThread]: Using duckdb connection "master"
[0m13:33:59.612179 [debug] [MainThread]: On master: BEGIN
[0m13:33:59.612372 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:33:59.612852 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:33:59.613053 [debug] [MainThread]: On master: COMMIT
[0m13:33:59.613241 [debug] [MainThread]: Using duckdb connection "master"
[0m13:33:59.613422 [debug] [MainThread]: On master: COMMIT
[0m13:33:59.613753 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:33:59.613947 [debug] [MainThread]: On master: Close
[0m13:33:59.614193 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:33:59.614377 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:33:59.614632 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:33:59.615588 [info ] [MainThread]: 
[0m13:33:59.616390 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m13:33:59.617212 [debug] [MainThread]: Command end result
[0m13:33:59.781412 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:33:59.785413 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:33:59.794320 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:33:59.794837 [info ] [MainThread]: 
[0m13:33:59.795699 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m13:33:59.796113 [info ] [MainThread]: 
[0m13:33:59.797817 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:33:59.798423 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:33:59.798714 [info ] [MainThread]: 
[0m13:33:59.799214 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:33:59.800004 [info ] [MainThread]: 
[0m13:33:59.801034 [error] [MainThread]: [31mFailure in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)[0m
[0m13:33:59.804223 [error] [MainThread]:   Runtime Error in model rpt_upgrade_analysis (models/marts/conversion/rpt_upgrade_analysis.sql)
  Binder Error: Values list "ep" does not have a column named "feature_a_used"
  
  LINE 100:         coalesce(ep.feature_a_used, 0) as feature_a_used_before_upgrade,
                             ^
[0m13:33:59.804636 [info ] [MainThread]: 
[0m13:33:59.804904 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:33:59.805257 [info ] [MainThread]: 
[0m13:33:59.805577 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=2
[0m13:33:59.812119 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.764882, "process_in_blocks": "0", "process_kernel_time": 0.313709, "process_mem_max_rss": "148078592", "process_out_blocks": "0", "process_user_time": 2.239911}
[0m13:33:59.812714 [debug] [MainThread]: Command `dbt run` failed at 13:33:59.812634 after 1.77 seconds
[0m13:33:59.813054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107649760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b56a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108027770>]}
[0m13:33:59.813347 [debug] [MainThread]: Flushing usage events
[0m13:33:59.951344 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:35:00.190269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102caa480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ed970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ed640>]}


============================== 13:35:00.197391 | fef3fb8b-2273-4f20-a5e2-ca7dcae984b3 ==============================
[0m13:35:00.197391 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:35:00.197893 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/conversion', 'empty': 'False', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'version_check': 'True', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'introspect': 'True', 'debug': 'False'}
[0m13:35:00.429053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105683f80>]}
[0m13:35:00.475326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10551f8f0>]}
[0m13:35:00.478031 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:35:00.557431 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:35:00.668708 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:35:00.669424 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_upgrade_analysis.sql
[0m13:35:01.028856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f39fd0>]}
[0m13:35:01.102453 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:35:01.104387 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:35:01.122988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105eba150>]}
[0m13:35:01.123396 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:35:01.123675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d9a7b0>]}
[0m13:35:01.126571 [info ] [MainThread]: 
[0m13:35:01.126853 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:35:01.127067 [info ] [MainThread]: 
[0m13:35:01.127407 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:35:01.131096 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:35:01.172553 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:35:01.172929 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:35:01.173171 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:35:01.201333 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m13:35:01.202683 [debug] [ThreadPool]: On list_analytics: Close
[0m13:35:01.203191 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:35:01.203520 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:35:01.208707 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:01.208980 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:35:01.209196 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:35:01.210204 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:35:01.211355 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:01.211600 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:35:01.211968 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:01.212162 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:01.212351 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:35:01.212720 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:01.213313 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:35:01.213523 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:01.213711 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:35:01.214019 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:01.214213 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:35:01.215437 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:35:01.219589 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:35:01.219839 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:35:01.220124 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:35:01.220500 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:01.220706 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:35:01.220904 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:35:01.235783 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m13:35:01.237011 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:35:01.238278 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:35:01.238514 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:35:01.240129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc80b0>]}
[0m13:35:01.240464 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:01.240671 [debug] [MainThread]: On master: BEGIN
[0m13:35:01.240860 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:35:01.241256 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:01.241460 [debug] [MainThread]: On master: COMMIT
[0m13:35:01.241652 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:01.241835 [debug] [MainThread]: On master: COMMIT
[0m13:35:01.242137 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:01.242323 [debug] [MainThread]: On master: Close
[0m13:35:01.244087 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:01.244364 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:01.244720 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:35:01.245101 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:35:01.245461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:35:01.245792 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:35:01.246041 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:01.246285 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:01.252538 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:01.256217 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.257143 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:01.257463 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:01.289805 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:01.292830 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.293626 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.293961 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:01.294223 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:35:01.294488 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:35:01.294763 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:35:01.295007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:35:01.295562 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:01.295805 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.296039 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:35:01.296415 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user_id,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used_events, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:35:01.296806 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:01.297710 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:35:01.304164 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::floaf / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:35:01.304632 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:35:01.305005 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:35:01.311758 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:35:01.312114 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:35:01.314279 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:35:01.315822 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104966900>]}
[0m13:35:01.316432 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.07s]
[0m13:35:01.316903 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:01.317271 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?.
[0m13:35:01.318466 [debug] [Thread-2 (]: SQL status: OK in 0.021 seconds
[0m13:35:01.323579 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.323929 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */
alter table "analytics"."main"."rpt_upgrade_analysis__dbt_tmp" rename to "rpt_upgrade_analysis"
[0m13:35:01.325133 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:01.335631 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:35:01.337381 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.337704 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:35:01.339347 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:01.343895 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:01.344208 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

      drop table if exists "analytics"."main"."rpt_upgrade_analysis__dbt_backup" cascade
    
[0m13:35:01.345075 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:01.346896 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:35:01.347340 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fef3fb8b-2273-4f20-a5e2-ca7dcae984b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d9ad80>]}
[0m13:35:01.347807 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.rpt_upgrade_analysis .................... [[32mOK[0m in 0.10s]
[0m13:35:01.348206 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:01.349072 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:01.349315 [debug] [MainThread]: On master: BEGIN
[0m13:35:01.349516 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:35:01.349914 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:01.350118 [debug] [MainThread]: On master: COMMIT
[0m13:35:01.350308 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:01.350491 [debug] [MainThread]: On master: COMMIT
[0m13:35:01.350801 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:01.350990 [debug] [MainThread]: On master: Close
[0m13:35:01.351232 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:35:01.351413 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:35:01.351596 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:35:01.351829 [info ] [MainThread]: 
[0m13:35:01.352049 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m13:35:01.352588 [debug] [MainThread]: Command end result
[0m13:35:01.384021 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:35:01.385756 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:35:01.390381 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:35:01.390670 [info ] [MainThread]: 
[0m13:35:01.390946 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:35:01.391173 [info ] [MainThread]: 
[0m13:35:01.391444 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:35:01.391700 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Catalog Error: Type with name floaf does not exist!
  Did you mean "float"?
[0m13:35:01.391888 [info ] [MainThread]: 
[0m13:35:01.392114 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:35:01.392310 [info ] [MainThread]: 
[0m13:35:01.392528 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m13:35:01.394448 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2603124, "process_in_blocks": "0", "process_kernel_time": 0.288643, "process_mem_max_rss": "166084608", "process_out_blocks": "0", "process_user_time": 2.045938}
[0m13:35:01.394810 [debug] [MainThread]: Command `dbt run` failed at 13:35:01.394743 after 1.26 seconds
[0m13:35:01.395096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104990650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ed730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ed820>]}
[0m13:35:01.395361 [debug] [MainThread]: Flushing usage events
[0m13:35:01.521298 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:35:34.203332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076c3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bb58e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bb56a0>]}


============================== 13:35:34.222590 | f9b70804-2bb8-4696-ba4a-9a9e322574c3 ==============================
[0m13:35:34.222590 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:35:34.223130 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'no_print': 'None', 'warn_error': 'None', 'empty': 'False', 'printer_width': '80', 'partial_parse': 'True', 'fail_fast': 'False', 'debug': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'version_check': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_colors': 'True', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None'}
[0m13:35:34.459898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107370d10>]}
[0m13:35:34.507477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078e6600>]}
[0m13:35:34.509394 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:35:34.591795 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:35:34.705073 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:35:34.705768 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:35:34.958539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119c4aa0>]}
[0m13:35:35.075123 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:35:35.077207 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:35:35.098129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111918a10>]}
[0m13:35:35.098564 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:35:35.098838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119076e0>]}
[0m13:35:35.101824 [info ] [MainThread]: 
[0m13:35:35.102115 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:35:35.102689 [info ] [MainThread]: 
[0m13:35:35.103106 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:35:35.107059 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:35:35.149608 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:35:35.149964 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:35:35.150197 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:35:35.175242 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m13:35:35.176625 [debug] [ThreadPool]: On list_analytics: Close
[0m13:35:35.177198 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:35:35.177545 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:35:35.182679 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:35.182963 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:35:35.183177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:35:35.184341 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:35:35.185327 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:35.185551 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:35:35.185897 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:35.186126 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:35.186366 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:35:35.186784 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:35.187402 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:35:35.187615 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:35:35.187806 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:35:35.188122 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:35.188315 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:35:35.189542 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:35:35.193790 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:35:35.194141 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:35:35.194347 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:35:35.194786 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:35:35.194982 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:35:35.195181 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:35:35.209402 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m13:35:35.210647 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:35:35.211906 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:35:35.212119 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:35:35.213850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110db1730>]}
[0m13:35:35.214205 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:35.214414 [debug] [MainThread]: On master: BEGIN
[0m13:35:35.214601 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:35:35.215036 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:35.215239 [debug] [MainThread]: On master: COMMIT
[0m13:35:35.215419 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:35.215593 [debug] [MainThread]: On master: COMMIT
[0m13:35:35.215901 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:35.216087 [debug] [MainThread]: On master: Close
[0m13:35:35.218095 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:35.218377 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:35.218739 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:35:35.219131 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:35:35.219503 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:35:35.219865 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:35:35.220140 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:35.220391 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:35.228452 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:35.232699 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.233703 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:35.233985 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:35.264227 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:35.267737 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.268841 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:35.269464 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.269740 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:35:35.270071 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:35:35.270365 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:35:35.270997 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:35:35.271756 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.272039 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.272291 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:35:35.272584 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.272965 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:35:35.273493 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user_id,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used_events, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:35:35.279634 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:35:35.280121 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m13:35:35.280481 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: ROLLBACK
[0m13:35:35.287814 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_event_conversion_correlation'
[0m13:35:35.288199 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:35:35.290186 [debug] [Thread-1 (]: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: * expression without FROM clause!
[0m13:35:35.292679 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d6d8b0>]}
[0m13:35:35.294245 [error] [Thread-1 (]: 1 of 2 ERROR creating sql table model main.rpt_event_conversion_correlation .... [[31mERROR[0m in 0.07s]
[0m13:35:35.294875 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:35:35.295371 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_event_conversion_correlation' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: * expression without FROM clause!.
[0m13:35:35.297098 [debug] [Thread-2 (]: SQL status: OK in 0.023 seconds
[0m13:35:35.302847 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.303286 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_upgrade_analysis'
  
[0m13:35:35.304172 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.305826 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.306232 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_upgrade_analysis'
  
[0m13:35:35.307263 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.313418 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.313792 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */
alter table "analytics"."main"."rpt_upgrade_analysis" rename to "rpt_upgrade_analysis__dbt_backup"
[0m13:35:35.314708 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.317250 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.317557 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */
alter table "analytics"."main"."rpt_upgrade_analysis__dbt_tmp" rename to "rpt_upgrade_analysis"
[0m13:35:35.318084 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m13:35:35.330443 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:35:35.330752 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.331004 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:35:35.332637 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.336894 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:35:35.337209 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

      drop table if exists "analytics"."main"."rpt_upgrade_analysis__dbt_backup" cascade
    
[0m13:35:35.338193 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:35:35.340009 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:35:35.340466 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9b70804-2bb8-4696-ba4a-9a9e322574c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1037fb020>]}
[0m13:35:35.340926 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.rpt_upgrade_analysis .................... [[32mOK[0m in 0.12s]
[0m13:35:35.341320 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:35:35.342161 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:35.342415 [debug] [MainThread]: On master: BEGIN
[0m13:35:35.342620 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:35:35.343019 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:35.343221 [debug] [MainThread]: On master: COMMIT
[0m13:35:35.343410 [debug] [MainThread]: Using duckdb connection "master"
[0m13:35:35.343590 [debug] [MainThread]: On master: COMMIT
[0m13:35:35.343893 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:35:35.344082 [debug] [MainThread]: On master: Close
[0m13:35:35.344322 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:35:35.344502 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:35:35.344672 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:35:35.344898 [info ] [MainThread]: 
[0m13:35:35.345111 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m13:35:35.345619 [debug] [MainThread]: Command end result
[0m13:35:35.375298 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:35:35.376858 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:35:35.381388 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:35:35.381627 [info ] [MainThread]: 
[0m13:35:35.381878 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:35:35.382084 [info ] [MainThread]: 
[0m13:35:35.382345 [error] [MainThread]: [31mFailure in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)[0m
[0m13:35:35.382587 [error] [MainThread]:   Runtime Error in model rpt_event_conversion_correlation (models/marts/conversion/rpt_event_conversion_correlation.sql)
  Binder Error: * expression without FROM clause!
[0m13:35:35.382774 [info ] [MainThread]: 
[0m13:35:35.383000 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:35:35.383189 [info ] [MainThread]: 
[0m13:35:35.383391 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=2
[0m13:35:35.385090 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2397147, "process_in_blocks": "0", "process_kernel_time": 0.305109, "process_mem_max_rss": "166019072", "process_out_blocks": "0", "process_user_time": 2.015814}
[0m13:35:35.385409 [debug] [MainThread]: Command `dbt run` failed at 13:35:35.385349 after 1.24 seconds
[0m13:35:35.385666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b6a6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10695d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111925ee0>]}
[0m13:35:35.385910 [debug] [MainThread]: Flushing usage events
[0m13:35:35.517007 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:35:59.095878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069a6c60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b58e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b5670>]}


============================== 13:35:59.104214 | 3768f7fc-193b-4609-b75c-bf124fd5c27b ==============================
[0m13:35:59.104214 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:35:59.104928 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'debug': 'False', 'log_cache_events': 'False', 'empty': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select path:models/marts/conversion', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'version_check': 'True', 'warn_error': 'None', 'target_path': 'None', 'quiet': 'False', 'static_parser': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True'}
[0m13:35:59.430640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106359b50>]}
[0m13:35:59.490119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708e990>]}
[0m13:35:59.492373 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:35:59.584733 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:35:59.698903 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:35:59.699636 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/conversion/rpt_event_conversion_correlation.sql
[0m13:35:59.954645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10766fda0>]}
[0m13:36:00.074734 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:36:00.077474 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:36:00.096910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e41460>]}
[0m13:36:00.097324 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:36:00.097600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e194f0>]}
[0m13:36:00.100553 [info ] [MainThread]: 
[0m13:36:00.100849 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:36:00.101058 [info ] [MainThread]: 
[0m13:36:00.101430 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:36:00.105174 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m13:36:00.147497 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m13:36:00.147859 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m13:36:00.148089 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:36:00.174468 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m13:36:00.175630 [debug] [ThreadPool]: On list_analytics: Close
[0m13:36:00.176153 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m13:36:00.176485 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m13:36:00.181635 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:36:00.181900 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m13:36:00.182120 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:36:00.183155 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m13:36:00.184114 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:36:00.184330 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m13:36:00.184678 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:36:00.184864 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:36:00.185047 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m13:36:00.185637 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:36:00.186288 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:36:00.186573 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m13:36:00.186801 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m13:36:00.187207 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:36:00.187415 [debug] [ThreadPool]: On create_analytics_main: Close
[0m13:36:00.188653 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m13:36:00.192851 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:36:00.193196 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:36:00.193402 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:36:00.193802 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:36:00.194005 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:36:00.194208 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:36:00.213390 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m13:36:00.214809 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:36:00.217213 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:36:00.217459 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:36:00.219340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e4e7b0>]}
[0m13:36:00.219689 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:00.219898 [debug] [MainThread]: On master: BEGIN
[0m13:36:00.220088 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:36:00.220493 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:00.220688 [debug] [MainThread]: On master: COMMIT
[0m13:36:00.220872 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:00.221048 [debug] [MainThread]: On master: COMMIT
[0m13:36:00.221349 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:00.221533 [debug] [MainThread]: On master: Close
[0m13:36:00.223979 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:36:00.224280 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_upgrade_analysis
[0m13:36:00.224698 [info ] [Thread-1 (]: 1 of 2 START sql table model main.rpt_event_conversion_correlation ............. [RUN]
[0m13:36:00.225098 [info ] [Thread-2 (]: 2 of 2 START sql table model main.rpt_upgrade_analysis ......................... [RUN]
[0m13:36:00.225437 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_event_conversion_correlation)
[0m13:36:00.225799 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_upgrade_analysis'
[0m13:36:00.226057 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_event_conversion_correlation
[0m13:36:00.226290 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_upgrade_analysis
[0m13:36:00.233012 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.237894 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.238853 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_event_conversion_correlation
[0m13:36:00.239160 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_upgrade_analysis
[0m13:36:00.269002 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.272461 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.273241 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.273561 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.273843 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: BEGIN
[0m13:36:00.274098 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: BEGIN
[0m13:36:00.274331 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:36:00.274558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:36:00.275098 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:36:00.275340 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.275583 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:36:00.275975 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

  
    
    

    create  table
      "analytics"."main"."rpt_upgrade_analysis__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the first paid subscription for each user
first_paid_subscription as (
    select 
        user_id,
        min(subscription_started_at) as first_paid_at,
        min(plan) as first_plan 
    from subscriptions 
    where plan != 'free'
    group by user_id 
),

--join to users to get signup date and calculate time-to-upgrade
upgrade_timing as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.country,
        fps.first_paid_at,
        fps.first_plan,

        --time to upgrade
        datediff('day', u.signed_up_at, fps.first_paid_at) as days_to_upgrade,
        datediff('hour', u.signed_up_at, fps.first_paid_at) as hours_to_upgrade,

        --bucketed time to upgrade
        case
            when datediff('day', u.signed_up_at, fps.first_paid_at) = 0 then '0 - Same Day'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 1 then '1 - Day 1'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 3  then '2 - Days 2-3'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 7 then '3 - Days 4-7'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 14 then '4 - Day 8-14'
            when datediff('day', u.signed_up_at, fps.first_paid_at) <= 30 then '5 - Day 15-30'
            else '6 - 30+ Days'
        end as upgrade_timeframe
    from users u 
    inner join first_paid_subscription fps 
        on u.user_id = fps.user_id 

),

--count events BEFORE upgrade to see what predicts conversion
events_before_upgrade as (
    select
        e.user_id,
        e.event_name,
        count(*) as event_count
    from events e 
    inner join upgrade_timing ut 
        on e.user_id = ut.user_id 
    where e.event_at < ut.first_paid_at --only events before they upgraded
    group by e.user_id, e.event_name
),

--pivot event counts to wide format
events_pivoted as (
    select 
        user_id,
        max(case when event_name = 'signup' then event_count else 0 end) as signup_events,
        max(case when event_name = 'onboarding_completed' then event_count else 0 end) as onboarding_completed_events,
        max(case when event_name = 'feature_a_used' then event_count else 0 end) as feature_a_used_events,
        max(case when event_name = 'feature_b_used' then event_count else 0 end) as feature_b_used_events,
        max(case when event_name = 'upgrade' then event_count else 0 end) as upgrade_events,
        max(case when event_name = 'cancel' then event_count else 0 end) as cancel_events
    from events_before_upgrade
    group by user_id 
),

--combine everything
final as (
    select 
        ut.*,

        --event counts before upgrade
        coalesce(ep.onboarding_completed_events, 0) as onboarding_completed_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) as feature_a_used_before_upgrade,
        coalesce(ep.feature_b_used_events, 0)  as feature_b_used_before_upgrade,

        --flags for key behaviors
        coalesce(ep.onboarding_completed_events, 0) > 0 as completed_onboarding_before_upgrade,
        coalesce(ep.feature_a_used_events, 0) > 0 as used_feature_a_before_upgrade,
        coalesce(ep.feature_b_used_events, 0) > 0 as used_feature_b_before_upgrade

    from upgrade_timing ut 
    left join events_pivoted ep 
        on ut.user_id = ep.user_id 


)

select * from final
    );
  
  
[0m13:36:00.276371 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.277305 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

  
    
    

    create  table
      "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get all events in first 7 days after signup
early_events as (
    select 
        e.user_id,
        e.event_name,
        count(*) as event_count 
    from events e 
    inner join users u 
        on e.user_id = u.user_id 
    where datediff('day', u.signed_up_at, e.event_at) <= 7
    group by e.user_id, e.event_name 
),

--pivot events to wide format
events_wide as (
    select
        user_id,
        sum(case when event_name = 'onboarding_completed' then event_count else 0 end) > 0 as completed_onboarding,
        sum(case when event_name = 'feature_a_used' then event_count else 0 end) > 0 as used_feature_a,
        sum(case when event_name = 'feature_b_used' then event_count else 0 end) > 0 as used_feature_b,
        sum(case when event_name = 'upgrade' then event_count else 0 end) > 0 as upgraded_in_week_1
    from early_events 
    group by user_id 
),

--join to users to get conversion status
user_conversion_status as (
    select
        u.user_id,
        u.current_plan != 'free' as is_paid_user,
        coalesce(ew.completed_onboarding, false) as completed_onboarding,
        coalesce(ew.used_feature_a, false) as used_feature_a,
        coalesce(ew.used_feature_b, false) as used_feature_b,
        coalesce(ew.upgraded_in_week_1, false) as upgraded_in_week_1
    from users u 
    left join events_wide ew 
        on u.user_id = ew.user_id 
),

--calculate conversion rates by event completion
conversion_by_onboarding as (
    select
        'Onboarding Completed' as event_type,
        completed_onboarding as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*)  * 100 conversion_rate
    from user_conversion_status 
    group by completed_onboarding
),

conversion_by_feature_a as (
    select 
        'Feature A Used' as event_type,
        used_feature_a as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate
    from user_conversion_status
    group by used_feature_a
),

conversion_by_feature_b as (
    select
        'Feature B Used' as event_type,
        used_feature_b as completed_event,
        count(*) as total_users,
        sum(case when is_paid_user then 1 else 0 end) as paid_users,
        sum(case when is_paid_user then 1 else 0 end)::float / count(*) * 100 as conversion_rate 
    from user_conversion_status
    group by used_feature_b
),

-- combine all metrics
final as (
    select * from conversion_by_onboarding
    union all 
    select * from conversion_by_feature_a
    union all
    select * from conversion_by_feature_b
)

select * from final
order by event_type, completed_event
    );
  
  
[0m13:36:00.299302 [debug] [Thread-1 (]: SQL status: OK in 0.022 seconds
[0m13:36:00.299737 [debug] [Thread-2 (]: SQL status: OK in 0.023 seconds
[0m13:36:00.305218 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.309506 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.309868 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */
alter table "analytics"."main"."rpt_event_conversion_correlation__dbt_tmp" rename to "rpt_event_conversion_correlation"
[0m13:36:00.310862 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_upgrade_analysis'
  
[0m13:36:00.312156 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:36:00.312718 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m13:36:00.315591 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.325934 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: COMMIT
[0m13:36:00.326309 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_upgrade_analysis'
  
[0m13:36:00.326940 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.327382 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: COMMIT
[0m13:36:00.328183 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:36:00.330691 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.330968 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */
alter table "analytics"."main"."rpt_upgrade_analysis" rename to "rpt_upgrade_analysis__dbt_backup"
[0m13:36:00.331231 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m13:36:00.335335 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_event_conversion_correlation"
[0m13:36:00.335671 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m13:36:00.335991 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_event_conversion_correlation"} */

      drop table if exists "analytics"."main"."rpt_event_conversion_correlation__dbt_backup" cascade
    
[0m13:36:00.338240 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.338653 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */
alter table "analytics"."main"."rpt_upgrade_analysis__dbt_tmp" rename to "rpt_upgrade_analysis"
[0m13:36:00.338972 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:36:00.340673 [debug] [Thread-1 (]: On model.saas_analytics.rpt_event_conversion_correlation: Close
[0m13:36:00.340942 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m13:36:00.343706 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:36:00.344907 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.345231 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: COMMIT
[0m13:36:00.345742 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d5a9f0>]}
[0m13:36:00.346225 [info ] [Thread-1 (]: 1 of 2 OK created sql table model main.rpt_event_conversion_correlation ........ [[32mOK[0m in 0.12s]
[0m13:36:00.346633 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_event_conversion_correlation
[0m13:36:00.346890 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:36:00.348848 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_upgrade_analysis"
[0m13:36:00.349114 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_upgrade_analysis"} */

      drop table if exists "analytics"."main"."rpt_upgrade_analysis__dbt_backup" cascade
    
[0m13:36:00.349819 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m13:36:00.350781 [debug] [Thread-2 (]: On model.saas_analytics.rpt_upgrade_analysis: Close
[0m13:36:00.351181 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3768f7fc-193b-4609-b75c-bf124fd5c27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11013c620>]}
[0m13:36:00.351610 [info ] [Thread-2 (]: 2 of 2 OK created sql table model main.rpt_upgrade_analysis .................... [[32mOK[0m in 0.13s]
[0m13:36:00.351984 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_upgrade_analysis
[0m13:36:00.353166 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:00.353486 [debug] [MainThread]: On master: BEGIN
[0m13:36:00.353687 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:36:00.354178 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:00.354381 [debug] [MainThread]: On master: COMMIT
[0m13:36:00.354573 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:00.354753 [debug] [MainThread]: On master: COMMIT
[0m13:36:00.355077 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:00.355268 [debug] [MainThread]: On master: Close
[0m13:36:00.355639 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:36:00.356001 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_event_conversion_correlation' was properly closed.
[0m13:36:00.356240 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_upgrade_analysis' was properly closed.
[0m13:36:00.356518 [info ] [MainThread]: 
[0m13:36:00.356774 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m13:36:00.357427 [debug] [MainThread]: Command end result
[0m13:36:00.387767 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:36:00.389370 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:36:00.394032 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:36:00.394324 [info ] [MainThread]: 
[0m13:36:00.394600 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:36:00.394815 [info ] [MainThread]: 
[0m13:36:00.395043 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m13:36:00.396814 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.3686572, "process_in_blocks": "0", "process_kernel_time": 0.350214, "process_mem_max_rss": "180715520", "process_out_blocks": "0", "process_user_time": 2.121973}
[0m13:36:00.397144 [debug] [MainThread]: Command `dbt run` succeeded at 13:36:00.397083 after 1.37 seconds
[0m13:36:00.397407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b5760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072b5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10704ea80>]}
[0m13:36:00.397667 [debug] [MainThread]: Flushing usage events
[0m13:36:00.517297 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:36:23.619281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088a1ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f219a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f21790>]}


============================== 13:36:23.639174 | ae305915-829e-49cd-a406-9374e4fd963d ==============================
[0m13:36:23.639174 [info ] [MainThread]: Running with dbt=1.11.4
[0m13:36:23.639733 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'introspect': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'printer_width': '80', 'use_experimental_parser': 'False', 'write_json': 'True', 'target_path': 'None', 'use_colors': 'True', 'log_format': 'default', 'invocation_command': 'dbt test --select path:models/marts/conversion', 'no_print': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m13:36:23.886354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090be120>]}
[0m13:36:23.934018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10880bb00>]}
[0m13:36:23.936086 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m13:36:24.017259 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m13:36:24.132355 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:36:24.132717 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m13:36:24.132946 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:36:24.169650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091e90d0>]}
[0m13:36:24.249548 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:36:24.251923 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:36:24.279744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097a28d0>]}
[0m13:36:24.280182 [info ] [MainThread]: Found 14 models, 51 data tests, 3 sources, 472 macros
[0m13:36:24.280449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091e98b0>]}
[0m13:36:24.283595 [info ] [MainThread]: 
[0m13:36:24.283881 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:36:24.284094 [info ] [MainThread]: 
[0m13:36:24.284439 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m13:36:24.288767 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m13:36:24.376316 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:36:24.376674 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m13:36:24.376897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:36:24.397406 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m13:36:24.397742 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m13:36:24.397973 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m13:36:24.419609 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m13:36:24.421596 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m13:36:24.422950 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m13:36:24.423191 [debug] [ThreadPool]: On list_analytics_main: Close
[0m13:36:24.425209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ae305915-829e-49cd-a406-9374e4fd963d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10947ee70>]}
[0m13:36:24.425554 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:24.425756 [debug] [MainThread]: On master: BEGIN
[0m13:36:24.425944 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:36:24.426412 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:24.426608 [debug] [MainThread]: On master: COMMIT
[0m13:36:24.426797 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:24.426982 [debug] [MainThread]: On master: COMMIT
[0m13:36:24.427286 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:24.427478 [debug] [MainThread]: On master: Close
[0m13:36:24.429632 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5
[0m13:36:24.429942 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae
[0m13:36:24.430440 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec
[0m13:36:24.430231 [info ] [Thread-1 (]: 1 of 3 START test not_null_rpt_upgrade_analysis_first_paid_at .................. [RUN]
[0m13:36:24.430810 [info ] [Thread-2 (]: 2 of 3 START test not_null_rpt_upgrade_analysis_user_id ........................ [RUN]
[0m13:36:24.431101 [info ] [Thread-3 (]: 3 of 3 START test unique_rpt_upgrade_analysis_user_id .......................... [RUN]
[0m13:36:24.431423 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5)
[0m13:36:24.431757 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae'
[0m13:36:24.432065 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec'
[0m13:36:24.432304 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5
[0m13:36:24.432546 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae
[0m13:36:24.432769 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec
[0m13:36:24.449809 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae"
[0m13:36:24.450955 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5"
[0m13:36:24.456528 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec"
[0m13:36:24.457611 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5
[0m13:36:24.457971 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec
[0m13:36:24.458263 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae
[0m13:36:24.474042 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec"
[0m13:36:24.474940 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5"
[0m13:36:24.476743 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae"
[0m13:36:24.477585 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec"
[0m13:36:24.477915 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5"
[0m13:36:24.478209 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae"
[0m13:36:24.478449 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec: BEGIN
[0m13:36:24.478700 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5: BEGIN
[0m13:36:24.478929 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae: BEGIN
[0m13:36:24.479157 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:36:24.479373 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:36:24.479586 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:36:24.480228 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m13:36:24.480488 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m13:36:24.480770 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5"
[0m13:36:24.481004 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m13:36:24.481232 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec"
[0m13:36:24.481500 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select first_paid_at
from "analytics"."main"."rpt_upgrade_analysis"
where first_paid_at is null



  
  
      
    ) dbt_internal_test
[0m13:36:24.481788 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae"
[0m13:36:24.482051 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_upgrade_analysis"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m13:36:24.482426 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."rpt_upgrade_analysis"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m13:36:24.484030 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m13:36:24.484284 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m13:36:24.488001 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5: ROLLBACK
[0m13:36:24.489427 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae: ROLLBACK
[0m13:36:24.490278 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5'
[0m13:36:24.490590 [debug] [Thread-3 (]: SQL status: OK in 0.008 seconds
[0m13:36:24.491293 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae'
[0m13:36:24.491587 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5: Close
[0m13:36:24.491959 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae: Close
[0m13:36:24.493065 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec: ROLLBACK
[0m13:36:24.493566 [info ] [Thread-1 (]: 1 of 3 PASS not_null_rpt_upgrade_analysis_first_paid_at ........................ [[32mPASS[0m in 0.06s]
[0m13:36:24.494761 [info ] [Thread-2 (]: 2 of 3 PASS not_null_rpt_upgrade_analysis_user_id .............................. [[32mPASS[0m in 0.06s]
[0m13:36:24.495516 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec'
[0m13:36:24.495893 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5
[0m13:36:24.496239 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae
[0m13:36:24.496475 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec: Close
[0m13:36:24.497006 [info ] [Thread-3 (]: 3 of 3 PASS unique_rpt_upgrade_analysis_user_id ................................ [[32mPASS[0m in 0.06s]
[0m13:36:24.497360 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec
[0m13:36:24.498182 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:24.498435 [debug] [MainThread]: On master: BEGIN
[0m13:36:24.498660 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:36:24.499182 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m13:36:24.499413 [debug] [MainThread]: On master: COMMIT
[0m13:36:24.499623 [debug] [MainThread]: Using duckdb connection "master"
[0m13:36:24.499811 [debug] [MainThread]: On master: COMMIT
[0m13:36:24.500132 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m13:36:24.500336 [debug] [MainThread]: On master: Close
[0m13:36:24.500604 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:36:24.500796 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_upgrade_analysis_first_paid_at.88709820a5' was properly closed.
[0m13:36:24.500979 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_upgrade_analysis_user_id.ec9c4f23ae' was properly closed.
[0m13:36:24.501154 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_upgrade_analysis_user_id.c0399eebec' was properly closed.
[0m13:36:24.501363 [info ] [MainThread]: 
[0m13:36:24.501585 [info ] [MainThread]: Finished running 3 data tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m13:36:24.502185 [debug] [MainThread]: Command end result
[0m13:36:24.537278 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m13:36:24.538929 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m13:36:24.544146 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m13:36:24.544415 [info ] [MainThread]: 
[0m13:36:24.544683 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:36:24.544896 [info ] [MainThread]: 
[0m13:36:24.545116 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m13:36:24.546531 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.98565763, "process_in_blocks": "0", "process_kernel_time": 0.292216, "process_mem_max_rss": "151044096", "process_out_blocks": "0", "process_user_time": 1.742425}
[0m13:36:24.546857 [debug] [MainThread]: Command `dbt test` succeeded at 13:36:24.546794 after 0.99 seconds
[0m13:36:24.547114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b72e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b72ea0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b72d80>]}
[0m13:36:24.547369 [debug] [MainThread]: Flushing usage events
[0m13:36:24.671637 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:59:24.937368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ed0320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c7da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c7d850>]}


============================== 11:59:24.964337 | abf715c3-5fd6-40ec-ac21-5befdd80feb4 ==============================
[0m11:59:24.964337 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:59:24.964799 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'use_colors': 'True', 'version_check': 'True', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'introspect': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'log_format': 'default', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'empty': 'False', 'partial_parse': 'True', 'no_print': 'None', 'target_path': 'None'}
[0m11:59:25.489026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e1a240>]}
[0m11:59:25.520181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ef5430>]}
[0m11:59:25.522870 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:59:25.588548 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:59:25.675564 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m11:59:25.676001 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m11:59:25.676181 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/revenue/rpt_customer_ltv.sql
[0m11:59:25.676405 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/revenue/revenue.yml
[0m11:59:25.676550 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/revenue/fct_mrr_by_month.sql
[0m11:59:25.923118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092e8980>]}
[0m11:59:25.971628 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:59:25.973486 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:59:25.991281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094f6510>]}
[0m11:59:25.991581 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m11:59:25.991774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10937a690>]}
[0m11:59:25.993879 [info ] [MainThread]: 
[0m11:59:25.994063 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:59:25.994206 [info ] [MainThread]: 
[0m11:59:25.994440 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:59:25.996649 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:59:26.027670 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:59:26.027928 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:59:26.028082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:59:26.039451 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:59:26.040186 [debug] [ThreadPool]: On list_analytics: Close
[0m11:59:26.040552 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:59:26.040810 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:59:26.044717 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:26.044911 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:59:26.045049 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:59:26.045559 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:26.046195 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:26.046332 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:59:26.046554 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:26.046674 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:26.046787 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:59:26.047011 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:26.047369 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:59:26.047508 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:26.047626 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:59:26.047825 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:26.047950 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:59:26.048861 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:59:26.051887 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:59:26.052101 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:59:26.052251 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:59:26.052604 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:26.052737 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:59:26.052875 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:59:26.061487 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:59:26.062549 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:59:26.063491 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:59:26.063664 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:59:26.064833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109557dd0>]}
[0m11:59:26.065063 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:26.065197 [debug] [MainThread]: On master: BEGIN
[0m11:59:26.065321 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:59:26.065603 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:26.065736 [debug] [MainThread]: On master: COMMIT
[0m11:59:26.065861 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:26.065980 [debug] [MainThread]: On master: COMMIT
[0m11:59:26.066182 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:26.066306 [debug] [MainThread]: On master: Close
[0m11:59:26.067707 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m11:59:26.067888 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m11:59:26.068142 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m11:59:26.068399 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m11:59:26.068630 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m11:59:26.068852 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m11:59:26.069025 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m11:59:26.069182 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m11:59:26.073193 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m11:59:26.075065 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m11:59:26.076018 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m11:59:26.076213 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m11:59:26.149540 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m11:59:26.150234 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m11:59:26.151114 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m11:59:26.151357 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:26.151557 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m11:59:26.151776 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m11:59:26.151981 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:59:26.152150 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:26.152914 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:59:26.153132 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m11:59:26.153346 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:59:26.153595 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m11:59:26.154098 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:26.155457 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m11:59:26.155832 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m11:59:26.156261 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:59:26.156828 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m11:59:26.157993 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m11:59:26.158374 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:59:26.158686 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: ROLLBACK
[0m11:59:26.177913 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m11:59:26.178837 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.fct_mrr_by_month'
[0m11:59:26.179064 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m11:59:26.179277 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m11:59:26.182130 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m11:59:26.184397 [debug] [Thread-1 (]: Runtime Error in model fct_mrr_by_month (models/marts/revenue/fct_mrr_by_month.sql)
  Binder Error: Referenced table "s" not found!
  Candidate tables: "subscriptions"
  
  LINE 43:     where d.month_date >= date_trunc('month', s.subscription_started_at)
                                                         ^
[0m11:59:26.184974 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109966a50>]}
[0m11:59:26.185152 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'abf715c3-5fd6-40ec-ac21-5befdd80feb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10997cc50>]}
[0m11:59:26.185519 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.11s]
[0m11:59:26.185779 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.fct_mrr_by_month .................... [[31mERROR[0m in 0.12s]
[0m11:59:26.186345 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m11:59:26.186818 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m11:59:26.187525 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m11:59:26.188596 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.fct_mrr_by_month' to be skipped because of status 'error'.  Reason: Runtime Error in model fct_mrr_by_month (models/marts/revenue/fct_mrr_by_month.sql)
  Binder Error: Referenced table "s" not found!
  Candidate tables: "subscriptions"
  
  LINE 43:     where d.month_date >= date_trunc('month', s.subscription_started_at)
                                                         ^.
[0m11:59:26.189806 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m11:59:26.190089 [info ] [Thread-4 (]: 3 of 3 SKIP relation main.rpt_mrr_movements .................................... [[33mSKIP[0m]
[0m11:59:26.190495 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m11:59:26.191305 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:26.191483 [debug] [MainThread]: On master: BEGIN
[0m11:59:26.191616 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:59:26.192294 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:59:26.192460 [debug] [MainThread]: On master: COMMIT
[0m11:59:26.192600 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:26.192726 [debug] [MainThread]: On master: COMMIT
[0m11:59:26.193017 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:26.193157 [debug] [MainThread]: On master: Close
[0m11:59:26.193366 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:59:26.193492 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m11:59:26.193607 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m11:59:26.193779 [info ] [MainThread]: 
[0m11:59:26.194081 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m11:59:26.194519 [debug] [MainThread]: Command end result
[0m11:59:26.219119 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:59:26.220271 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:59:26.223809 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:59:26.224041 [info ] [MainThread]: 
[0m11:59:26.224287 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:59:26.224464 [info ] [MainThread]: 
[0m11:59:26.224660 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m11:59:26.224833 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m11:59:26.224961 [info ] [MainThread]: 
[0m11:59:26.225110 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m11:59:26.225245 [info ] [MainThread]: 
[0m11:59:26.225398 [error] [MainThread]: [31mFailure in model fct_mrr_by_month (models/marts/revenue/fct_mrr_by_month.sql)[0m
[0m11:59:26.225555 [error] [MainThread]:   Runtime Error in model fct_mrr_by_month (models/marts/revenue/fct_mrr_by_month.sql)
  Binder Error: Referenced table "s" not found!
  Candidate tables: "subscriptions"
  
  LINE 43:     where d.month_date >= date_trunc('month', s.subscription_started_at)
                                                         ^
[0m11:59:26.225677 [info ] [MainThread]: 
[0m11:59:26.225817 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/fct_mrr_by_month.sql
[0m11:59:26.225939 [info ] [MainThread]: 
[0m11:59:26.226082 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=1 NO-OP=0 TOTAL=3
[0m11:59:26.238001 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3491362, "process_in_blocks": "0", "process_kernel_time": 0.257426, "process_mem_max_rss": "148832256", "process_out_blocks": "0", "process_user_time": 1.445719}
[0m11:59:26.238443 [debug] [MainThread]: Command `dbt run` failed at 11:59:26.238351 after 1.35 seconds
[0m11:59:26.238690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ed0350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108eebcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10941bd10>]}
[0m11:59:26.238927 [debug] [MainThread]: Flushing usage events
[0m11:59:26.370361 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:59:50.667810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f804d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d09a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d097c0>]}


============================== 11:59:50.672854 | e7fe8d32-502c-4db0-9682-f7eb86adca76 ==============================
[0m11:59:50.672854 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:59:50.673185 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'partial_parse': 'True', 'debug': 'False', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'printer_width': '80', 'quiet': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'write_json': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'version_check': 'True', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'static_parser': 'True', 'cache_selected_only': 'False', 'empty': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False'}
[0m11:59:50.853789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e132c0>]}
[0m11:59:50.884027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f5c3b0>]}
[0m11:59:50.886877 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:59:50.940938 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:59:51.018082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:59:51.018568 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/fct_mrr_by_month.sql
[0m11:59:51.247026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107579eb0>]}
[0m11:59:51.296755 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:59:51.298193 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:59:51.312162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107437c20>]}
[0m11:59:51.312445 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m11:59:51.312626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10742b200>]}
[0m11:59:51.314747 [info ] [MainThread]: 
[0m11:59:51.314944 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:59:51.315088 [info ] [MainThread]: 
[0m11:59:51.315333 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:59:51.317509 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:59:51.346836 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:59:51.347077 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:59:51.347232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:59:51.366034 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:59:51.367052 [debug] [ThreadPool]: On list_analytics: Close
[0m11:59:51.367428 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:59:51.367683 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:59:51.371051 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:51.371401 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:59:51.371579 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:59:51.372474 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:59:51.373109 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:51.373254 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:59:51.373483 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:51.373610 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:51.373734 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:59:51.374198 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:51.374572 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:59:51.374793 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:59:51.374995 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:59:51.375252 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:51.375407 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:59:51.376469 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:59:51.379368 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:59:51.379563 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:59:51.379766 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:59:51.380114 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:59:51.380256 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:59:51.380392 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:59:51.391167 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:59:51.392024 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:59:51.392982 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:59:51.393157 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:59:51.394309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074280b0>]}
[0m11:59:51.394557 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:51.394693 [debug] [MainThread]: On master: BEGIN
[0m11:59:51.394812 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:59:51.395076 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:51.395208 [debug] [MainThread]: On master: COMMIT
[0m11:59:51.395339 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:51.395457 [debug] [MainThread]: On master: COMMIT
[0m11:59:51.395662 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:51.395796 [debug] [MainThread]: On master: Close
[0m11:59:51.397109 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m11:59:51.397298 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m11:59:51.397546 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m11:59:51.397778 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m11:59:51.398008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m11:59:51.398227 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m11:59:51.398400 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m11:59:51.398547 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m11:59:51.402707 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.405330 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m11:59:51.405993 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m11:59:51.406190 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m11:59:51.422049 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m11:59:51.423728 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.424260 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m11:59:51.424478 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m11:59:51.424648 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:59:51.425010 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.425214 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m11:59:51.425367 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:51.425596 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:59:51.425774 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m11:59:51.425985 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m11:59:51.428964 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m11:59:51.429194 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:59:51.429405 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m11:59:51.429563 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:59:51.429752 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.430610 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m11:59:51.434324 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m11:59:51.434518 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m11:59:51.435871 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m11:59:51.436927 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079e9a90>]}
[0m11:59:51.437288 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.04s]
[0m11:59:51.437625 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m11:59:51.437956 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m11:59:51.444649 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m11:59:51.448001 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.448212 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m11:59:51.448905 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:59:51.455212 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m11:59:51.455471 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.455636 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m11:59:51.458207 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:59:51.462741 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m11:59:51.462998 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m11:59:51.463730 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:59:51.464971 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m11:59:51.465298 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043b2f30>]}
[0m11:59:51.465616 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.07s]
[0m11:59:51.465875 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m11:59:51.466226 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m11:59:51.466518 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m11:59:51.466810 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m11:59:51.466985 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m11:59:51.468815 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m11:59:51.469740 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m11:59:51.471427 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m11:59:51.471897 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m11:59:51.472111 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m11:59:51.472277 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:59:51.472639 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:59:51.472794 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m11:59:51.473045 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = add_months(month_date, -1)
            then true
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m11:59:51.476777 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = add_months(month_date, -1)
            then true
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m11:59:51.477271 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:59:51.477572 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: ROLLBACK
[0m11:59:51.479180 [debug] [Thread-4 (]: Failed to rollback 'model.saas_analytics.rpt_mrr_movements'
[0m11:59:51.479374 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m11:59:51.480575 [debug] [Thread-4 (]: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name add_months does not exist!
  Did you mean "add"?
  
  LINE 53:                 and last_month_date = add_months(month_date, -1)
                                                 ^
[0m11:59:51.480814 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7fe8d32-502c-4db0-9682-f7eb86adca76', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a93140>]}
[0m11:59:51.481111 [error] [Thread-4 (]: 3 of 3 ERROR creating sql table model main.rpt_mrr_movements ................... [[31mERROR[0m in 0.01s]
[0m11:59:51.481368 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m11:59:51.481628 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_mrr_movements' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name add_months does not exist!
  Did you mean "add"?
  
  LINE 53:                 and last_month_date = add_months(month_date, -1)
                                                 ^.
[0m11:59:51.482282 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:51.482448 [debug] [MainThread]: On master: BEGIN
[0m11:59:51.482585 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:59:51.482925 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:51.483062 [debug] [MainThread]: On master: COMMIT
[0m11:59:51.483188 [debug] [MainThread]: Using duckdb connection "master"
[0m11:59:51.483302 [debug] [MainThread]: On master: COMMIT
[0m11:59:51.483495 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:59:51.483625 [debug] [MainThread]: On master: Close
[0m11:59:51.483830 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:59:51.483968 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m11:59:51.484086 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m11:59:51.484203 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m11:59:51.484367 [info ] [MainThread]: 
[0m11:59:51.484518 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m11:59:51.484932 [debug] [MainThread]: Command end result
[0m11:59:51.505301 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:59:51.506451 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:59:51.509355 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:59:51.509536 [info ] [MainThread]: 
[0m11:59:51.509712 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:59:51.509857 [info ] [MainThread]: 
[0m11:59:51.510045 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m11:59:51.510212 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m11:59:51.510341 [info ] [MainThread]: 
[0m11:59:51.510497 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m11:59:51.510633 [info ] [MainThread]: 
[0m11:59:51.510800 [error] [MainThread]: [31mFailure in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)[0m
[0m11:59:51.510972 [error] [MainThread]:   Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name add_months does not exist!
  Did you mean "add"?
  
  LINE 53:                 and last_month_date = add_months(month_date, -1)
                                                 ^
[0m11:59:51.511102 [info ] [MainThread]: 
[0m11:59:51.511253 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_mrr_movements.sql
[0m11:59:51.511379 [info ] [MainThread]: 
[0m11:59:51.511518 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m11:59:51.512908 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.88479835, "process_in_blocks": "0", "process_kernel_time": 0.208687, "process_mem_max_rss": "156368896", "process_out_blocks": "0", "process_user_time": 1.328771}
[0m11:59:51.513158 [debug] [MainThread]: Command `dbt run` failed at 11:59:51.513114 after 0.89 seconds
[0m11:59:51.513346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d09880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d097f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d098b0>]}
[0m11:59:51.513530 [debug] [MainThread]: Flushing usage events
[0m11:59:51.614101 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:01:54.651171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105063830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069b5ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069b5880>]}


============================== 12:01:54.673329 | f8305a65-e52a-49a7-aa26-d9f76dca12f6 ==============================
[0m12:01:54.673329 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:01:54.673676 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'version_check': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'printer_width': '80', 'target_path': 'None', 'log_cache_events': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'write_json': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m12:01:54.847039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a34410>]}
[0m12:01:54.877205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10694fe30>]}
[0m12:01:54.878543 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:01:54.933537 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:01:55.015702 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:01:55.016573 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:01:55.251929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10752ed20>]}
[0m12:01:55.303487 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:01:55.305801 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:01:55.322864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107523530>]}
[0m12:01:55.323202 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:01:55.323386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107379400>]}
[0m12:01:55.325661 [info ] [MainThread]: 
[0m12:01:55.325957 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:01:55.326158 [info ] [MainThread]: 
[0m12:01:55.326514 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:01:55.328890 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:01:55.363504 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:01:55.363803 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:01:55.363971 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:01:55.385106 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:01:55.386121 [debug] [ThreadPool]: On list_analytics: Close
[0m12:01:55.386506 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:01:55.386770 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:01:55.390361 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:01:55.390570 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:01:55.390778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:01:55.393000 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m12:01:55.393813 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:01:55.393962 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:01:55.394230 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:01:55.394354 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:01:55.394478 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:01:55.394724 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:01:55.395114 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:01:55.395253 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:01:55.395372 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:01:55.395612 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:01:55.395807 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:01:55.396783 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:01:55.399676 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:01:55.399844 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:01:55.400035 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:01:55.400390 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:01:55.400524 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:01:55.400655 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:01:55.410447 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m12:01:55.411370 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:01:55.412379 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:01:55.412528 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:01:55.413718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10727d190>]}
[0m12:01:55.413995 [debug] [MainThread]: Using duckdb connection "master"
[0m12:01:55.414253 [debug] [MainThread]: On master: BEGIN
[0m12:01:55.414399 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:01:55.414932 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m12:01:55.415416 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.415561 [debug] [MainThread]: Using duckdb connection "master"
[0m12:01:55.415691 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.415928 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:01:55.416075 [debug] [MainThread]: On master: Close
[0m12:01:55.417740 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:01:55.417943 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:01:55.418219 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:01:55.418621 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:01:55.418897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:01:55.419153 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:01:55.419340 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:01:55.419502 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:01:55.423583 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.428397 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:01:55.433926 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:01:55.434166 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:01:55.452525 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:01:55.452281 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.453402 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.453578 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:01:55.453730 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:01:55.454029 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:01:55.454202 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:01:55.454350 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:01:55.454562 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:01:55.454722 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.454873 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:01:55.455066 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:01:55.455258 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:01:55.455627 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:01:55.458886 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:01:55.459134 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m12:01:55.459359 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m12:01:55.464058 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m12:01:55.464299 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:01:55.465790 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:01:55.466742 [debug] [Thread-1 (]: SQL status: OK in 0.011 seconds
[0m12:01:55.466943 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502f890>]}
[0m12:01:55.469725 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.470240 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:01:55.470076 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.05s]
[0m12:01:55.470631 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:01:55.470883 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m12:01:55.471058 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:01:55.471995 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.472154 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:01:55.472631 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:01:55.475950 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.476165 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:01:55.476836 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:01:55.478384 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.478570 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:01:55.478993 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:01:55.486221 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:01:55.486468 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.486631 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:01:55.488624 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:01:55.491318 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:01:55.491507 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:01:55.492312 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:01:55.493560 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:01:55.493838 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107962ea0>]}
[0m12:01:55.494125 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.07s]
[0m12:01:55.494382 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:01:55.494709 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:01:55.494990 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:01:55.495260 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:01:55.495432 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:01:55.497141 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:01:55.498036 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:01:55.500531 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:01:55.501226 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:01:55.501411 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:01:55.501563 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:01:55.501953 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:01:55.502130 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:01:55.502383 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = dateadd(month', -1, month_date)
            then true
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:01:55.502951 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = dateadd(month', -1, month_date)
            then true
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:01:55.503202 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m12:01:55.503392 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: ROLLBACK
[0m12:01:55.504752 [debug] [Thread-4 (]: Failed to rollback 'model.saas_analytics.rpt_mrr_movements'
[0m12:01:55.504902 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:01:55.505999 [debug] [Thread-4 (]: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Parser Error: syntax error at or near "new"
  
  LINE 65:         count(distinct case when movement_type = 'new' then user_id end) as new_customers,
                                                             ^
[0m12:01:55.506216 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8305a65-e52a-49a7-aa26-d9f76dca12f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d52cf0>]}
[0m12:01:55.506496 [error] [Thread-4 (]: 3 of 3 ERROR creating sql table model main.rpt_mrr_movements ................... [[31mERROR[0m in 0.01s]
[0m12:01:55.506737 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:01:55.506964 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_mrr_movements' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Parser Error: syntax error at or near "new"
  
  LINE 65:         count(distinct case when movement_type = 'new' then user_id end) as new_customers,
                                                             ^.
[0m12:01:55.507548 [debug] [MainThread]: Using duckdb connection "master"
[0m12:01:55.507692 [debug] [MainThread]: On master: BEGIN
[0m12:01:55.507807 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:01:55.508047 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:01:55.508175 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.508313 [debug] [MainThread]: Using duckdb connection "master"
[0m12:01:55.508457 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.508756 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:01:55.508905 [debug] [MainThread]: On master: Close
[0m12:01:55.509090 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:01:55.509216 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:01:55.509333 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:01:55.509447 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:01:55.509624 [info ] [MainThread]: 
[0m12:01:55.509776 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m12:01:55.510199 [debug] [MainThread]: Command end result
[0m12:01:55.530217 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:01:55.531419 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:01:55.538970 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:01:55.539190 [info ] [MainThread]: 
[0m12:01:55.539373 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:01:55.539521 [info ] [MainThread]: 
[0m12:01:55.539705 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m12:01:55.539868 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:01:55.539991 [info ] [MainThread]: 
[0m12:01:55.540139 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m12:01:55.540266 [info ] [MainThread]: 
[0m12:01:55.540412 [error] [MainThread]: [31mFailure in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)[0m
[0m12:01:55.540567 [error] [MainThread]:   Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Parser Error: syntax error at or near "new"
  
  LINE 65:         count(distinct case when movement_type = 'new' then user_id end) as new_customers,
                                                             ^
[0m12:01:55.540691 [info ] [MainThread]: 
[0m12:01:55.540832 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_mrr_movements.sql
[0m12:01:55.540949 [info ] [MainThread]: 
[0m12:01:55.541087 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m12:01:55.542512 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9290791, "process_in_blocks": "0", "process_kernel_time": 0.216866, "process_mem_max_rss": "157171712", "process_out_blocks": "0", "process_user_time": 1.346139}
[0m12:01:55.542724 [debug] [MainThread]: Command `dbt run` failed at 12:01:55.542686 after 0.93 seconds
[0m12:01:55.542895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069b5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10744bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105060a40>]}
[0m12:01:55.543054 [debug] [MainThread]: Flushing usage events
[0m12:01:55.668145 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:03:01.691037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bfdbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107449a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107449820>]}


============================== 12:03:01.696102 | 6d01bc06-ca79-4f8b-b91a-98edd4de7d2d ==============================
[0m12:03:01.696102 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:03:01.696484 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'fail_fast': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'introspect': 'True', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'quiet': 'False', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'False', 'no_print': 'None'}
[0m12:03:01.865032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104af3ef0>]}
[0m12:03:01.893890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107578740>]}
[0m12:03:01.895476 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:03:01.948803 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:03:02.022592 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:03:02.023068 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:03:02.253448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ccb2f0>]}
[0m12:03:02.301639 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:03:02.303010 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:03:02.319341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107caf020>]}
[0m12:03:02.319658 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:03:02.319838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b68290>]}
[0m12:03:02.321964 [info ] [MainThread]: 
[0m12:03:02.322153 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:03:02.322295 [info ] [MainThread]: 
[0m12:03:02.322526 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:03:02.324733 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:03:02.355094 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:03:02.355339 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:03:02.355486 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:03:02.378851 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m12:03:02.379907 [debug] [ThreadPool]: On list_analytics: Close
[0m12:03:02.380271 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:03:02.380532 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:03:02.383979 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:02.384159 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:03:02.384295 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:03:02.385203 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:03:02.386038 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:02.386179 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:03:02.386407 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:02.386537 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:02.386662 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:03:02.387084 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:02.387460 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:03:02.387593 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:02.387710 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:03:02.387904 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:02.388031 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:03:02.388949 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:03:02.391599 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:03:02.391757 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:03:02.392003 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:03:02.392300 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:02.392442 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:03:02.392584 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:03:02.403450 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m12:03:02.404244 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:03:02.405203 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:03:02.405353 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:03:02.406490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107caca40>]}
[0m12:03:02.406750 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:02.406900 [debug] [MainThread]: On master: BEGIN
[0m12:03:02.407030 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:03:02.407303 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:02.407447 [debug] [MainThread]: On master: COMMIT
[0m12:03:02.407580 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:02.407699 [debug] [MainThread]: On master: COMMIT
[0m12:03:02.407902 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:02.408030 [debug] [MainThread]: On master: Close
[0m12:03:02.409384 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:03:02.409563 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:03:02.409795 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:03:02.410197 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:03:02.409991 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:03:02.410405 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:03:02.410620 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:03:02.414533 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.414740 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:03:02.417409 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:03:02.417979 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:03:02.418164 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:03:02.439318 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.439701 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:03:02.440261 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.440445 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:03:02.440629 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:03:02.440810 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:03:02.440977 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:03:02.441286 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:03:02.441593 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:03:02.441798 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.441975 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:03:02.442180 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:03:02.442389 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:03:02.442777 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:03:02.446066 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:03:02.446336 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m12:03:02.446589 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m12:03:02.451258 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m12:03:02.451487 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:03:02.452836 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:03:02.453860 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f6f020>]}
[0m12:03:02.454258 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.04s]
[0m12:03:02.454567 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:03:02.454822 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m12:03:02.456819 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m12:03:02.460066 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.460323 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:03:02.460878 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:02.461637 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.461830 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:03:02.462455 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:02.465828 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.466032 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:03:02.467012 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:03:02.468547 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.468735 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:03:02.469054 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:02.476865 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:03:02.477148 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.477317 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:03:02.479292 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:03:02.482327 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:02.482529 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:03:02.483306 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:03:02.484580 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:03:02.485344 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080b2720>]}
[0m12:03:02.485677 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.08s]
[0m12:03:02.485950 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:03:02.486284 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:03:02.486556 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:03:02.486841 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:03:02.487011 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:03:02.488877 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:03:02.489860 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:03:02.492533 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:03:02.493573 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:03:02.493843 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:03:02.493990 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:03:02.494363 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:03:02.494513 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:03:02.494759 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = dateadd(month, -1, month_date)
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:03:02.499290 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = dateadd(month, -1, month_date)
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:03:02.499584 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m12:03:02.499925 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: ROLLBACK
[0m12:03:02.501393 [debug] [Thread-4 (]: Failed to rollback 'model.saas_analytics.rpt_mrr_movements'
[0m12:03:02.501563 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:03:02.502732 [debug] [Thread-4 (]: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name dateadd does not exist!
  Did you mean "date_add"?
  
  LINE 53:                 and last_month_date = dateadd(month, -1, month_date)
                                                 ^
[0m12:03:02.502960 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d01bc06-ca79-4f8b-b91a-98edd4de7d2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081ea960>]}
[0m12:03:02.503253 [error] [Thread-4 (]: 3 of 3 ERROR creating sql table model main.rpt_mrr_movements ................... [[31mERROR[0m in 0.02s]
[0m12:03:02.503499 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:03:02.503725 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_mrr_movements' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name dateadd does not exist!
  Did you mean "date_add"?
  
  LINE 53:                 and last_month_date = dateadd(month, -1, month_date)
                                                 ^.
[0m12:03:02.504328 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:02.504469 [debug] [MainThread]: On master: BEGIN
[0m12:03:02.504592 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:03:02.504885 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:02.505013 [debug] [MainThread]: On master: COMMIT
[0m12:03:02.505133 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:02.505240 [debug] [MainThread]: On master: COMMIT
[0m12:03:02.505428 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:02.505548 [debug] [MainThread]: On master: Close
[0m12:03:02.505705 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:03:02.505817 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:03:02.505919 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:03:02.506023 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:03:02.506177 [info ] [MainThread]: 
[0m12:03:02.506322 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m12:03:02.506724 [debug] [MainThread]: Command end result
[0m12:03:02.527856 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:03:02.529599 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:03:02.535162 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:03:02.535395 [info ] [MainThread]: 
[0m12:03:02.535593 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:03:02.535751 [info ] [MainThread]: 
[0m12:03:02.535933 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m12:03:02.536108 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:03:02.536241 [info ] [MainThread]: 
[0m12:03:02.536393 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m12:03:02.536523 [info ] [MainThread]: 
[0m12:03:02.536678 [error] [MainThread]: [31mFailure in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)[0m
[0m12:03:02.536835 [error] [MainThread]:   Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Catalog Error: Scalar Function with name dateadd does not exist!
  Did you mean "date_add"?
  
  LINE 53:                 and last_month_date = dateadd(month, -1, month_date)
                                                 ^
[0m12:03:02.536959 [info ] [MainThread]: 
[0m12:03:02.537100 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_mrr_movements.sql
[0m12:03:02.537223 [info ] [MainThread]: 
[0m12:03:02.537370 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m12:03:02.538558 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8861586, "process_in_blocks": "0", "process_kernel_time": 0.200995, "process_mem_max_rss": "160137216", "process_out_blocks": "0", "process_user_time": 1.324559}
[0m12:03:02.538815 [debug] [MainThread]: Command `dbt run` failed at 12:03:02.538768 after 0.89 seconds
[0m12:03:02.539016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074498e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10672ae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b73320>]}
[0m12:03:02.539201 [debug] [MainThread]: Flushing usage events
[0m12:03:02.654178 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:03:14.942756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ccb7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105381b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105381910>]}


============================== 12:03:14.947616 | 8d3ba259-b9ef-4b8b-a04f-697eb0a1032a ==============================
[0m12:03:14.947616 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:03:14.948201 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'empty': 'False', 'version_check': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'warn_error': 'None', 'log_format': 'default', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'use_colors': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m12:03:15.099090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105400410>]}
[0m12:03:15.127847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10523e000>]}
[0m12:03:15.129878 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:03:15.182026 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:03:15.254830 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:03:15.255283 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:03:15.478837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c04350>]}
[0m12:03:15.524595 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:03:15.526017 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:03:15.540140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c0d280>]}
[0m12:03:15.540444 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:03:15.540627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b97dd0>]}
[0m12:03:15.542828 [info ] [MainThread]: 
[0m12:03:15.543033 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:03:15.543183 [info ] [MainThread]: 
[0m12:03:15.543417 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:03:15.545560 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:03:15.573520 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:03:15.573766 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:03:15.573917 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:03:15.588137 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m12:03:15.588932 [debug] [ThreadPool]: On list_analytics: Close
[0m12:03:15.589289 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:03:15.589544 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:03:15.593301 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:15.593553 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:03:15.593704 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:03:15.594560 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:03:15.595312 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:15.595472 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:03:15.595729 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:15.595857 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:15.595984 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:03:15.596244 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:15.596643 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:03:15.596787 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:03:15.596909 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:03:15.597122 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:15.597251 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:03:15.598511 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:03:15.601595 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:03:15.601822 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:03:15.602032 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:03:15.602377 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:03:15.602513 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:03:15.602645 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:03:15.612494 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m12:03:15.613545 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:03:15.614534 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:03:15.614689 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:03:15.615866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105aa1190>]}
[0m12:03:15.616146 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:15.616296 [debug] [MainThread]: On master: BEGIN
[0m12:03:15.616425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:03:15.616692 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:15.616828 [debug] [MainThread]: On master: COMMIT
[0m12:03:15.616953 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:15.617074 [debug] [MainThread]: On master: COMMIT
[0m12:03:15.617274 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:15.617397 [debug] [MainThread]: On master: Close
[0m12:03:15.618724 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:03:15.618901 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:03:15.619133 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:03:15.619377 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:03:15.619597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:03:15.619820 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:03:15.619989 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:03:15.620157 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:03:15.623987 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.626624 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:03:15.627207 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:03:15.627397 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:03:15.644033 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:03:15.646954 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.647566 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:03:15.647751 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:03:15.647904 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:03:15.648212 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.648392 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:03:15.648537 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:03:15.648792 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:03:15.648940 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:03:15.649141 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:03:15.651880 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:03:15.652117 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m12:03:15.652265 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m12:03:15.652473 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m12:03:15.652634 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.653498 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:03:15.657517 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m12:03:15.657726 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:03:15.659115 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:03:15.660436 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fe67e0>]}
[0m12:03:15.660804 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.04s]
[0m12:03:15.661106 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:03:15.661386 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m12:03:15.667662 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m12:03:15.670749 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.670953 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:03:15.671483 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:15.672111 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.672276 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:03:15.673401 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:03:15.676974 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.677262 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:03:15.678060 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:03:15.679817 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.680001 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:03:15.680378 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:15.687926 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:03:15.688178 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.688337 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:03:15.690300 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:03:15.693156 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:03:15.693353 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:03:15.693872 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:03:15.695075 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:03:15.695377 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a2eea0>]}
[0m12:03:15.695681 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.08s]
[0m12:03:15.696325 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:03:15.696682 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:03:15.696939 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:03:15.697239 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:03:15.697476 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:03:15.699411 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:03:15.700313 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:03:15.702907 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:03:15.704127 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:03:15.704294 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:03:15.704440 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:03:15.704786 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:03:15.704937 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:03:15.705184 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = date_add(month, -1, month_date)
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:03:15.707444 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = date_add(month, -1, month_date)
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:03:15.707733 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m12:03:15.707940 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: ROLLBACK
[0m12:03:15.709618 [debug] [Thread-4 (]: Failed to rollback 'model.saas_analytics.rpt_mrr_movements'
[0m12:03:15.709818 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:03:15.711448 [debug] [Thread-4 (]: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Macro date_add() does not support the supplied arguments. You might need to add explicit type casts.
  Candidate macros:
  	date_add(date, interval)
  
  LINE 53:                 and last_month_date = date_add(month, -1, month_date)
                                                 ^
[0m12:03:15.711797 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8d3ba259-b9ef-4b8b-a04f-697eb0a1032a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106120a70>]}
[0m12:03:15.712131 [error] [Thread-4 (]: 3 of 3 ERROR creating sql table model main.rpt_mrr_movements ................... [[31mERROR[0m in 0.01s]
[0m12:03:15.712392 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:03:15.712637 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_mrr_movements' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Macro date_add() does not support the supplied arguments. You might need to add explicit type casts.
  Candidate macros:
  	date_add(date, interval)
  
  LINE 53:                 and last_month_date = date_add(month, -1, month_date)
                                                 ^.
[0m12:03:15.713289 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:15.713437 [debug] [MainThread]: On master: BEGIN
[0m12:03:15.713553 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:03:15.713877 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:15.714004 [debug] [MainThread]: On master: COMMIT
[0m12:03:15.714127 [debug] [MainThread]: Using duckdb connection "master"
[0m12:03:15.714240 [debug] [MainThread]: On master: COMMIT
[0m12:03:15.714447 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:03:15.714572 [debug] [MainThread]: On master: Close
[0m12:03:15.714734 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:03:15.714847 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:03:15.714949 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:03:15.715050 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:03:15.715196 [info ] [MainThread]: 
[0m12:03:15.715330 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m12:03:15.715725 [debug] [MainThread]: Command end result
[0m12:03:15.735782 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:03:15.736928 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:03:15.739664 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:03:15.739825 [info ] [MainThread]: 
[0m12:03:15.739998 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:03:15.740141 [info ] [MainThread]: 
[0m12:03:15.740320 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m12:03:15.740488 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:03:15.740623 [info ] [MainThread]: 
[0m12:03:15.740772 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m12:03:15.740906 [info ] [MainThread]: 
[0m12:03:15.741057 [error] [MainThread]: [31mFailure in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)[0m
[0m12:03:15.741216 [error] [MainThread]:   Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Macro date_add() does not support the supplied arguments. You might need to add explicit type casts.
  Candidate macros:
  	date_add(date, interval)
  
  LINE 53:                 and last_month_date = date_add(month, -1, month_date)
                                                 ^
[0m12:03:15.741347 [info ] [MainThread]: 
[0m12:03:15.741489 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_mrr_movements.sql
[0m12:03:15.741609 [info ] [MainThread]: 
[0m12:03:15.741749 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m12:03:15.743001 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.84064174, "process_in_blocks": "0", "process_kernel_time": 0.207487, "process_mem_max_rss": "160432128", "process_out_blocks": "0", "process_user_time": 1.321193}
[0m12:03:15.743264 [debug] [MainThread]: Command `dbt run` failed at 12:03:15.743218 after 0.84 seconds
[0m12:03:15.743464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046601a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104662cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b62fc0>]}
[0m12:03:15.743646 [debug] [MainThread]: Flushing usage events
[0m12:03:15.883203 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:05:14.684697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108143920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089fda90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089fd760>]}


============================== 12:05:14.708496 | 455b068c-4bde-4833-a19a-e58ed495c734 ==============================
[0m12:05:14.708496 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:05:14.708884 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select path:models/marts/revenue', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'quiet': 'False', 'warn_error': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'no_print': 'None', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_cache_events': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'introspect': 'True', 'write_json': 'True', 'cache_selected_only': 'False'}
[0m12:05:14.891472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bb54f0>]}
[0m12:05:14.922910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106231b80>]}
[0m12:05:14.924276 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:05:14.976018 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:05:15.051564 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:05:15.052025 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:05:15.278690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a3320>]}
[0m12:05:15.326453 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:05:15.328243 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:05:15.343035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109269be0>]}
[0m12:05:15.343312 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:05:15.343488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091e6a80>]}
[0m12:05:15.345550 [info ] [MainThread]: 
[0m12:05:15.345727 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:05:15.345866 [info ] [MainThread]: 
[0m12:05:15.346095 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:05:15.348256 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:05:15.377766 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:05:15.378014 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:05:15.378168 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:05:15.398183 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m12:05:15.399173 [debug] [ThreadPool]: On list_analytics: Close
[0m12:05:15.399521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:05:15.399778 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:05:15.403232 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:15.403410 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:05:15.403546 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:05:15.404240 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:05:15.404853 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:15.404992 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:05:15.405214 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:15.405339 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:15.405464 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:05:15.405850 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:15.406232 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:05:15.406364 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:15.406484 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:05:15.406681 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:15.406806 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:05:15.407711 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:05:15.410332 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:05:15.410504 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:05:15.410691 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:05:15.410960 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:15.411096 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:05:15.411231 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:05:15.422076 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m12:05:15.422897 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:05:15.423873 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:05:15.424033 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:05:15.425175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109300710>]}
[0m12:05:15.425426 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:15.425585 [debug] [MainThread]: On master: BEGIN
[0m12:05:15.425761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:15.426035 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:15.426176 [debug] [MainThread]: On master: COMMIT
[0m12:05:15.426310 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:15.426433 [debug] [MainThread]: On master: COMMIT
[0m12:05:15.426634 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:15.426763 [debug] [MainThread]: On master: Close
[0m12:05:15.428174 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:05:15.428362 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:05:15.428603 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:05:15.428854 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:05:15.429076 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:05:15.429301 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:05:15.429466 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:05:15.429618 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:05:15.433307 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.435997 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:05:15.436962 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:05:15.453237 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.453624 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:05:15.455364 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:05:15.455685 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.455877 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:05:15.456029 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:15.456358 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:05:15.456537 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:05:15.456697 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:15.456840 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:05:15.456989 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.457237 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:05:15.457608 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:05:15.457756 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:05:15.457959 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:05:15.461159 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:05:15.461443 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m12:05:15.461672 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m12:05:15.466653 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m12:05:15.466903 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:05:15.468298 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:05:15.469350 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c771d0>]}
[0m12:05:15.469684 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.04s]
[0m12:05:15.469966 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:05:15.470223 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^.
[0m12:05:15.472045 [debug] [Thread-1 (]: SQL status: OK in 0.015 seconds
[0m12:05:15.474723 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.474917 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:05:15.475430 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.476102 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.476339 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:05:15.477160 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:15.480372 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.480563 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:05:15.481368 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:15.482783 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.482951 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:05:15.483239 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.490191 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:05:15.490380 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.490531 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:05:15.492587 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:05:15.495857 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:15.496127 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:05:15.497123 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:15.498321 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:05:15.498628 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109662ff0>]}
[0m12:05:15.498946 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.07s]
[0m12:05:15.499204 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:05:15.499533 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:05:15.499821 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:05:15.500107 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:05:15.500279 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:05:15.501975 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.502812 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:05:15.504366 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.504960 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.505135 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:05:15.505328 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:05:15.505723 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.505887 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.506144 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:05:15.526129 [debug] [Thread-4 (]: SQL status: OK in 0.020 seconds
[0m12:05:15.528888 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.529126 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:05:15.529531 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.530229 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:05:15.530388 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.530533 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:05:15.531020 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.532206 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:15.532384 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:05:15.532677 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:15.533338 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:05:15.533630 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '455b068c-4bde-4833-a19a-e58ed495c734', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097a07a0>]}
[0m12:05:15.533938 [info ] [Thread-4 (]: 3 of 3 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.03s]
[0m12:05:15.534198 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:05:15.534878 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:15.535069 [debug] [MainThread]: On master: BEGIN
[0m12:05:15.535209 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:05:15.535509 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:15.535647 [debug] [MainThread]: On master: COMMIT
[0m12:05:15.535778 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:15.535902 [debug] [MainThread]: On master: COMMIT
[0m12:05:15.536107 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:15.536235 [debug] [MainThread]: On master: Close
[0m12:05:15.536402 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:05:15.536521 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:05:15.536634 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:05:15.536742 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:05:15.536907 [info ] [MainThread]: 
[0m12:05:15.537060 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m12:05:15.537480 [debug] [MainThread]: Command end result
[0m12:05:15.562513 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:05:15.563644 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:05:15.566598 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:05:15.566760 [info ] [MainThread]: 
[0m12:05:15.566931 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:05:15.567070 [info ] [MainThread]: 
[0m12:05:15.567243 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m12:05:15.567409 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Parser Error: syntax error at or near "u"
  
  LINE 91:         u.user_id = ltv.user_id 
                   ^
[0m12:05:15.567533 [info ] [MainThread]: 
[0m12:05:15.567684 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m12:05:15.567813 [info ] [MainThread]: 
[0m12:05:15.567952 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m12:05:15.568973 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9227995, "process_in_blocks": "0", "process_kernel_time": 0.221767, "process_mem_max_rss": "219250688", "process_out_blocks": "0", "process_user_time": 1.339677}
[0m12:05:15.569182 [debug] [MainThread]: Command `dbt run` failed at 12:05:15.569143 after 0.92 seconds
[0m12:05:15.569361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089fd910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c771d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108143920>]}
[0m12:05:15.569528 [debug] [MainThread]: Flushing usage events
[0m12:05:15.699547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:05:44.236554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105effc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b5a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b5730>]}


============================== 12:05:44.243198 | e0bb9544-9d18-4e08-9359-9787ae0035c0 ==============================
[0m12:05:44.243198 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:05:44.243559 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'use_colors': 'True', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'debug': 'False', 'no_print': 'None', 'static_parser': 'True', 'partial_parse': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'version_check': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'fail_fast': 'False', 'introspect': 'True', 'empty': 'False', 'write_json': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'quiet': 'False', 'warn_error': 'None', 'use_experimental_parser': 'False'}
[0m12:05:44.406006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f0a960>]}
[0m12:05:44.434843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106eff6b0>]}
[0m12:05:44.436188 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:05:44.489090 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:05:44.561602 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:05:44.562077 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_customer_ltv.sql
[0m12:05:44.787219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f39460>]}
[0m12:05:44.833657 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:05:44.835335 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:05:44.849633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f1f050>]}
[0m12:05:44.849896 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:05:44.850074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e25af0>]}
[0m12:05:44.852121 [info ] [MainThread]: 
[0m12:05:44.852305 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:05:44.852444 [info ] [MainThread]: 
[0m12:05:44.852676 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:05:44.854846 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:05:44.884679 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:05:44.884912 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:05:44.885055 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:05:44.906167 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:05:44.908735 [debug] [ThreadPool]: On list_analytics: Close
[0m12:05:44.909168 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:05:44.909450 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:05:44.912850 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:44.913026 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:05:44.913160 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:05:44.914034 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:05:44.914675 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:44.914814 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:05:44.915035 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:44.915162 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:44.915285 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:05:44.916207 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:05:44.916618 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:05:44.917914 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:05:44.918059 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:05:44.918305 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:44.918475 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:05:44.919417 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:05:44.922059 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:05:44.922217 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:05:44.922395 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:05:44.922639 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:05:44.922770 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:05:44.922902 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:05:44.935566 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m12:05:44.936471 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:05:44.937449 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:05:44.937607 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:05:44.938805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f9b5c0>]}
[0m12:05:44.939070 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:44.939209 [debug] [MainThread]: On master: BEGIN
[0m12:05:44.939331 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:44.939637 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:44.939794 [debug] [MainThread]: On master: COMMIT
[0m12:05:44.939929 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:44.940049 [debug] [MainThread]: On master: COMMIT
[0m12:05:44.940263 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:44.940397 [debug] [MainThread]: On master: Close
[0m12:05:44.942052 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:05:44.942235 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:05:44.942484 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:05:44.942737 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:05:44.942955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:05:44.943177 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:05:44.943338 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:05:44.943491 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:05:44.947220 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.949782 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:05:44.950407 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:05:44.950610 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:05:44.971669 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:05:44.972085 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.972462 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:05:44.972636 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:05:44.972785 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:05:44.973119 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.973292 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:05:44.973443 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:44.973598 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:05:44.973803 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:05:44.974015 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        on u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:05:44.974225 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:44.974586 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.974771 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:05:44.979553 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscription as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        on u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:05:44.981978 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m12:05:44.982248 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: ROLLBACK
[0m12:05:44.987876 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_customer_ltv'
[0m12:05:44.988125 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:05:44.989506 [debug] [Thread-2 (]: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Catalog Error: Table with name subscriptions does not exist!
  Did you mean "raw_subscriptions"?
  
  LINE 42:     from subscriptions s 
                    ^
[0m12:05:44.990583 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113a0c80>]}
[0m12:05:44.990930 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_customer_ltv .................... [[31mERROR[0m in 0.05s]
[0m12:05:44.991232 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:05:44.991483 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_customer_ltv' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Catalog Error: Table with name subscriptions does not exist!
  Did you mean "raw_subscriptions"?
  
  LINE 42:     from subscriptions s 
                    ^.
[0m12:05:44.991681 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m12:05:44.994920 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.995137 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:05:44.995686 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:05:44.996327 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:44.996502 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:05:44.997158 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.000388 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:45.000601 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:05:45.002098 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:45.003468 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:45.003639 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:05:45.003926 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.011214 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:05:45.011523 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:45.011763 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:05:45.013557 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:05:45.016328 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:05:45.016516 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:05:45.017374 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:05:45.018538 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:05:45.018824 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ca3830>]}
[0m12:05:45.019116 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.08s]
[0m12:05:45.019372 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:05:45.019695 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:05:45.019954 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:05:45.020216 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:05:45.020378 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:05:45.021968 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.022600 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:05:45.024995 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.025627 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.025871 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:05:45.026038 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:05:45.026429 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.026590 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.026853 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:05:45.048356 [debug] [Thread-4 (]: SQL status: OK in 0.021 seconds
[0m12:05:45.049060 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.049235 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:05:45.049710 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.050285 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.050444 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:05:45.050873 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.052494 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.052671 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements" rename to "rpt_mrr_movements__dbt_backup"
[0m12:05:45.052980 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.054306 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.054462 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:05:45.054734 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.055495 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:05:45.055648 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.055785 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:05:45.056238 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.057356 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:05:45.057540 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:05:45.058026 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:05:45.058681 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:05:45.058964 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0bb9544-9d18-4e08-9359-9787ae0035c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111656510>]}
[0m12:05:45.059544 [info ] [Thread-4 (]: 3 of 3 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.04s]
[0m12:05:45.059904 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:05:45.060882 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:45.061327 [debug] [MainThread]: On master: BEGIN
[0m12:05:45.061454 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:05:45.061791 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:45.061922 [debug] [MainThread]: On master: COMMIT
[0m12:05:45.062047 [debug] [MainThread]: Using duckdb connection "master"
[0m12:05:45.062166 [debug] [MainThread]: On master: COMMIT
[0m12:05:45.066162 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:05:45.066379 [debug] [MainThread]: On master: Close
[0m12:05:45.066631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:05:45.066781 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:05:45.066898 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:05:45.067011 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:05:45.067192 [info ] [MainThread]: 
[0m12:05:45.067347 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m12:05:45.067776 [debug] [MainThread]: Command end result
[0m12:05:45.088517 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:05:45.089669 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:05:45.092353 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:05:45.092550 [info ] [MainThread]: 
[0m12:05:45.092752 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:05:45.092914 [info ] [MainThread]: 
[0m12:05:45.093099 [error] [MainThread]: [31mFailure in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)[0m
[0m12:05:45.093270 [error] [MainThread]:   Runtime Error in model rpt_customer_ltv (models/marts/revenue/rpt_customer_ltv.sql)
  Catalog Error: Table with name subscriptions does not exist!
  Did you mean "raw_subscriptions"?
  
  LINE 42:     from subscriptions s 
                    ^
[0m12:05:45.093399 [info ] [MainThread]: 
[0m12:05:45.093548 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_customer_ltv.sql
[0m12:05:45.093682 [info ] [MainThread]: 
[0m12:05:45.093825 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m12:05:45.095656 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.90164363, "process_in_blocks": "0", "process_kernel_time": 0.226918, "process_mem_max_rss": "214908928", "process_out_blocks": "0", "process_user_time": 1.364931}
[0m12:05:45.095913 [debug] [MainThread]: Command `dbt run` failed at 12:05:45.095865 after 0.90 seconds
[0m12:05:45.096101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b58b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b5a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104270ad0>]}
[0m12:05:45.096276 [debug] [MainThread]: Flushing usage events
[0m12:05:45.228303 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:06:09.714157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085ab980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c61a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c61820>]}


============================== 12:06:09.733843 | 9b0f3283-f884-4d10-975a-8e8cbecbcefe ==============================
[0m12:06:09.733843 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:06:09.734300 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'warn_error': 'None', 'introspect': 'True', 'static_parser': 'True', 'empty': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'partial_parse': 'True', 'quiet': 'False', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'debug': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select path:models/marts/revenue', 'fail_fast': 'False'}
[0m12:06:09.908606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d903b0>]}
[0m12:06:09.938307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10879bce0>]}
[0m12:06:09.939711 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:06:09.992365 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:06:10.073035 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:06:10.073533 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_customer_ltv.sql
[0m12:06:10.298433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094c2ed0>]}
[0m12:06:10.345171 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:06:10.348112 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:06:10.363823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109364f80>]}
[0m12:06:10.364092 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:06:10.364270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093838f0>]}
[0m12:06:10.366369 [info ] [MainThread]: 
[0m12:06:10.366558 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:06:10.366703 [info ] [MainThread]: 
[0m12:06:10.366946 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:06:10.369128 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:06:10.401039 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:06:10.401291 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:06:10.401890 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:06:10.421508 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m12:06:10.422540 [debug] [ThreadPool]: On list_analytics: Close
[0m12:06:10.422903 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:06:10.423148 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:06:10.426595 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:06:10.426774 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:06:10.426912 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:06:10.427776 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:06:10.428443 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:06:10.428591 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:06:10.428820 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:06:10.428947 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:06:10.429072 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:06:10.429542 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:06:10.429927 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:06:10.430076 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:06:10.430204 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:06:10.430411 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:06:10.430541 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:06:10.431481 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:06:10.434205 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:06:10.434365 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:06:10.434545 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:06:10.434794 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:06:10.434924 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:06:10.435058 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:06:10.446170 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m12:06:10.446939 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:06:10.447913 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:06:10.448078 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:06:10.449322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093d1670>]}
[0m12:06:10.449559 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:10.449691 [debug] [MainThread]: On master: BEGIN
[0m12:06:10.449808 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:06:10.450068 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:10.450202 [debug] [MainThread]: On master: COMMIT
[0m12:06:10.450327 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:10.450446 [debug] [MainThread]: On master: COMMIT
[0m12:06:10.450640 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:10.450767 [debug] [MainThread]: On master: Close
[0m12:06:10.452259 [debug] [Thread-1 (]: Began running node model.saas_analytics.fct_mrr_by_month
[0m12:06:10.452448 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_customer_ltv
[0m12:06:10.452683 [info ] [Thread-1 (]: 1 of 3 START sql table model main.fct_mrr_by_month ............................. [RUN]
[0m12:06:10.453111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.fct_mrr_by_month)
[0m12:06:10.452896 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_customer_ltv ............................. [RUN]
[0m12:06:10.453317 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.fct_mrr_by_month
[0m12:06:10.453526 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_customer_ltv'
[0m12:06:10.457317 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.457529 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_customer_ltv
[0m12:06:10.460286 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.461130 [debug] [Thread-1 (]: Began executing node model.saas_analytics.fct_mrr_by_month
[0m12:06:10.461373 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_customer_ltv
[0m12:06:10.479087 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.479859 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.480342 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.480517 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: BEGIN
[0m12:06:10.480677 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:06:10.480986 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.481172 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: BEGIN
[0m12:06:10.481322 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:06:10.481577 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:10.481730 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.481914 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

  
    
    

    create  table
      "analytics"."main"."fct_mrr_by_month__dbt_tmp"
  
    as (
      

with subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--generate a date spine for all months we have data
date_spine as (
    select distinct 
        date_trunc('month', subscription_started_at) as month_date
    from subscriptions 

    union 

    select distinct 
        date_trunc('month', subscription_ended_at) as month_date 
    from subscriptions 
    where subscription_ended_at is not null
),

--for each subscription, determine which months it was active
subscription_months as (
    select
        s.subscription_id,
        s.user_id,
        s.plan,
        s.monthly_revenue_usd,
        s.subscription_started_at,
        s.subscription_ended_at,
        d.month_date 
    from subscriptions s
    cross join date_spine d 
    where d.month_date >= date_trunc('month', s.subscription_started_at)
        and (
            s.subscription_ended_at is null 
            or d.month_date < date_trunc('month', s.subscription_ended_at)
        )
        and s.plan != 'free' --excluding free tier
),

final as (
    select 
        month_date,
        user_id,
        subscription_id,
        plan,
        monthly_revenue_usd as mrr,

        --subscription age in months
        datediff('month', date_trunc('month', subscription_started_at), month_date) as subscription_age_months,

        --flags
        date_trunc('month', subscription_started_at) = month_date as is_first_month,
        subscription_ended_at is null as is_active_subscription
    from subscription_months
)

select * from final
order by month_date, user_id
    );
  
  
[0m12:06:10.482270 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:06:10.482420 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.482622 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

  
    
    

    create  table
      "analytics"."main"."rpt_customer_ltv__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

subscriptions as (
    select * from "analytics"."main"."stg_subscriptions"
),

--calculate lifetime metrics per user
user_lifetime_metrics as (
    select 
        s.user_id,
        min(s.subscription_started_at) as first_subscription_date,
        max(s.subscription_ended_at) as last_subscription_end_date,

        --lifetime in months (for currently active users, use current date)
        case 
            when max(s.is_active) = true then datediff('month', min(s.subscription_started_at), current_date)
            else datediff('month', min(s.subscription_started_at), max(s.subscription_ended_at))
        end as lifetime_months,
        count(distinct s.subscription_id) as total_subscriptions,

        --revenue metrics
        sum(s.monthly_revenue_usd) as total_revenue,
        avg(s.monthly_revenue_usd) as avg_monthly_revenue,

        --current status
        max(s.is_active) as is_currently_active,
        max(s.is_churned) as has_churned
    from subscriptions s 
    where s.plan != 'free'
    group by s.user_id 
),

--calculate LTV
user_ltv as (
    select
        ulm.*,

        --LTV = total revenue (this is simplified; could also be monthly_revenue  * lifetime_months)
        ulm.total_revenue as ltv,

        --average revenue per month
        case
            when ulm.lifetime_months > 0 then ulm.total_revenue / ulm.lifetime_months 
            else ulm.total_revenue 
        end as arpu --average revenue per user per month
    from user_lifetime_metrics ulm 

),

--join users to get cohort and channel info
final as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.country,

        --lifetime metrics
        ltv.first_subscription_date,
        ltv.last_subscription_end_date,
        ltv.lifetime_months,
        ltv.total_subscriptions,
        ltv.total_revenue,
        ltv.avg_monthly_revenue,
        ltv.ltv,
        ltv.arpu,

        --status
        ltv.is_currently_active,
        ltv.has_churned,

        --time to first subscription 
        datediff('day', u.signed_up_at, ltv.first_subscription_date) as days_to_first_subscription
    from users u 
    inner join user_ltv ltv 
        on u.user_id = ltv.user_id 
)

select * from final
    );
  
  
[0m12:06:10.495808 [debug] [Thread-2 (]: SQL status: OK in 0.013 seconds
[0m12:06:10.499252 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.499463 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */
alter table "analytics"."main"."rpt_customer_ltv__dbt_tmp" rename to "rpt_customer_ltv"
[0m12:06:10.500130 [debug] [Thread-1 (]: SQL status: OK in 0.018 seconds
[0m12:06:10.502787 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.502984 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:06:10.503232 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m12:06:10.510230 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: COMMIT
[0m12:06:10.510497 [debug] [Thread-1 (]: SQL status: OK in 0.007 seconds
[0m12:06:10.510676 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.510884 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: COMMIT
[0m12:06:10.511499 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.511761 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'fct_mrr_by_month'
  
[0m12:06:10.512769 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:10.514430 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.514601 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month" rename to "fct_mrr_by_month__dbt_backup"
[0m12:06:10.514854 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m12:06:10.517586 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_customer_ltv"
[0m12:06:10.517772 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_customer_ltv"} */

      drop table if exists "analytics"."main"."rpt_customer_ltv__dbt_backup" cascade
    
[0m12:06:10.518191 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m12:06:10.519534 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.519699 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */
alter table "analytics"."main"."fct_mrr_by_month__dbt_tmp" rename to "fct_mrr_by_month"
[0m12:06:10.519856 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m12:06:10.521020 [debug] [Thread-2 (]: On model.saas_analytics.rpt_customer_ltv: Close
[0m12:06:10.521223 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:10.523014 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:06:10.523194 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.523521 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: COMMIT
[0m12:06:10.524252 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10997fdd0>]}
[0m12:06:10.524588 [info ] [Thread-2 (]: 2 of 3 OK created sql table model main.rpt_customer_ltv ........................ [[32mOK[0m in 0.07s]
[0m12:06:10.524929 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_customer_ltv
[0m12:06:10.525134 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:10.526416 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.fct_mrr_by_month"
[0m12:06:10.526575 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.fct_mrr_by_month"} */

      drop table if exists "analytics"."main"."fct_mrr_by_month__dbt_backup" cascade
    
[0m12:06:10.527147 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.527798 [debug] [Thread-1 (]: On model.saas_analytics.fct_mrr_by_month: Close
[0m12:06:10.528042 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10980fa70>]}
[0m12:06:10.528302 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.fct_mrr_by_month ........................ [[32mOK[0m in 0.07s]
[0m12:06:10.528538 [debug] [Thread-1 (]: Finished running node model.saas_analytics.fct_mrr_by_month
[0m12:06:10.528875 [debug] [Thread-4 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:06:10.529109 [info ] [Thread-4 (]: 3 of 3 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:06:10.529355 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_mrr_movements'
[0m12:06:10.529526 [debug] [Thread-4 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:06:10.532530 [debug] [Thread-4 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.533158 [debug] [Thread-4 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:06:10.534834 [debug] [Thread-4 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.535219 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.535378 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:06:10.535519 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:06:10.535878 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.536026 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.536266 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:06:10.549148 [debug] [Thread-4 (]: SQL status: OK in 0.013 seconds
[0m12:06:10.549843 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.550029 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:06:10.550516 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.551106 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.551267 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:06:10.551699 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.553363 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.553542 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements" rename to "rpt_mrr_movements__dbt_backup"
[0m12:06:10.553878 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.555206 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.555370 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:06:10.555662 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.556417 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:06:10.556568 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.556702 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:06:10.557248 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.558337 [debug] [Thread-4 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:06:10.558486 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:06:10.558888 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m12:06:10.559586 [debug] [Thread-4 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:06:10.559887 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b0f3283-f884-4d10-975a-8e8cbecbcefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10997f530>]}
[0m12:06:10.560187 [info ] [Thread-4 (]: 3 of 3 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.03s]
[0m12:06:10.560445 [debug] [Thread-4 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:06:10.562096 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:10.562248 [debug] [MainThread]: On master: BEGIN
[0m12:06:10.562367 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:06:10.562629 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:10.562758 [debug] [MainThread]: On master: COMMIT
[0m12:06:10.562878 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:10.562993 [debug] [MainThread]: On master: COMMIT
[0m12:06:10.563175 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:10.563299 [debug] [MainThread]: On master: Close
[0m12:06:10.563456 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:06:10.563574 [debug] [MainThread]: Connection 'model.saas_analytics.fct_mrr_by_month' was properly closed.
[0m12:06:10.563679 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_customer_ltv' was properly closed.
[0m12:06:10.563781 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:06:10.563927 [info ] [MainThread]: 
[0m12:06:10.564069 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m12:06:10.564454 [debug] [MainThread]: Command end result
[0m12:06:10.588761 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:06:10.589889 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:06:10.592774 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:06:10.592965 [info ] [MainThread]: 
[0m12:06:10.593161 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:06:10.593304 [info ] [MainThread]: 
[0m12:06:10.593454 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m12:06:10.594866 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9278264, "process_in_blocks": "0", "process_kernel_time": 0.234968, "process_mem_max_rss": "215777280", "process_out_blocks": "0", "process_user_time": 1.38919}
[0m12:06:10.595119 [debug] [MainThread]: Command `dbt run` succeeded at 12:06:10.595073 after 0.93 seconds
[0m12:06:10.595301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085ab980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b21d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10879bce0>]}
[0m12:06:10.595475 [debug] [MainThread]: Flushing usage events
[0m12:06:10.692632 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:06:31.214115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102176930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf1880>]}


============================== 12:06:31.221173 | 61e90a92-d015-4899-a330-68b1598c7af4 ==============================
[0m12:06:31.221173 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:06:31.221549 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'printer_width': '80', 'use_colors': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt test --select path:models/marts/revenue', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'empty': 'None', 'target_path': 'None', 'use_experimental_parser': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_format': 'default', 'introspect': 'True', 'no_print': 'None', 'static_parser': 'True', 'debug': 'False'}
[0m12:06:31.391616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10453bf50>]}
[0m12:06:31.420112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10229bef0>]}
[0m12:06:31.422103 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:06:31.476458 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:06:31.549176 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:06:31.549430 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m12:06:31.549569 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:06:31.572219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f8c8f0>]}
[0m12:06:31.621042 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:06:31.622594 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:06:31.641303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bb3cb0>]}
[0m12:06:31.641599 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:06:31.641770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f8c920>]}
[0m12:06:31.643954 [info ] [MainThread]: 
[0m12:06:31.644136 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:06:31.644273 [info ] [MainThread]: 
[0m12:06:31.644516 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:06:31.647048 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m12:06:31.705056 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:06:31.705284 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:06:31.705419 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:06:31.720907 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m12:06:31.721166 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:06:31.721330 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:06:31.741154 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m12:06:31.742363 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:06:31.743599 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:06:31.743762 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:06:31.745459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61e90a92-d015-4899-a330-68b1598c7af4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e7e990>]}
[0m12:06:31.745730 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:31.745874 [debug] [MainThread]: On master: BEGIN
[0m12:06:31.746134 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:06:31.746753 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m12:06:31.747080 [debug] [MainThread]: On master: COMMIT
[0m12:06:31.747315 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:31.747460 [debug] [MainThread]: On master: COMMIT
[0m12:06:31.747766 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:31.747979 [debug] [MainThread]: On master: Close
[0m12:06:31.750438 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef
[0m12:06:31.750816 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d
[0m12:06:31.751134 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec
[0m12:06:31.751341 [info ] [Thread-1 (]: 1 of 8 START test not_null_fct_mrr_by_month_month_date ......................... [RUN]
[0m12:06:31.751673 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531
[0m12:06:31.752050 [info ] [Thread-2 (]: 2 of 8 START test not_null_fct_mrr_by_month_mrr ................................ [RUN]
[0m12:06:31.752304 [info ] [Thread-3 (]: 3 of 8 START test not_null_fct_mrr_by_month_subscription_id .................... [RUN]
[0m12:06:31.752791 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef)
[0m12:06:31.753009 [info ] [Thread-4 (]: 4 of 8 START test not_null_fct_mrr_by_month_user_id ............................ [RUN]
[0m12:06:31.753483 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d'
[0m12:06:31.753715 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec'
[0m12:06:31.753890 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef
[0m12:06:31.754107 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531'
[0m12:06:31.754295 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d
[0m12:06:31.754714 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec
[0m12:06:31.765698 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef"
[0m12:06:31.765959 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531
[0m12:06:31.768066 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d"
[0m12:06:31.769999 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec"
[0m12:06:31.773579 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531"
[0m12:06:31.774287 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531
[0m12:06:31.780522 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec
[0m12:06:31.780749 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d
[0m12:06:31.783145 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531"
[0m12:06:31.783355 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef
[0m12:06:31.784589 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec"
[0m12:06:31.785696 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d"
[0m12:06:31.786887 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef"
[0m12:06:31.787418 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec"
[0m12:06:31.787619 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d"
[0m12:06:31.787853 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef"
[0m12:06:31.788020 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec: BEGIN
[0m12:06:31.788199 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d: BEGIN
[0m12:06:31.788360 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef: BEGIN
[0m12:06:31.788529 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531"
[0m12:06:31.788678 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:06:31.788831 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:06:31.788976 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:06:31.789124 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531: BEGIN
[0m12:06:31.789520 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:06:31.789748 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.789926 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.790070 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.790214 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef"
[0m12:06:31.790359 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.790503 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d"
[0m12:06:31.790655 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec"
[0m12:06:31.790812 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month_date
from "analytics"."main"."fct_mrr_by_month"
where month_date is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.790969 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531"
[0m12:06:31.791122 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select mrr
from "analytics"."main"."fct_mrr_by_month"
where mrr is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.791294 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select subscription_id
from "analytics"."main"."fct_mrr_by_month"
where subscription_id is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.791537 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."fct_mrr_by_month"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.792865 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.793039 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.793204 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m12:06:31.793341 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m12:06:31.795591 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef: ROLLBACK
[0m12:06:31.796287 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d: ROLLBACK
[0m12:06:31.796933 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531: ROLLBACK
[0m12:06:31.797520 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef'
[0m12:06:31.798176 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec: ROLLBACK
[0m12:06:31.798619 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d'
[0m12:06:31.799076 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531'
[0m12:06:31.799246 [debug] [Thread-1 (]: On test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef: Close
[0m12:06:31.799677 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec'
[0m12:06:31.799832 [debug] [Thread-2 (]: On test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d: Close
[0m12:06:31.799993 [debug] [Thread-4 (]: On test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531: Close
[0m12:06:31.800168 [debug] [Thread-3 (]: On test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec: Close
[0m12:06:31.800450 [info ] [Thread-1 (]: 1 of 8 PASS not_null_fct_mrr_by_month_month_date ............................... [[32mPASS[0m in 0.05s]
[0m12:06:31.800713 [info ] [Thread-2 (]: 2 of 8 PASS not_null_fct_mrr_by_month_mrr ...................................... [[32mPASS[0m in 0.05s]
[0m12:06:31.800950 [info ] [Thread-4 (]: 4 of 8 PASS not_null_fct_mrr_by_month_user_id .................................. [[32mPASS[0m in 0.05s]
[0m12:06:31.801428 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef
[0m12:06:31.801216 [info ] [Thread-3 (]: 3 of 8 PASS not_null_fct_mrr_by_month_subscription_id .......................... [[32mPASS[0m in 0.05s]
[0m12:06:31.801687 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d
[0m12:06:31.801906 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531
[0m12:06:31.802088 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1
[0m12:06:31.802331 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec
[0m12:06:31.802500 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245
[0m12:06:31.802710 [debug] [Thread-4 (]: Began running node test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe
[0m12:06:31.803070 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2
[0m12:06:31.802903 [info ] [Thread-1 (]: 5 of 8 START test not_null_rpt_customer_ltv_user_id ............................ [RUN]
[0m12:06:31.803296 [info ] [Thread-2 (]: 6 of 8 START test not_null_rpt_mrr_movements_month_date ........................ [RUN]
[0m12:06:31.803492 [info ] [Thread-4 (]: 7 of 8 START test unique_rpt_customer_ltv_user_id .............................. [RUN]
[0m12:06:31.803688 [info ] [Thread-3 (]: 8 of 8 START test unique_rpt_mrr_movements_month_date .......................... [RUN]
[0m12:06:31.804238 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_mrr_by_month_month_date.b74b7397ef, now test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1)
[0m12:06:31.804448 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_mrr_by_month_mrr.18d6867b2d, now test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245)
[0m12:06:31.804629 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_mrr_by_month_user_id.4471216531, now test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe)
[0m12:06:31.804799 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_fct_mrr_by_month_subscription_id.e86f660bec, now test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2)
[0m12:06:31.804970 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1
[0m12:06:31.805128 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245
[0m12:06:31.805278 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe
[0m12:06:31.805493 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2
[0m12:06:31.808023 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1"
[0m12:06:31.810116 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245"
[0m12:06:31.814465 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe"
[0m12:06:31.816499 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2"
[0m12:06:31.817099 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1
[0m12:06:31.818289 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1"
[0m12:06:31.818568 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2
[0m12:06:31.820649 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2"
[0m12:06:31.820901 [debug] [Thread-4 (]: Began executing node test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe
[0m12:06:31.821082 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245
[0m12:06:31.822224 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe"
[0m12:06:31.823298 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245"
[0m12:06:31.823551 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2"
[0m12:06:31.823747 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1"
[0m12:06:31.823954 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2: BEGIN
[0m12:06:31.824138 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1: BEGIN
[0m12:06:31.824325 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:06:31.824485 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:06:31.824648 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe"
[0m12:06:31.824819 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245"
[0m12:06:31.825238 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe: BEGIN
[0m12:06:31.825411 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.825566 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.825720 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245: BEGIN
[0m12:06:31.825884 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:06:31.826066 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2"
[0m12:06:31.826266 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1"
[0m12:06:31.826429 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:06:31.826661 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    month_date as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_mrr_movements"
where month_date is not null
group by month_date
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:06:31.826844 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."rpt_customer_ltv"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.827009 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.827355 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.827552 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe"
[0m12:06:31.827715 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245"
[0m12:06:31.827906 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_customer_ltv"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:06:31.828106 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month_date
from "analytics"."main"."rpt_mrr_movements"
where month_date is null



  
  
      
    ) dbt_internal_test
[0m12:06:31.828272 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.829326 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1: ROLLBACK
[0m12:06:31.829572 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m12:06:31.830024 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1'
[0m12:06:31.830795 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245: ROLLBACK
[0m12:06:31.831022 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1: Close
[0m12:06:31.831582 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245'
[0m12:06:31.831981 [info ] [Thread-1 (]: 5 of 8 PASS not_null_rpt_customer_ltv_user_id .................................. [[32mPASS[0m in 0.03s]
[0m12:06:31.832272 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245: Close
[0m12:06:31.832575 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m12:06:31.832827 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1
[0m12:06:31.832990 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m12:06:31.833996 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2: ROLLBACK
[0m12:06:31.833252 [info ] [Thread-2 (]: 6 of 8 PASS not_null_rpt_mrr_movements_month_date .............................. [[32mPASS[0m in 0.03s]
[0m12:06:31.834781 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe: ROLLBACK
[0m12:06:31.835259 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2'
[0m12:06:31.835485 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245
[0m12:06:31.835933 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe'
[0m12:06:31.836088 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2: Close
[0m12:06:31.836290 [debug] [Thread-4 (]: On test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe: Close
[0m12:06:31.836572 [info ] [Thread-3 (]: 8 of 8 PASS unique_rpt_mrr_movements_month_date ................................ [[32mPASS[0m in 0.03s]
[0m12:06:31.837023 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2
[0m12:06:31.836778 [info ] [Thread-4 (]: 7 of 8 PASS unique_rpt_customer_ltv_user_id .................................... [[32mPASS[0m in 0.03s]
[0m12:06:31.837458 [debug] [Thread-4 (]: Finished running node test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe
[0m12:06:31.838206 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:31.838361 [debug] [MainThread]: On master: BEGIN
[0m12:06:31.838490 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:06:31.838837 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:31.838970 [debug] [MainThread]: On master: COMMIT
[0m12:06:31.839097 [debug] [MainThread]: Using duckdb connection "master"
[0m12:06:31.839215 [debug] [MainThread]: On master: COMMIT
[0m12:06:31.839415 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:06:31.839546 [debug] [MainThread]: On master: Close
[0m12:06:31.839714 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:06:31.839837 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_customer_ltv_user_id.8538ccf3b1' was properly closed.
[0m12:06:31.839953 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_mrr_movements_month_date.eff3b02245' was properly closed.
[0m12:06:31.840068 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_mrr_movements_month_date.748ae43cf2' was properly closed.
[0m12:06:31.840176 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_customer_ltv_user_id.994aaa22fe' was properly closed.
[0m12:06:31.840323 [info ] [MainThread]: 
[0m12:06:31.840465 [info ] [MainThread]: Finished running 8 data tests in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m12:06:31.841115 [debug] [MainThread]: Command end result
[0m12:06:31.867552 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:06:31.868981 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:06:31.872854 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:06:31.873104 [info ] [MainThread]: 
[0m12:06:31.873328 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:06:31.873500 [info ] [MainThread]: 
[0m12:06:31.873667 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m12:06:31.875270 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.70181453, "process_in_blocks": "0", "process_kernel_time": 0.208107, "process_mem_max_rss": "149045248", "process_out_blocks": "0", "process_user_time": 1.135547}
[0m12:06:31.875557 [debug] [MainThread]: Command `dbt test` succeeded at 12:06:31.875511 after 0.70 seconds
[0m12:06:31.875767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf18e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf15b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bf1a90>]}
[0m12:06:31.876451 [debug] [MainThread]: Flushing usage events
[0m12:06:31.989185 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:20:34.110975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113497e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bb1e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bb1c10>]}


============================== 12:20:34.142206 | adda4394-f195-4d0f-9c9d-687a9c165ceb ==============================
[0m12:20:34.142206 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:20:34.142638 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_format': 'default', 'no_print': 'None', 'introspect': 'True', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'fail_fast': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'static_parser': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run --select rpt_rr_movements', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'quiet': 'False', 'use_colors': 'True', 'empty': 'False'}
[0m12:20:34.310298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adda4394-f195-4d0f-9c9d-687a9c165ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113b49eb0>]}
[0m12:20:34.338777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adda4394-f195-4d0f-9c9d-687a9c165ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113470e90>]}
[0m12:20:34.340080 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:20:34.391696 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:20:34.467825 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:20:34.468278 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:20:34.693854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'adda4394-f195-4d0f-9c9d-687a9c165ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115037980>]}
[0m12:20:34.741850 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:20:34.743545 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:20:34.758748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'adda4394-f195-4d0f-9c9d-687a9c165ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9ba40>]}
[0m12:20:34.759020 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:20:34.759200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'adda4394-f195-4d0f-9c9d-687a9c165ceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114f76db0>]}
[0m12:20:34.759675 [warn ] [MainThread]: The selection criterion 'rpt_rr_movements' does not match any enabled nodes
[0m12:20:34.760396 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m12:20:34.760650 [debug] [MainThread]: Command end result
[0m12:20:34.787228 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:20:34.788339 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:20:34.790333 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:20:34.791751 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.7249057, "process_in_blocks": "0", "process_kernel_time": 0.201694, "process_mem_max_rss": "142540800", "process_out_blocks": "0", "process_user_time": 1.226578}
[0m12:20:34.791984 [debug] [MainThread]: Command `dbt run` succeeded at 12:20:34.791942 after 0.73 seconds
[0m12:20:34.792165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9b9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9b980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114874080>]}
[0m12:20:34.792335 [debug] [MainThread]: Flushing usage events
[0m12:20:34.920877 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:20:46.092120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100db110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138b1f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138b1d30>]}


============================== 12:20:46.098197 | 74404414-6984-490e-bda3-f1e380e4c0bd ==============================
[0m12:20:46.098197 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:20:46.098544 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'quiet': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'static_parser': 'True', 'empty': 'False', 'warn_error': 'None', 'no_print': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'debug': 'False', 'indirect_selection': 'eager', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'version_check': 'True', 'introspect': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select rpt_mrr_movements', 'log_format': 'default', 'write_json': 'True', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True'}
[0m12:20:46.259372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138467b0>]}
[0m12:20:46.288081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110171160>]}
[0m12:20:46.289505 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:20:46.340773 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:20:46.412619 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:20:46.412867 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m12:20:46.413015 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:20:46.435048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e5d5b0>]}
[0m12:20:46.483009 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:20:46.484517 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:20:46.498648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1143164e0>]}
[0m12:20:46.498954 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:20:46.499135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114362cc0>]}
[0m12:20:46.500321 [info ] [MainThread]: 
[0m12:20:46.500508 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:20:46.500664 [info ] [MainThread]: 
[0m12:20:46.500931 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:20:46.501728 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:20:46.562721 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:20:46.562989 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:20:46.563161 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:20:46.582507 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m12:20:46.583493 [debug] [ThreadPool]: On list_analytics: Close
[0m12:20:46.583841 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:20:46.584081 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:20:46.587334 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:20:46.587501 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:20:46.587636 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:20:46.588394 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:20:46.588995 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:20:46.589131 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:20:46.589346 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:20:46.589469 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:20:46.589596 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:20:46.590060 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:20:46.590436 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:20:46.590561 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:20:46.590673 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:20:46.590859 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:20:46.590986 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:20:46.593645 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:20:46.596195 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:20:46.596347 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:20:46.596460 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:20:46.596682 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:20:46.596803 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:20:46.596930 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:20:46.607724 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m12:20:46.608559 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:20:46.609536 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:20:46.609694 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:20:46.610897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114359730>]}
[0m12:20:46.611143 [debug] [MainThread]: Using duckdb connection "master"
[0m12:20:46.611288 [debug] [MainThread]: On master: BEGIN
[0m12:20:46.611411 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:20:46.611667 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:20:46.611798 [debug] [MainThread]: On master: COMMIT
[0m12:20:46.611920 [debug] [MainThread]: Using duckdb connection "master"
[0m12:20:46.612038 [debug] [MainThread]: On master: COMMIT
[0m12:20:46.612230 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:20:46.612352 [debug] [MainThread]: On master: Close
[0m12:20:46.613747 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:20:46.614090 [info ] [Thread-1 (]: 1 of 1 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:20:46.614305 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_mrr_movements)
[0m12:20:46.614470 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:20:46.618508 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:20:46.619836 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:20:46.635547 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:20:46.636538 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:20:46.636730 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:20:46.636885 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:20:46.637242 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:20:46.637400 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:20:46.637672 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get all months and all users who ever had MRR
all_months as (
    select distinct month_date from mrr_by_month
),

all_users as (
    select distinct user_id from mrr_by_month
),

--create a complete spine of month-user combinations
month_user_spine as (
    select 
        m.month_date,
        u.user_id
    from all_months
    cross join all_users u
),

user_mrr_complete as (
    select 
        s.month_date,
        s.user_id,
        coalesce(sum(m.mrr), 0) as mrr_this_month 
    from month_user_spine s 
    left join mrr_by_month m 
        on s.month_date = m.month_date 
        and s.user_id = m.user_id 
    group by s.month_date, s.user_id 
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:20:46.641987 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get all months and all users who ever had MRR
all_months as (
    select distinct month_date from mrr_by_month
),

all_users as (
    select distinct user_id from mrr_by_month
),

--create a complete spine of month-user combinations
month_user_spine as (
    select 
        m.month_date,
        u.user_id
    from all_months
    cross join all_users u
),

user_mrr_complete as (
    select 
        s.month_date,
        s.user_id,
        coalesce(sum(m.mrr), 0) as mrr_this_month 
    from month_user_spine s 
    left join mrr_by_month m 
        on s.month_date = m.month_date 
        and s.user_id = m.user_id 
    group by s.month_date, s.user_id 
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:20:46.642300 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m12:20:46.642508 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: ROLLBACK
[0m12:20:46.646673 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_mrr_movements'
[0m12:20:46.646888 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:20:46.648078 [debug] [Thread-1 (]: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Referenced table "m" not found!
  Candidate tables: "all_months"
  
  LINE 29:         m.month_date,
                   ^
[0m12:20:46.649066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74404414-6984-490e-bda3-f1e380e4c0bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113a1fad0>]}
[0m12:20:46.649387 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model main.rpt_mrr_movements ................... [[31mERROR[0m in 0.03s]
[0m12:20:46.649659 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:20:46.649906 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_mrr_movements' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Referenced table "m" not found!
  Candidate tables: "all_months"
  
  LINE 29:         m.month_date,
                   ^.
[0m12:20:46.650770 [debug] [MainThread]: Using duckdb connection "master"
[0m12:20:46.650938 [debug] [MainThread]: On master: BEGIN
[0m12:20:46.651067 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:20:46.651375 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:20:46.651508 [debug] [MainThread]: On master: COMMIT
[0m12:20:46.651632 [debug] [MainThread]: Using duckdb connection "master"
[0m12:20:46.651750 [debug] [MainThread]: On master: COMMIT
[0m12:20:46.651944 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:20:46.652065 [debug] [MainThread]: On master: Close
[0m12:20:46.652225 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:20:46.652343 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:20:46.652463 [info ] [MainThread]: 
[0m12:20:46.652601 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m12:20:46.652891 [debug] [MainThread]: Command end result
[0m12:20:46.673335 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:20:46.674455 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:20:46.677441 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:20:46.677611 [info ] [MainThread]: 
[0m12:20:46.677798 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:20:46.677947 [info ] [MainThread]: 
[0m12:20:46.678122 [error] [MainThread]: [31mFailure in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)[0m
[0m12:20:46.678286 [error] [MainThread]:   Runtime Error in model rpt_mrr_movements (models/marts/revenue/rpt_mrr_movements.sql)
  Binder Error: Referenced table "m" not found!
  Candidate tables: "all_months"
  
  LINE 29:         m.month_date,
                   ^
[0m12:20:46.678413 [info ] [MainThread]: 
[0m12:20:46.678560 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/revenue/rpt_mrr_movements.sql
[0m12:20:46.678693 [info ] [MainThread]: 
[0m12:20:46.678832 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:20:46.680307 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.62395364, "process_in_blocks": "0", "process_kernel_time": 0.199758, "process_mem_max_rss": "146046976", "process_out_blocks": "0", "process_user_time": 1.145053}
[0m12:20:46.680555 [debug] [MainThread]: Command `dbt run` failed at 12:20:46.680510 after 0.62 seconds
[0m12:20:46.680774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11045faa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146a53d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11433c920>]}
[0m12:20:46.680968 [debug] [MainThread]: Flushing usage events
[0m12:20:46.798136 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:21:09.143711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105809bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b5d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b5ac0>]}


============================== 12:21:09.157451 | 49576b57-96ba-4fb9-933c-ef81986e37e1 ==============================
[0m12:21:09.157451 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:21:09.157821 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'use_colors': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select rpt_mrr_movements', 'target_path': 'None', 'cache_selected_only': 'False', 'debug': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'warn_error': 'None', 'partial_parse': 'True', 'printer_width': '80', 'version_check': 'True'}
[0m12:21:09.323712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c2bb30>]}
[0m12:21:09.352332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106870d70>]}
[0m12:21:09.353616 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:21:09.407371 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:21:09.483274 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:21:09.483786 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:21:09.736364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110839970>]}
[0m12:21:09.792192 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:21:09.793778 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:21:09.809745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107be4830>]}
[0m12:21:09.810114 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:21:09.810337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cc66f0>]}
[0m12:21:09.811557 [info ] [MainThread]: 
[0m12:21:09.811748 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:21:09.811884 [info ] [MainThread]: 
[0m12:21:09.812124 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:21:09.812579 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:21:09.848006 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:21:09.848269 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:21:09.848425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:21:09.875210 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m12:21:09.876289 [debug] [ThreadPool]: On list_analytics: Close
[0m12:21:09.876677 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:21:09.876930 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:21:09.880419 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:21:09.880610 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:21:09.880750 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:21:09.881949 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:21:09.882713 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:21:09.882872 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:21:09.883127 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:21:09.883254 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:21:09.883376 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:21:09.883845 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:21:09.884226 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:21:09.884357 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:21:09.884474 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:21:09.884671 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:21:09.884800 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:21:09.887792 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:21:09.890863 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:21:09.891075 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:21:09.891214 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:21:09.891596 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:21:09.891782 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:21:09.891935 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:21:09.904372 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m12:21:09.905347 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:21:09.906396 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:21:09.906597 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:21:09.907945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1108b8920>]}
[0m12:21:09.908195 [debug] [MainThread]: Using duckdb connection "master"
[0m12:21:09.908326 [debug] [MainThread]: On master: BEGIN
[0m12:21:09.908442 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:21:09.908823 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:21:09.908986 [debug] [MainThread]: On master: COMMIT
[0m12:21:09.909125 [debug] [MainThread]: Using duckdb connection "master"
[0m12:21:09.909246 [debug] [MainThread]: On master: COMMIT
[0m12:21:09.909461 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:21:09.909591 [debug] [MainThread]: On master: Close
[0m12:21:09.911029 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:21:09.911283 [info ] [Thread-1 (]: 1 of 1 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:21:09.911497 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_mrr_movements)
[0m12:21:09.911657 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:21:09.915816 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.916338 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:21:09.933112 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.933718 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.933891 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:21:09.934038 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:21:09.934422 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:21:09.934616 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.934898 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get all months and all users who ever had MRR
all_months as (
    select distinct month_date from mrr_by_month
),

all_users as (
    select distinct user_id from mrr_by_month
),

--create a complete spine of month-user combinations
month_user_spine as (
    select 
        m.month_date,
        u.user_id
    from all_months m
    cross join all_users u
),

user_mrr_complete as (
    select 
        s.month_date,
        s.user_id,
        coalesce(sum(m.mrr), 0) as mrr_this_month 
    from month_user_spine s 
    left join mrr_by_month m 
        on s.month_date = m.month_date 
        and s.user_id = m.user_id 
    group by s.month_date, s.user_id 
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    where mrr_this_month > 0 or mrr_last_month > 0 --only keep rows where there was MRR at some point
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:21:09.966093 [debug] [Thread-1 (]: SQL status: OK in 0.031 seconds
[0m12:21:09.969099 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.969311 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:21:09.969972 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:21:09.970685 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.970869 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:21:09.971732 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:21:09.975073 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.975291 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements" rename to "rpt_mrr_movements__dbt_backup"
[0m12:21:09.976496 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:21:09.977968 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.978141 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:21:09.978445 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:21:09.985739 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:21:09.985945 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.986100 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:21:09.986910 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:21:09.989548 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:21:09.989729 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:21:09.990598 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:21:09.991719 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:21:09.992652 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49576b57-96ba-4fb9-933c-ef81986e37e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c2ac60>]}
[0m12:21:09.992959 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.08s]
[0m12:21:09.993229 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:21:09.993826 [debug] [MainThread]: Using duckdb connection "master"
[0m12:21:09.993982 [debug] [MainThread]: On master: BEGIN
[0m12:21:09.994102 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:21:09.994374 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:21:09.994509 [debug] [MainThread]: On master: COMMIT
[0m12:21:09.994635 [debug] [MainThread]: Using duckdb connection "master"
[0m12:21:09.994755 [debug] [MainThread]: On master: COMMIT
[0m12:21:09.994978 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:21:09.995127 [debug] [MainThread]: On master: Close
[0m12:21:09.995291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:21:09.995405 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:21:09.995527 [info ] [MainThread]: 
[0m12:21:09.995666 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m12:21:09.995951 [debug] [MainThread]: Command end result
[0m12:21:10.016020 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:21:10.017073 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:21:10.019692 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:21:10.019869 [info ] [MainThread]: 
[0m12:21:10.020075 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:21:10.020219 [info ] [MainThread]: 
[0m12:21:10.020374 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:21:10.021827 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9168193, "process_in_blocks": "0", "process_kernel_time": 0.226787, "process_mem_max_rss": "213581824", "process_out_blocks": "0", "process_user_time": 1.327132}
[0m12:21:10.022269 [debug] [MainThread]: Command `dbt run` succeeded at 12:21:10.022225 after 0.92 seconds
[0m12:21:10.022458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b4c710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d8e990>]}
[0m12:21:10.022645 [debug] [MainThread]: Flushing usage events
[0m12:21:10.135102 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:25:40.974728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e28bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c9a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c9880>]}


============================== 12:25:41.001754 | 89fa46fa-c0d1-4c6f-aa8e-f895d8991618 ==============================
[0m12:25:41.001754 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:25:41.002206 [debug] [MainThread]: running dbt with arguments {'introspect': 'True', 'use_experimental_parser': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'version_check': 'True', 'printer_width': '80', 'empty': 'False', 'use_colors': 'True', 'no_print': 'None', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'quiet': 'False', 'invocation_command': 'dbt run --select rpt_mrr_movements', 'target_path': 'None'}
[0m12:25:41.181328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108af8080>]}
[0m12:25:41.210181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108313200>]}
[0m12:25:41.211481 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:25:41.262688 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:25:41.339306 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:25:41.339763 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:25:41.564778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10924f350>]}
[0m12:25:41.611958 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:25:41.613807 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:25:41.629013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109257650>]}
[0m12:25:41.629285 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:25:41.629457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092327b0>]}
[0m12:25:41.630529 [info ] [MainThread]: 
[0m12:25:41.630693 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:25:41.630822 [info ] [MainThread]: 
[0m12:25:41.631055 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:25:41.631458 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:25:41.661181 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:25:41.661420 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:25:41.661567 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:25:41.682853 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m12:25:41.683697 [debug] [ThreadPool]: On list_analytics: Close
[0m12:25:41.684072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:25:41.684330 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:25:41.687826 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:25:41.688011 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:25:41.688146 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:25:41.689103 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:25:41.689744 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:25:41.689885 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:25:41.690108 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:25:41.690233 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:25:41.690358 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:25:41.690868 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:25:41.691246 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:25:41.691378 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:25:41.691497 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:25:41.691697 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:25:41.691831 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:25:41.694628 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:25:41.697558 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:25:41.697741 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:25:41.697880 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:25:41.698185 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:25:41.698323 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:25:41.698458 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:25:41.708185 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m12:25:41.709158 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:25:41.710113 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:25:41.710273 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:25:41.711515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109061520>]}
[0m12:25:41.711748 [debug] [MainThread]: Using duckdb connection "master"
[0m12:25:41.711882 [debug] [MainThread]: On master: BEGIN
[0m12:25:41.712000 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:25:41.712263 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:25:41.712406 [debug] [MainThread]: On master: COMMIT
[0m12:25:41.712536 [debug] [MainThread]: Using duckdb connection "master"
[0m12:25:41.712653 [debug] [MainThread]: On master: COMMIT
[0m12:25:41.712853 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:25:41.712980 [debug] [MainThread]: On master: Close
[0m12:25:41.714397 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:25:41.714653 [info ] [Thread-1 (]: 1 of 1 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:25:41.714855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_mrr_movements)
[0m12:25:41.715021 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:25:41.719056 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.720145 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:25:41.737609 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.738446 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.738637 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:25:41.738792 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:25:41.739165 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:25:41.739319 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.739590 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (
    select * from "analytics"."main"."fct_mrr_by_month"
),

--get all months and all users who ever had MRR
all_months as (
    select distinct month_date from mrr_by_month
),

all_users as (
    select distinct user_id from mrr_by_month
),

--create a complete spine of month-user combinations
month_user_spine as (
    select 
        m.month_date,
        u.user_id
    from all_months m
    cross join all_users u
),

user_mrr_complete as (
    select 
        s.month_date,
        s.user_id,
        coalesce(sum(m.mrr), 0) as mrr_this_month 
    from month_user_spine s 
    left join mrr_by_month m 
        on s.month_date = m.month_date 
        and s.user_id = m.user_id 
    group by s.month_date, s.user_id 
),

--get current and prior month MRR for each user
user_mrr_current_and_prior as (
    select 
        month_date,
        user_id,
        sum(mrr) as mrr_this_month,
        lag(sum(mrr)) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date
    from mrr_by_month 
    group by month_date, user_id 
),

--classify MRR movement type for each user, each month
mrr_changes as (
    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        --change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        --movement type
        case 
            when mrr_last_month is null or mrr_last_month = 0 then 'new' --new customer (no MRR last month)
            when mrr_this_month = 0 then 'churn' --churned (had MRR last, none this month)
            when mrr_this_month > mrr_last_month then 'expansion' --expansion (increased MRR)
            when mrr_this_month < mrr_last_month then 'contraction' -- contraction (descreased MRR)
            else 'retained' --no change
        end as movement_type,

        --check for reactivation (churned before, now back)
        case 
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
            then true 
            else false
        end is_reactivation
    from user_mrr_current_and_prior 
    -- only include row where user had MRR at some point (current or prior month)
    -- this excludes users who never had any MRR in this or previous months
    where (mrr_this_month > 0 or mrr_last_month > 0)
        and mrr_last_month is not null 
),

--aggregate movements by month
monthly_movements as (
    select 
        month_date,
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        --MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        --total MRR
        sum(mrr_this_month) as total_mrr,
        sum(mrr_last_month) as prior_month_mrr

    from mrr_changes
    group by month_date
),

--calculate growth rates and net change
final as (
    select 
        month_date,

        --customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        --MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        --net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        --growth rates
        case
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null 
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end churn_rate 
    from monthly_movements

)

select * from final 
order by month_date
    );
  
  
[0m12:25:41.777639 [debug] [Thread-1 (]: SQL status: OK in 0.038 seconds
[0m12:25:41.780640 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.780849 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:25:41.781388 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:25:41.782133 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.782341 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:25:41.783225 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:25:41.786678 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.786893 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements" rename to "rpt_mrr_movements__dbt_backup"
[0m12:25:41.787773 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:25:41.789186 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.789345 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:25:41.789625 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:25:41.796598 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:25:41.796787 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.796941 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:25:41.797765 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:25:41.800364 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:25:41.800551 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:25:41.801024 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:25:41.802116 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:25:41.803032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89fa46fa-c0d1-4c6f-aa8e-f895d8991618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106076ed0>]}
[0m12:25:41.803330 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.09s]
[0m12:25:41.803588 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:25:41.804249 [debug] [MainThread]: Using duckdb connection "master"
[0m12:25:41.804399 [debug] [MainThread]: On master: BEGIN
[0m12:25:41.804524 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:25:41.804783 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:25:41.804910 [debug] [MainThread]: On master: COMMIT
[0m12:25:41.805029 [debug] [MainThread]: Using duckdb connection "master"
[0m12:25:41.805145 [debug] [MainThread]: On master: COMMIT
[0m12:25:41.805338 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:25:41.805461 [debug] [MainThread]: On master: Close
[0m12:25:41.805613 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:25:41.805731 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:25:41.805853 [info ] [MainThread]: 
[0m12:25:41.805990 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m12:25:41.806275 [debug] [MainThread]: Command end result
[0m12:25:41.826579 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:25:41.827810 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:25:41.830699 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:25:41.830869 [info ] [MainThread]: 
[0m12:25:41.831063 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:25:41.831199 [info ] [MainThread]: 
[0m12:25:41.831347 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:25:41.832792 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.89815545, "process_in_blocks": "0", "process_kernel_time": 0.232986, "process_mem_max_rss": "206667776", "process_out_blocks": "0", "process_user_time": 1.311456}
[0m12:25:41.833014 [debug] [MainThread]: Command `dbt run` succeeded at 12:25:41.832973 after 0.90 seconds
[0m12:25:41.833203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10922e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c9910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089c9670>]}
[0m12:25:41.833378 [debug] [MainThread]: Flushing usage events
[0m12:25:41.948114 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:29:04.627543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ceb9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a3db20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a3d8e0>]}


============================== 12:29:04.655179 | a34d31f4-e796-4c20-86a9-7c2a474ce909 ==============================
[0m12:29:04.655179 [info ] [MainThread]: Running with dbt=1.11.4
[0m12:29:04.655525 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'write_json': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'no_print': 'None', 'fail_fast': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'introspect': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'version_check': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select rpt_mrr_movements', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_cache_events': 'False', 'warn_error': 'None', 'target_path': 'None', 'quiet': 'False', 'indirect_selection': 'eager'}
[0m12:29:04.873980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108aa7d70>]}
[0m12:29:04.904808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ad8650>]}
[0m12:29:04.906456 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m12:29:04.967391 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m12:29:05.043966 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:29:05.044424 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/revenue/rpt_mrr_movements.sql
[0m12:29:05.274317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092c0bf0>]}
[0m12:29:05.321563 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:29:05.323192 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:29:05.338407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a8140>]}
[0m12:29:05.338710 [info ] [MainThread]: Found 17 models, 59 data tests, 3 sources, 472 macros
[0m12:29:05.338892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a4d70>]}
[0m12:29:05.339992 [info ] [MainThread]: 
[0m12:29:05.340174 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:29:05.340311 [info ] [MainThread]: 
[0m12:29:05.340544 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m12:29:05.340947 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m12:29:05.370288 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m12:29:05.370530 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m12:29:05.370677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:29:05.394231 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m12:29:05.395291 [debug] [ThreadPool]: On list_analytics: Close
[0m12:29:05.395657 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m12:29:05.396016 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m12:29:05.399410 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:29:05.399589 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m12:29:05.399725 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:29:05.400632 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:29:05.401437 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:29:05.401580 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m12:29:05.401827 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:29:05.401954 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:29:05.402080 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m12:29:05.402911 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m12:29:05.403299 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:29:05.403434 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m12:29:05.403554 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m12:29:05.403752 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:29:05.403883 [debug] [ThreadPool]: On create_analytics_main: Close
[0m12:29:05.407234 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m12:29:05.410127 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:29:05.410301 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m12:29:05.410432 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:29:05.410715 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m12:29:05.410845 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m12:29:05.410976 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m12:29:05.421476 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m12:29:05.422498 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m12:29:05.423435 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m12:29:05.423592 [debug] [ThreadPool]: On list_analytics_main: Close
[0m12:29:05.424807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092a5220>]}
[0m12:29:05.425048 [debug] [MainThread]: Using duckdb connection "master"
[0m12:29:05.425185 [debug] [MainThread]: On master: BEGIN
[0m12:29:05.425307 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:29:05.425558 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:29:05.425689 [debug] [MainThread]: On master: COMMIT
[0m12:29:05.425812 [debug] [MainThread]: Using duckdb connection "master"
[0m12:29:05.425927 [debug] [MainThread]: On master: COMMIT
[0m12:29:05.426116 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:29:05.426240 [debug] [MainThread]: On master: Close
[0m12:29:05.427718 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_mrr_movements
[0m12:29:05.427962 [info ] [Thread-1 (]: 1 of 1 START sql table model main.rpt_mrr_movements ............................ [RUN]
[0m12:29:05.428163 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_mrr_movements)
[0m12:29:05.428324 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_mrr_movements
[0m12:29:05.432456 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.433443 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_mrr_movements
[0m12:29:05.449517 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.450333 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.450519 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: BEGIN
[0m12:29:05.450675 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:29:05.451058 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:29:05.451207 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.451493 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

  
    
    

    create  table
      "analytics"."main"."rpt_mrr_movements__dbt_tmp"
  
    as (
      

with mrr_by_month as (

    select * from "analytics"."main"."fct_mrr_by_month"

),

-- Get all months and all users who ever had MRR
all_months as (
    select distinct month_date from mrr_by_month
),

all_users as (
    select distinct user_id from mrr_by_month
),

-- Create a complete spine of month-user combinations
month_user_spine as (
    select 
        m.month_date,
        u.user_id
    from all_months m
    cross join all_users u
),

-- Get MRR for each month-user, filling in 0 for missing combinations
user_mrr_complete as (
    select
        s.month_date,
        s.user_id,
        coalesce(sum(m.mrr), 0) as mrr_this_month
    from month_user_spine s
    left join mrr_by_month m
        on s.month_date = m.month_date
        and s.user_id = m.user_id
    group by s.month_date, s.user_id
),

-- Get current and prior month MRR for each user
user_mrr_current_and_prior as (

    select
        month_date,
        user_id,
        mrr_this_month,
        lag(mrr_this_month) over (partition by user_id order by month_date) as mrr_last_month,
        lag(month_date) over (partition by user_id order by month_date) as last_month_date

    from user_mrr_complete

),

-- Classify MRR movement type for each user each month
mrr_changes as (

    select
        month_date,
        user_id,
        mrr_this_month,
        mrr_last_month,

        -- change amount
        mrr_this_month - coalesce(mrr_last_month, 0) as mrr_change,

        -- movement type
        case
            -- new customer (no MRR last month, has MRR this month)
            when (mrr_last_month is null or mrr_last_month = 0) and mrr_this_month > 0 then 'new'
            
            -- churned (had MRR last month, none this month)
            when mrr_last_month > 0 and mrr_this_month = 0 then 'churn'
            
            -- expansion (increased MRR)
            when mrr_last_month > 0 and mrr_this_month > mrr_last_month then 'expansion'
            
            -- contraction (decreased MRR)
            when mrr_last_month > 0 and mrr_this_month > 0 and mrr_this_month < mrr_last_month then 'contraction'
            
            -- retained (same MRR)
            when mrr_last_month > 0 and mrr_this_month = mrr_last_month then 'retained'
            
            -- no MRR in either month - skip these
            else 'inactive'
        end as movement_type,

        -- check for reactivation (churned before, now back)
        case
            when mrr_last_month is not null 
                and mrr_last_month = 0 
                and last_month_date = month_date - interval '1 month'
                and mrr_this_month > 0
            then true
            else false
        end as is_reactivation

    from user_mrr_current_and_prior
    where mrr_last_month is not null  -- skip first month for each user

),

-- Aggregate movements by month
monthly_movements as (

    select
        month_date,

        -- customer counts (exclude 'inactive' users)
        count(distinct case when movement_type = 'new' then user_id end) as new_customers,
        count(distinct case when movement_type = 'expansion' then user_id end) as expansion_customers,
        count(distinct case when movement_type = 'contraction' then user_id end) as contraction_customers,
        count(distinct case when movement_type = 'churn' then user_id end) as churned_customers,
        count(distinct case when movement_type = 'retained' then user_id end) as retained_customers,
        count(distinct case when is_reactivation then user_id end) as reactivated_customers,

        -- MRR amounts
        sum(case when movement_type = 'new' then mrr_this_month else 0 end) as new_mrr,
        sum(case when movement_type = 'expansion' then mrr_change else 0 end) as expansion_mrr,
        sum(case when movement_type = 'contraction' then abs(mrr_change) else 0 end) as contraction_mrr,
        sum(case when movement_type = 'churn' then mrr_last_month else 0 end) as churned_mrr,
        sum(case when movement_type = 'retained' then mrr_this_month else 0 end) as retained_mrr,

        -- total MRR (sum of all users with MRR > 0 this month)
        sum(case when mrr_this_month > 0 then mrr_this_month else 0 end) as total_mrr,
        sum(case when mrr_last_month > 0 then mrr_last_month else 0 end) as prior_month_mrr

    from mrr_changes
    where movement_type != 'inactive'  -- exclude users with no MRR in both months
    group by month_date

),

-- Calculate growth rates and net change
final as (

    select
        month_date,

        -- customer counts
        new_customers,
        expansion_customers,
        contraction_customers,
        churned_customers,
        retained_customers,
        reactivated_customers,
        new_customers + expansion_customers + contraction_customers + churned_customers + retained_customers as total_customers,

        -- MRR amounts
        new_mrr,
        expansion_mrr,
        contraction_mrr,
        churned_mrr,
        retained_mrr,
        total_mrr,
        prior_month_mrr,

        -- net change
        new_mrr + expansion_mrr - contraction_mrr - churned_mrr as net_mrr_change,

        -- growth rates
        case 
            when prior_month_mrr > 0 
            then (new_mrr + expansion_mrr - contraction_mrr - churned_mrr) / prior_month_mrr * 100
            else null
        end as mrr_growth_rate,

        case
            when prior_month_mrr > 0
            then churned_mrr / prior_month_mrr * 100
            else null
        end as churn_rate

    from monthly_movements

)

select * from final
order by month_date
    );
  
  
[0m12:29:05.477398 [debug] [Thread-1 (]: SQL status: OK in 0.026 seconds
[0m12:29:05.480315 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.480517 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:29:05.481016 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:29:05.481622 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.481785 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_mrr_movements'
  
[0m12:29:05.482271 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:29:05.486075 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.486306 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements" rename to "rpt_mrr_movements__dbt_backup"
[0m12:29:05.491018 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m12:29:05.492582 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.492753 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */
alter table "analytics"."main"."rpt_mrr_movements__dbt_tmp" rename to "rpt_mrr_movements"
[0m12:29:05.493067 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:29:05.500334 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:29:05.500526 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.500677 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: COMMIT
[0m12:29:05.501341 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m12:29:05.503968 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_mrr_movements"
[0m12:29:05.504151 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_mrr_movements"} */

      drop table if exists "analytics"."main"."rpt_mrr_movements__dbt_backup" cascade
    
[0m12:29:05.504799 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m12:29:05.505953 [debug] [Thread-1 (]: On model.saas_analytics.rpt_mrr_movements: Close
[0m12:29:05.506893 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a34d31f4-e796-4c20-86a9-7c2a474ce909', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060ead80>]}
[0m12:29:05.507213 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.rpt_mrr_movements ....................... [[32mOK[0m in 0.08s]
[0m12:29:05.507479 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_mrr_movements
[0m12:29:05.508077 [debug] [MainThread]: Using duckdb connection "master"
[0m12:29:05.508241 [debug] [MainThread]: On master: BEGIN
[0m12:29:05.508364 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:29:05.508619 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:29:05.508747 [debug] [MainThread]: On master: COMMIT
[0m12:29:05.508867 [debug] [MainThread]: Using duckdb connection "master"
[0m12:29:05.508976 [debug] [MainThread]: On master: COMMIT
[0m12:29:05.509163 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:29:05.509283 [debug] [MainThread]: On master: Close
[0m12:29:05.509441 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:29:05.509557 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_mrr_movements' was properly closed.
[0m12:29:05.509676 [info ] [MainThread]: 
[0m12:29:05.509822 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m12:29:05.510106 [debug] [MainThread]: Command end result
[0m12:29:05.530662 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m12:29:05.531878 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m12:29:05.534993 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m12:29:05.535179 [info ] [MainThread]: 
[0m12:29:05.535383 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:29:05.535523 [info ] [MainThread]: 
[0m12:29:05.535673 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:29:05.537079 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.94735664, "process_in_blocks": "0", "process_kernel_time": 0.237075, "process_mem_max_rss": "215138304", "process_out_blocks": "0", "process_user_time": 1.331196}
[0m12:29:05.537288 [debug] [MainThread]: Command `dbt run` succeeded at 12:29:05.537251 after 0.95 seconds
[0m12:29:05.537459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a3db20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10879b740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084d6990>]}
[0m12:29:05.537627 [debug] [MainThread]: Flushing usage events
[0m12:29:05.681678 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:33:19.572281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104df2cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105611a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056117c0>]}


============================== 11:33:19.603395 | cc5b9b18-2d29-47dd-a9db-4b1241d682fa ==============================
[0m11:33:19.603395 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:33:19.603777 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'use_colors': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select path:models/marts/product', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'target_path': 'None', 'empty': 'False', 'debug': 'False', 'fail_fast': 'False', 'write_json': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'log_format': 'default', 'no_print': 'None', 'partial_parse': 'True'}
[0m11:33:20.102462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102cdc290>]}
[0m11:33:20.131384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c50d0>]}
[0m11:33:20.132731 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:33:20.196611 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:33:20.277243 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m11:33:20.277676 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/product/rpt_activation_funnel.sql
[0m11:33:20.277872 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/product/rpt_feature_adoption.sql
[0m11:33:20.278026 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/product/rpt_user_engagement_score.sql
[0m11:33:20.278256 [debug] [MainThread]: Partial parsing: added file: saas_analytics://models/marts/product/product.yml
[0m11:33:20.522967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dccfe0>]}
[0m11:33:20.572321 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:33:20.574339 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:33:20.591135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d23080>]}
[0m11:33:20.591424 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:33:20.591604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d091f0>]}
[0m11:33:20.594053 [info ] [MainThread]: 
[0m11:33:20.594237 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:33:20.594377 [info ] [MainThread]: 
[0m11:33:20.594622 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:33:20.596896 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:33:20.639949 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:33:20.640211 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:33:20.640374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:20.653277 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m11:33:20.654111 [debug] [ThreadPool]: On list_analytics: Close
[0m11:33:20.654515 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:33:20.654785 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:33:20.658308 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:33:20.658506 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:33:20.658648 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:20.659315 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:33:20.660071 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:33:20.660312 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:33:20.661093 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:33:20.661397 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:33:20.661600 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:33:20.661960 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:33:20.662521 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:33:20.662683 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:33:20.662816 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:33:20.663060 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:33:20.663223 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:33:20.664370 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:33:20.667389 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:33:20.667578 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:33:20.667777 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:20.668111 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:33:20.668242 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:33:20.668374 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:33:20.676446 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:33:20.677364 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:33:20.678291 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:33:20.678461 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:33:20.679748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d8bad0>]}
[0m11:33:20.679991 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:20.680128 [debug] [MainThread]: On master: BEGIN
[0m11:33:20.680256 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:33:20.680536 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:20.680664 [debug] [MainThread]: On master: COMMIT
[0m11:33:20.680787 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:20.680907 [debug] [MainThread]: On master: COMMIT
[0m11:33:20.681116 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:20.681245 [debug] [MainThread]: On master: Close
[0m11:33:20.682680 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:33:20.682865 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:33:20.683251 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:33:20.683115 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:33:20.683530 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:33:20.683768 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:33:20.683997 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:33:20.684244 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:33:20.684458 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:33:20.684623 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:33:20.684880 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:33:20.685044 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:33:20.689123 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:33:20.691908 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:33:20.693848 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:33:20.694768 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:33:20.694963 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:33:20.701396 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:33:20.712952 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:33:20.714253 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:33:20.715859 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:33:20.716508 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:33:20.716713 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:33:20.716875 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:33:20.717187 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:33:20.717373 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:33:20.717524 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:33:20.717814 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:33:20.718007 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:33:20.718156 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:33:20.718324 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:33:20.718475 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:33:20.718624 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:33:20.718849 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:33:20.719132 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgrade_at, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:33:20.719542 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscription,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:33:20.719806 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:33:20.721581 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgrade_at, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:33:20.722174 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:33:20.722460 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:20.722789 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:33:20.723227 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: ROLLBACK
[0m11:33:20.723531 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscription,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:33:20.724332 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:20.725297 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: ROLLBACK
[0m11:33:20.726337 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:33:20.726674 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:20.726957 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: ROLLBACK
[0m11:33:20.731829 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_feature_adoption'
[0m11:33:20.732024 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:33:20.732773 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.rpt_user_engagement_score'
[0m11:33:20.733515 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_activation_funnel'
[0m11:33:20.734170 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:33:20.734358 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:33:20.735098 [debug] [Thread-2 (]: Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^
[0m11:33:20.737462 [debug] [Thread-3 (]: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:33:20.738308 [debug] [Thread-1 (]: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgrade_at, false) as 'Full Activated (Paid)'
                                                           ^
[0m11:33:20.738558 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10635dbe0>]}
[0m11:33:20.738720 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e237d0>]}
[0m11:33:20.738865 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc5b9b18-2d29-47dd-a9db-4b1241d682fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10635d9d0>]}
[0m11:33:20.739169 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_feature_adoption ................ [[31mERROR[0m in 0.05s]
[0m11:33:20.739919 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:33:20.739404 [error] [Thread-3 (]: 3 of 3 ERROR creating sql table model main.rpt_user_engagement_score ........... [[31mERROR[0m in 0.05s]
[0m11:33:20.740197 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_feature_adoption' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^.
[0m11:33:20.740436 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:33:20.739689 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.rpt_activation_funnel ............... [[31mERROR[0m in 0.05s]
[0m11:33:20.741013 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_user_engagement_score' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^.
[0m11:33:20.741255 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:33:20.741498 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_activation_funnel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgrade_at, false) as 'Full Activated (Paid)'
                                                           ^.
[0m11:33:20.742103 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:20.742245 [debug] [MainThread]: On master: BEGIN
[0m11:33:20.742366 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:33:20.742871 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:20.743015 [debug] [MainThread]: On master: COMMIT
[0m11:33:20.743147 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:20.743269 [debug] [MainThread]: On master: COMMIT
[0m11:33:20.743498 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:20.743647 [debug] [MainThread]: On master: Close
[0m11:33:20.743825 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:33:20.743943 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:33:20.744051 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:33:20.744155 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:33:20.744310 [info ] [MainThread]: 
[0m11:33:20.744453 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m11:33:20.744969 [debug] [MainThread]: Command end result
[0m11:33:20.767797 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:33:20.769112 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:33:20.772475 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:33:20.772634 [info ] [MainThread]: 
[0m11:33:20.772806 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:33:20.772951 [info ] [MainThread]: 
[0m11:33:20.773125 [error] [MainThread]: [31mFailure in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)[0m
[0m11:33:20.773294 [error] [MainThread]:   Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^
[0m11:33:20.773423 [info ] [MainThread]: 
[0m11:33:20.773575 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_feature_adoption.sql
[0m11:33:20.773708 [info ] [MainThread]: 
[0m11:33:20.773863 [error] [MainThread]: [31mFailure in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)[0m
[0m11:33:20.774026 [error] [MainThread]:   Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:33:20.774147 [info ] [MainThread]: 
[0m11:33:20.774299 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_user_engagement_score.sql
[0m11:33:20.774422 [info ] [MainThread]: 
[0m11:33:20.774571 [error] [MainThread]: [31mFailure in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)[0m
[0m11:33:20.774726 [error] [MainThread]:   Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgrade_at, false) as 'Full Activated (Paid)'
                                                           ^
[0m11:33:20.774845 [info ] [MainThread]: 
[0m11:33:20.774987 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_activation_funnel.sql
[0m11:33:20.775106 [info ] [MainThread]: 
[0m11:33:20.775248 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m11:33:20.795599 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.2664611, "process_in_blocks": "0", "process_kernel_time": 0.234827, "process_mem_max_rss": "152436736", "process_out_blocks": "0", "process_user_time": 1.390331}
[0m11:33:20.795961 [debug] [MainThread]: Command `dbt run` failed at 11:33:20.795910 after 1.27 seconds
[0m11:33:20.796174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105611880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048f2f00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10627eae0>]}
[0m11:33:20.796371 [debug] [MainThread]: Flushing usage events
[0m11:33:20.930904 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:34:34.563632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051bdd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10582da00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10582d790>]}


============================== 11:34:34.569364 | dfbe13ae-d9cf-40f1-807b-782bd37a35e8 ==============================
[0m11:34:34.569364 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:34:34.569703 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'static_parser': 'True', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'cache_selected_only': 'False', 'target_path': 'None', 'log_format': 'default', 'partial_parse': 'True', 'invocation_command': 'dbt run --select path:models/marts/product', 'empty': 'False', 'use_colors': 'True', 'printer_width': '80', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager'}
[0m11:34:34.720930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057c61e0>]}
[0m11:34:34.750307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046ca000>]}
[0m11:34:34.751659 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:34:34.803436 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:34:34.877034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:34:34.877529 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_activation_funnel.sql
[0m11:34:35.104784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105efdeb0>]}
[0m11:34:35.153576 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:34:35.156778 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:34:35.171657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106028a40>]}
[0m11:34:35.171951 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:34:35.172139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c67b60>]}
[0m11:34:35.174392 [info ] [MainThread]: 
[0m11:34:35.174606 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:34:35.174790 [info ] [MainThread]: 
[0m11:34:35.175031 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:34:35.177270 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:34:35.209056 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:34:35.209308 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:34:35.209462 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:35.226840 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m11:34:35.227622 [debug] [ThreadPool]: On list_analytics: Close
[0m11:34:35.227972 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:34:35.228228 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:34:35.231626 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:34:35.231809 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:34:35.231942 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:34:35.232628 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:34:35.233243 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:34:35.233385 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:34:35.233607 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:35.233734 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:34:35.233861 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:34:35.234251 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:35.234637 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:34:35.234774 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:34:35.234890 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:34:35.235092 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:35.235248 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:34:35.236327 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:34:35.239000 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:34:35.239157 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:34:35.239334 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:34:35.239575 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:35.239705 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:34:35.239837 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:34:35.249724 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:34:35.250527 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:34:35.251460 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:34:35.251606 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:34:35.252923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f68ef0>]}
[0m11:34:35.253177 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:35.253322 [debug] [MainThread]: On master: BEGIN
[0m11:34:35.253450 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:34:35.253703 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:35.253839 [debug] [MainThread]: On master: COMMIT
[0m11:34:35.253963 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:35.254083 [debug] [MainThread]: On master: COMMIT
[0m11:34:35.254285 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:35.254412 [debug] [MainThread]: On master: Close
[0m11:34:35.255796 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:34:35.255984 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:34:35.256373 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:34:35.256233 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:34:35.256627 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:34:35.256841 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:34:35.257068 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:34:35.257311 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:34:35.257532 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:34:35.257695 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:34:35.257940 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:34:35.258102 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:34:35.262076 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:34:35.263828 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:34:35.265997 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:34:35.266986 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:34:35.267203 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:34:35.267403 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:34:35.286344 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:34:35.286741 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:34:35.288310 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:34:35.288938 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:34:35.289140 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:34:35.289304 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:35.289620 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:34:35.289791 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:34:35.289938 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:34:35.290187 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:34:35.290376 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:34:35.290528 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:34:35.290776 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:34:35.290930 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:34:35.291161 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:34:35.291404 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:34:35.291555 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:34:35.291843 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:34:35.295187 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:34:35.295464 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:34:35.295621 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:35.295789 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:34:35.296037 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: ROLLBACK
[0m11:34:35.296265 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscription,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:34:35.297962 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:34:35.298283 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:35.298565 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscription,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:34:35.298828 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: ROLLBACK
[0m11:34:35.298999 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:35.299849 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: ROLLBACK
[0m11:34:35.304777 [debug] [Thread-2 (]: Failed to rollback 'model.saas_analytics.rpt_feature_adoption'
[0m11:34:35.304980 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:34:35.305706 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_activation_funnel'
[0m11:34:35.306394 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:34:35.307632 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.rpt_user_engagement_score'
[0m11:34:35.307801 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:34:35.309012 [debug] [Thread-3 (]: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:34:35.309971 [debug] [Thread-1 (]: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^
[0m11:34:35.311422 [debug] [Thread-2 (]: Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^
[0m11:34:35.311836 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106556870>]}
[0m11:34:35.311998 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10653c170>]}
[0m11:34:35.312189 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dfbe13ae-d9cf-40f1-807b-782bd37a35e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064bc140>]}
[0m11:34:35.312446 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.rpt_activation_funnel ............... [[31mERROR[0m in 0.05s]
[0m11:34:35.312683 [error] [Thread-3 (]: 3 of 3 ERROR creating sql table model main.rpt_user_engagement_score ........... [[31mERROR[0m in 0.05s]
[0m11:34:35.313228 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:34:35.312964 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model main.rpt_feature_adoption ................ [[31mERROR[0m in 0.05s]
[0m11:34:35.313499 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:34:35.313721 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_activation_funnel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^.
[0m11:34:35.313959 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:34:35.314477 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_user_engagement_score' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^.
[0m11:34:35.314721 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_feature_adoption' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^.
[0m11:34:35.315411 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:35.315596 [debug] [MainThread]: On master: BEGIN
[0m11:34:35.315751 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:34:35.316161 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:35.316378 [debug] [MainThread]: On master: COMMIT
[0m11:34:35.316514 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:35.316641 [debug] [MainThread]: On master: COMMIT
[0m11:34:35.316857 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:35.316989 [debug] [MainThread]: On master: Close
[0m11:34:35.317160 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:34:35.317279 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:34:35.317390 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:34:35.317495 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:34:35.317641 [info ] [MainThread]: 
[0m11:34:35.317784 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m11:34:35.318198 [debug] [MainThread]: Command end result
[0m11:34:35.340155 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:34:35.341353 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:34:35.344062 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:34:35.344225 [info ] [MainThread]: 
[0m11:34:35.344404 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:34:35.344554 [info ] [MainThread]: 
[0m11:34:35.344730 [error] [MainThread]: [31mFailure in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)[0m
[0m11:34:35.344900 [error] [MainThread]:   Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^
[0m11:34:35.345029 [info ] [MainThread]: 
[0m11:34:35.345177 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_activation_funnel.sql
[0m11:34:35.345305 [info ] [MainThread]: 
[0m11:34:35.345459 [error] [MainThread]: [31mFailure in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)[0m
[0m11:34:35.345614 [error] [MainThread]:   Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:34:35.345734 [info ] [MainThread]: 
[0m11:34:35.345877 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_user_engagement_score.sql
[0m11:34:35.345997 [info ] [MainThread]: 
[0m11:34:35.346144 [error] [MainThread]: [31mFailure in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)[0m
[0m11:34:35.346295 [error] [MainThread]:   Runtime Error in model rpt_feature_adoption (models/marts/product/rpt_feature_adoption.sql)
  Binder Error: Values list "u" does not have a column named "is_currently_subscription"
  
  LINE 43:         u.is_currently_subscription,
                   ^
[0m11:34:35.346416 [info ] [MainThread]: 
[0m11:34:35.346555 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_feature_adoption.sql
[0m11:34:35.346673 [info ] [MainThread]: 
[0m11:34:35.346810 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m11:34:35.348208 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.82819897, "process_in_blocks": "0", "process_kernel_time": 0.199257, "process_mem_max_rss": "150519808", "process_out_blocks": "0", "process_user_time": 1.310755}
[0m11:34:35.348461 [debug] [MainThread]: Command `dbt run` failed at 11:34:35.348416 after 0.83 seconds
[0m11:34:35.348647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b0ec60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051bdd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106148800>]}
[0m11:34:35.348812 [debug] [MainThread]: Flushing usage events
[0m11:34:35.467882 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:35:25.913297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fb4740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105729a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057297c0>]}


============================== 11:35:25.919642 | 0f681449-a1f2-4f21-9c1b-963cc33954b5 ==============================
[0m11:35:25.919642 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:35:25.920029 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'fail_fast': 'False', 'write_json': 'True', 'log_format': 'default', 'no_print': 'None', 'empty': 'False', 'target_path': 'None', 'debug': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select path:models/marts/product', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'static_parser': 'True', 'use_experimental_parser': 'False', 'version_check': 'True', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'introspect': 'True', 'printer_width': '80'}
[0m11:35:26.084716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102f5daf0>]}
[0m11:35:26.115142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101c26810>]}
[0m11:35:26.116652 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:35:26.172873 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:35:26.250973 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:35:26.251522 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_feature_adoption.sql
[0m11:35:26.492700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fd1730>]}
[0m11:35:26.541812 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:35:26.543313 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:35:26.557565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f28d70>]}
[0m11:35:26.557844 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:35:26.558028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e6e000>]}
[0m11:35:26.560219 [info ] [MainThread]: 
[0m11:35:26.560400 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:35:26.560538 [info ] [MainThread]: 
[0m11:35:26.560758 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:35:26.562920 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:35:26.592249 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:35:26.592488 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:35:26.592642 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:35:26.612222 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:35:26.613213 [debug] [ThreadPool]: On list_analytics: Close
[0m11:35:26.613555 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:35:26.613771 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:35:26.617234 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:35:26.617418 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:35:26.617551 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:35:26.618320 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:35:26.619014 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:35:26.619192 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:35:26.619439 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:35:26.619579 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:35:26.619709 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:35:26.620193 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:35:26.620670 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:35:26.620831 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:35:26.620964 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:35:26.621176 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:35:26.621312 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:35:26.622285 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:35:26.624984 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:35:26.625149 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:35:26.625329 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:35:26.625572 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:35:26.625704 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:35:26.625835 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:35:26.636807 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:35:26.637659 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:35:26.638642 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:35:26.638808 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:35:26.640060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063672c0>]}
[0m11:35:26.640302 [debug] [MainThread]: Using duckdb connection "master"
[0m11:35:26.640443 [debug] [MainThread]: On master: BEGIN
[0m11:35:26.640565 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:35:26.640813 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:35:26.640942 [debug] [MainThread]: On master: COMMIT
[0m11:35:26.641066 [debug] [MainThread]: Using duckdb connection "master"
[0m11:35:26.641184 [debug] [MainThread]: On master: COMMIT
[0m11:35:26.641379 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:35:26.641505 [debug] [MainThread]: On master: Close
[0m11:35:26.642899 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:35:26.643094 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:35:26.643268 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:35:26.643494 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:35:26.643694 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:35:26.644122 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:35:26.643913 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:35:26.644380 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:35:26.644546 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:35:26.644737 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:35:26.644984 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:35:26.648944 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:35:26.649143 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:35:26.650831 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.653724 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:35:26.654359 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:35:26.654573 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:35:26.654755 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:35:26.670632 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.672242 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:35:26.673831 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:35:26.674393 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:35:26.674562 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:35:26.674709 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:35:26.675019 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.675200 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:35:26.675354 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:35:26.675611 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:35:26.675762 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:35:26.675902 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:35:26.676171 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:35:26.676355 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.676575 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscribed,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:35:26.677020 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m11:35:26.677178 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:35:26.677467 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:35:26.678458 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:35:26.678615 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:35:26.678855 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:35:26.681389 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) as 'Feature User'
            when coalesce(um.completed_onboarding, false) as 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:35:26.681701 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:35:26.681947 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: ROLLBACK
[0m11:35:26.683681 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:35:26.684048 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:35:26.684280 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: ROLLBACK
[0m11:35:26.689660 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.rpt_user_engagement_score'
[0m11:35:26.690005 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:35:26.690785 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_activation_funnel'
[0m11:35:26.691504 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:35:26.692968 [debug] [Thread-3 (]: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:35:26.693650 [debug] [Thread-1 (]: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^
[0m11:35:26.694823 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049a3380>]}
[0m11:35:26.694992 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10643ac60>]}
[0m11:35:26.695334 [error] [Thread-3 (]: 3 of 3 ERROR creating sql table model main.rpt_user_engagement_score ........... [[31mERROR[0m in 0.05s]
[0m11:35:26.696033 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:35:26.695749 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.rpt_activation_funnel ............... [[31mERROR[0m in 0.05s]
[0m11:35:26.696324 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_user_engagement_score' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^.
[0m11:35:26.696596 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:35:26.697190 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_activation_funnel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^.
[0m11:35:26.699087 [debug] [Thread-2 (]: SQL status: OK in 0.022 seconds
[0m11:35:26.702783 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.703051 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption__dbt_tmp" rename to "rpt_feature_adoption"
[0m11:35:26.705052 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:35:26.711608 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:35:26.711823 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.711981 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:35:26.715221 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:35:26.717936 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:35:26.718123 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

      drop table if exists "analytics"."main"."rpt_feature_adoption__dbt_backup" cascade
    
[0m11:35:26.718680 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:35:26.719843 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:35:26.720129 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f681449-a1f2-4f21-9c1b-963cc33954b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102dd3080>]}
[0m11:35:26.720425 [info ] [Thread-2 (]: 2 of 3 OK created sql table model main.rpt_feature_adoption .................... [[32mOK[0m in 0.08s]
[0m11:35:26.720681 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:35:26.721262 [debug] [MainThread]: Using duckdb connection "master"
[0m11:35:26.721411 [debug] [MainThread]: On master: BEGIN
[0m11:35:26.721534 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:35:26.721789 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:35:26.721913 [debug] [MainThread]: On master: COMMIT
[0m11:35:26.722032 [debug] [MainThread]: Using duckdb connection "master"
[0m11:35:26.722146 [debug] [MainThread]: On master: COMMIT
[0m11:35:26.722325 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:35:26.722444 [debug] [MainThread]: On master: Close
[0m11:35:26.722598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:35:26.722718 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:35:26.722822 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:35:26.722924 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:35:26.723067 [info ] [MainThread]: 
[0m11:35:26.723207 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m11:35:26.723602 [debug] [MainThread]: Command end result
[0m11:35:26.745424 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:35:26.746565 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:35:26.749241 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:35:26.749404 [info ] [MainThread]: 
[0m11:35:26.749583 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:35:26.749733 [info ] [MainThread]: 
[0m11:35:26.749917 [error] [MainThread]: [31mFailure in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)[0m
[0m11:35:26.750090 [error] [MainThread]:   Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:35:26.750222 [info ] [MainThread]: 
[0m11:35:26.750373 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_user_engagement_score.sql
[0m11:35:26.750504 [info ] [MainThread]: 
[0m11:35:26.750662 [error] [MainThread]: [31mFailure in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)[0m
[0m11:35:26.750820 [error] [MainThread]:   Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Parser Error: syntax error at or near "as"
  
  LINE 79:             when coalesce(um.upgraded, false) as 'Full Activated (Paid)'
                                                         ^
[0m11:35:26.750945 [info ] [MainThread]: 
[0m11:35:26.751089 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_activation_funnel.sql
[0m11:35:26.751210 [info ] [MainThread]: 
[0m11:35:26.751356 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m11:35:26.752782 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8799265, "process_in_blocks": "0", "process_kernel_time": 0.199765, "process_mem_max_rss": "174211072", "process_out_blocks": "0", "process_user_time": 1.342656}
[0m11:35:26.753041 [debug] [MainThread]: Command `dbt run` failed at 11:35:26.752997 after 0.88 seconds
[0m11:35:26.753229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057297f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062796a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a09a60>]}
[0m11:35:26.753397 [debug] [MainThread]: Flushing usage events
[0m11:35:26.882507 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:17.972674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089a5970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089a5700>]}


============================== 11:36:17.993260 | b9679af9-ce77-4c2d-bf91-2f7cfe8469a4 ==============================
[0m11:36:17.993260 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:36:17.993630 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'introspect': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'quiet': 'False', 'write_json': 'True', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'cache_selected_only': 'False', 'invocation_command': 'dbt run --select path:models/marts/product', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'log_cache_events': 'False', 'fail_fast': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'empty': 'False', 'indirect_selection': 'eager', 'printer_width': '80'}
[0m11:36:18.220157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b96240>]}
[0m11:36:18.248947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d7680>]}
[0m11:36:18.250350 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:36:18.302876 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:36:18.377658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:36:18.378156 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_activation_funnel.sql
[0m11:36:18.604446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10926f890>]}
[0m11:36:18.662186 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:36:18.665343 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:36:18.699180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091ff890>]}
[0m11:36:18.699591 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:36:18.699877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090f4bf0>]}
[0m11:36:18.702966 [info ] [MainThread]: 
[0m11:36:18.703361 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:36:18.703556 [info ] [MainThread]: 
[0m11:36:18.703880 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:36:18.710552 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:36:18.753437 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:36:18.753678 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:36:18.753826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:18.772439 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:36:18.773228 [debug] [ThreadPool]: On list_analytics: Close
[0m11:36:18.773581 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:36:18.773836 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:36:18.777145 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:36:18.777318 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:36:18.777454 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:36:18.778305 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:36:18.778938 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:36:18.779082 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:36:18.779310 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:18.779439 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:36:18.779563 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:36:18.779812 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:18.780185 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:36:18.780650 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:36:18.780788 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:36:18.781024 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:18.781249 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:36:18.782225 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:36:18.784899 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:36:18.785216 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:36:18.785482 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:36:18.785890 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:18.786023 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:36:18.786158 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:36:18.796595 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:36:18.797490 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:36:18.798732 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:36:18.798892 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:36:18.800171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091fe7b0>]}
[0m11:36:18.800428 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:18.800566 [debug] [MainThread]: On master: BEGIN
[0m11:36:18.800685 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:36:18.800928 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:18.801059 [debug] [MainThread]: On master: COMMIT
[0m11:36:18.801181 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:18.801298 [debug] [MainThread]: On master: COMMIT
[0m11:36:18.801483 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:18.801614 [debug] [MainThread]: On master: Close
[0m11:36:18.803039 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:36:18.803227 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:36:18.803605 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:36:18.803466 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:36:18.803858 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:36:18.804102 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:36:18.804322 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:36:18.804540 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:36:18.804814 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:36:18.804980 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:36:18.805131 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:36:18.805280 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:36:18.809257 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:36:18.811033 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.813224 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:36:18.813883 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:36:18.814084 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:36:18.814311 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:36:18.832663 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:36:18.833826 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:36:18.835536 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.836166 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:36:18.836346 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:36:18.836502 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:36:18.836805 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:36:18.836983 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:36:18.837135 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:36:18.837392 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.837546 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:36:18.837683 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:36:18.837941 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:18.838098 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:36:18.838336 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) then 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) then 'Feature User'
            when coalesce(um.completed_onboarding, false) then 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:36:18.838863 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m11:36:18.839009 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:36:18.839289 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:36:18.840309 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:36:18.840485 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.840708 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscribed,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:36:18.843538 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:36:18.844046 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) then 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) then 'Feature User'
            when coalesce(um.completed_onboarding, false) then 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:36:18.844322 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:36:18.844506 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:36:18.844755 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: ROLLBACK
[0m11:36:18.844983 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: ROLLBACK
[0m11:36:18.851372 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.rpt_user_engagement_score'
[0m11:36:18.852328 [debug] [Thread-1 (]: Failed to rollback 'model.saas_analytics.rpt_activation_funnel'
[0m11:36:18.852545 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:36:18.852750 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:36:18.854706 [debug] [Thread-3 (]: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:36:18.855478 [debug] [Thread-1 (]: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Binder Error: Values list "u" does not have a column named "upgraded"
  
  LINE 90:         (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
                                       ^
[0m11:36:18.856604 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096b10a0>]}
[0m11:36:18.856776 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096b3e90>]}
[0m11:36:18.857115 [error] [Thread-3 (]: 3 of 3 ERROR creating sql table model main.rpt_user_engagement_score ........... [[31mERROR[0m in 0.05s]
[0m11:36:18.857562 [error] [Thread-1 (]: 1 of 3 ERROR creating sql table model main.rpt_activation_funnel ............... [[31mERROR[0m in 0.05s]
[0m11:36:18.857911 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:36:18.858393 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:36:18.858890 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_user_engagement_score' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^.
[0m11:36:18.859142 [debug] [Thread-2 (]: SQL status: OK in 0.018 seconds
[0m11:36:18.859865 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_activation_funnel' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Binder Error: Values list "u" does not have a column named "upgraded"
  
  LINE 90:         (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
                                       ^.
[0m11:36:18.862789 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.863072 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:36:18.863610 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:18.864348 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.864644 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:36:18.865415 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:18.868822 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.869054 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption" rename to "rpt_feature_adoption__dbt_backup"
[0m11:36:18.869762 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:18.871468 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.871667 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption__dbt_tmp" rename to "rpt_feature_adoption"
[0m11:36:18.872028 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:18.879077 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:36:18.879271 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.879425 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:36:18.882317 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:36:18.885028 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:36:18.885230 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

      drop table if exists "analytics"."main"."rpt_feature_adoption__dbt_backup" cascade
    
[0m11:36:18.886114 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:18.887316 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:36:18.887593 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9679af9-ce77-4c2d-bf91-2f7cfe8469a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096375c0>]}
[0m11:36:18.887880 [info ] [Thread-2 (]: 2 of 3 OK created sql table model main.rpt_feature_adoption .................... [[32mOK[0m in 0.08s]
[0m11:36:18.888134 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:36:18.888708 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:18.888863 [debug] [MainThread]: On master: BEGIN
[0m11:36:18.888995 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:36:18.889248 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:18.889378 [debug] [MainThread]: On master: COMMIT
[0m11:36:18.889501 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:18.889621 [debug] [MainThread]: On master: COMMIT
[0m11:36:18.889819 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:18.889943 [debug] [MainThread]: On master: Close
[0m11:36:18.890099 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:36:18.890220 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:36:18.890328 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:36:18.890426 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:36:18.890565 [info ] [MainThread]: 
[0m11:36:18.890701 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m11:36:18.891105 [debug] [MainThread]: Command end result
[0m11:36:18.917337 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:36:18.918484 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:36:18.921266 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:36:18.921432 [info ] [MainThread]: 
[0m11:36:18.921611 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:36:18.921755 [info ] [MainThread]: 
[0m11:36:18.921937 [error] [MainThread]: [31mFailure in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)[0m
[0m11:36:18.922111 [error] [MainThread]:   Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:36:18.922239 [info ] [MainThread]: 
[0m11:36:18.922395 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_user_engagement_score.sql
[0m11:36:18.922529 [info ] [MainThread]: 
[0m11:36:18.922683 [error] [MainThread]: [31mFailure in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)[0m
[0m11:36:18.922843 [error] [MainThread]:   Runtime Error in model rpt_activation_funnel (models/marts/product/rpt_activation_funnel.sql)
  Binder Error: Values list "u" does not have a column named "upgraded"
  
  LINE 90:         (case when coalesce(u.upgraded, false) then 1 else 0 end) as activation_score
                                       ^
[0m11:36:18.922968 [info ] [MainThread]: 
[0m11:36:18.923111 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_activation_funnel.sql
[0m11:36:18.923234 [info ] [MainThread]: 
[0m11:36:18.923380 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=3
[0m11:36:18.924794 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.991084, "process_in_blocks": "0", "process_kernel_time": 0.213547, "process_mem_max_rss": "172605440", "process_out_blocks": "0", "process_user_time": 1.360533}
[0m11:36:18.925041 [debug] [MainThread]: Command `dbt run` failed at 11:36:18.924996 after 0.99 seconds
[0m11:36:18.925236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089a57c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109500ce0>]}
[0m11:36:18.925416 [debug] [MainThread]: Flushing usage events
[0m11:36:19.031077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:37:07.870361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e70f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094b5c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1094b5a00>]}


============================== 11:37:07.898247 | 099b54f7-fea1-4201-9a7d-baa7b213414d ==============================
[0m11:37:07.898247 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:37:07.898599 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'quiet': 'False', 'no_print': 'None', 'write_json': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select path:models/marts/product', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'indirect_selection': 'eager', 'debug': 'False', 'partial_parse': 'True', 'target_path': 'None', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'fail_fast': 'False', 'printer_width': '80', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'version_check': 'True', 'static_parser': 'True', 'empty': 'False', 'cache_selected_only': 'False'}
[0m11:37:08.062655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10951d700>]}
[0m11:37:08.091458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109591610>]}
[0m11:37:08.092812 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:37:08.144904 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:37:08.220947 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:37:08.221443 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_activation_funnel.sql
[0m11:37:08.448627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d87860>]}
[0m11:37:08.496809 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:37:08.498149 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:37:08.513815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bfe270>]}
[0m11:37:08.514101 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:37:08.514277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d08aa0>]}
[0m11:37:08.516494 [info ] [MainThread]: 
[0m11:37:08.516677 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:37:08.516822 [info ] [MainThread]: 
[0m11:37:08.517051 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:37:08.519197 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:37:08.548498 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:37:08.548730 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:37:08.548876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:08.567998 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:37:08.568974 [debug] [ThreadPool]: On list_analytics: Close
[0m11:37:08.569322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:37:08.569573 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:37:08.573026 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:08.573233 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:37:08.573374 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:08.574191 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:37:08.574807 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:08.574944 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:37:08.575165 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:08.575291 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:08.575415 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:37:08.575870 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:08.576240 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:37:08.576369 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:08.576483 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:37:08.576681 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:08.576811 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:37:08.577766 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:37:08.580420 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:37:08.580573 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:37:08.580750 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:08.580991 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:08.581121 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:37:08.581254 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:37:08.592436 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:37:08.593470 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:37:08.594445 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:37:08.594605 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:37:08.595878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109cb6a80>]}
[0m11:37:08.596129 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:08.596271 [debug] [MainThread]: On master: BEGIN
[0m11:37:08.596393 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:37:08.596655 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:08.596788 [debug] [MainThread]: On master: COMMIT
[0m11:37:08.596911 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:08.597027 [debug] [MainThread]: On master: COMMIT
[0m11:37:08.597217 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:08.597343 [debug] [MainThread]: On master: Close
[0m11:37:08.598740 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:37:08.598924 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:37:08.599291 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:37:08.599155 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:37:08.599567 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:37:08.599819 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:37:08.600055 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:37:08.600281 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:37:08.600559 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:37:08.600726 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:37:08.600886 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:37:08.601037 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:37:08.605004 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.606743 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.608962 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:08.609748 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:37:08.622236 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:37:08.628719 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.629170 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.629407 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:37:08.631013 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:08.631729 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.631934 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:08.632114 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.632268 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:37:08.632425 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:37:08.632575 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:37:08.632719 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:37:08.632860 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:37:08.632999 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:37:08.633443 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:08.633609 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:37:08.633771 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.633915 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:37:08.634066 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:08.634271 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscribed,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:37:08.634478 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.634762 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:37:08.635499 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) then 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) then 'Feature User'
            when coalesce(um.completed_onboarding, false) then 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(um.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:37:08.641621 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with user as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:37:08.642031 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:37:08.642333 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: ROLLBACK
[0m11:37:08.648112 [debug] [Thread-3 (]: Failed to rollback 'model.saas_analytics.rpt_user_engagement_score'
[0m11:37:08.648402 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:37:08.649942 [debug] [Thread-3 (]: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:37:08.651218 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e853a0>]}
[0m11:37:08.651603 [error] [Thread-3 (]: 3 of 3 ERROR creating sql table model main.rpt_user_engagement_score ........... [[31mERROR[0m in 0.05s]
[0m11:37:08.651962 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:37:08.652215 [debug] [Thread-7 (]: Marking all children of 'model.saas_analytics.rpt_user_engagement_score' to be skipped because of status 'error'.  Reason: Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^.
[0m11:37:08.653973 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m11:37:08.657317 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.657529 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */
alter table "analytics"."main"."rpt_activation_funnel__dbt_tmp" rename to "rpt_activation_funnel"
[0m11:37:08.657705 [debug] [Thread-2 (]: SQL status: OK in 0.023 seconds
[0m11:37:08.661476 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.661739 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:37:08.661925 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:37:08.668388 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: COMMIT
[0m11:37:08.668617 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m11:37:08.668812 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.669030 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: COMMIT
[0m11:37:08.669665 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.669914 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:37:08.670956 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:08.672919 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.673109 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption" rename to "rpt_feature_adoption__dbt_backup"
[0m11:37:08.673355 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:37:08.676069 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:08.676263 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

      drop table if exists "analytics"."main"."rpt_activation_funnel__dbt_backup" cascade
    
[0m11:37:08.676429 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:37:08.678297 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.678464 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption__dbt_tmp" rename to "rpt_feature_adoption"
[0m11:37:08.678659 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:37:08.679738 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:37:08.679927 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:08.681711 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:37:08.681890 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.682052 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a29ff80>]}
[0m11:37:08.682214 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:37:08.682512 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.rpt_activation_funnel ................... [[32mOK[0m in 0.08s]
[0m11:37:08.682833 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:37:08.686055 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:37:08.687449 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:08.687627 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

      drop table if exists "analytics"."main"."rpt_feature_adoption__dbt_backup" cascade
    
[0m11:37:08.688137 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:37:08.688794 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:37:08.689058 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '099b54f7-fea1-4201-9a7d-baa7b213414d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d8ca40>]}
[0m11:37:08.689406 [info ] [Thread-2 (]: 2 of 3 OK created sql table model main.rpt_feature_adoption .................... [[32mOK[0m in 0.09s]
[0m11:37:08.689811 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:37:08.690578 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:08.690743 [debug] [MainThread]: On master: BEGIN
[0m11:37:08.690879 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:37:08.691246 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:08.691408 [debug] [MainThread]: On master: COMMIT
[0m11:37:08.691548 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:08.691675 [debug] [MainThread]: On master: COMMIT
[0m11:37:08.691942 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:08.692110 [debug] [MainThread]: On master: Close
[0m11:37:08.692295 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:37:08.692422 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:37:08.692536 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:37:08.692643 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:37:08.692808 [info ] [MainThread]: 
[0m11:37:08.692948 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m11:37:08.693360 [debug] [MainThread]: Command end result
[0m11:37:08.715774 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:37:08.716902 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:37:08.719818 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:37:08.719995 [info ] [MainThread]: 
[0m11:37:08.720170 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:37:08.720316 [info ] [MainThread]: 
[0m11:37:08.720491 [error] [MainThread]: [31mFailure in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)[0m
[0m11:37:08.720657 [error] [MainThread]:   Runtime Error in model rpt_user_engagement_score (models/marts/product/rpt_user_engagement_score.sql)
  Catalog Error: Table with name users does not exist!
  Did you mean "fct_user_activity_by_date"?
  
  LINE 161:     from users u 
                     ^
[0m11:37:08.720782 [info ] [MainThread]: 
[0m11:37:08.720931 [info ] [MainThread]:   compiled code at target/compiled/saas_analytics/models/marts/product/rpt_user_engagement_score.sql
[0m11:37:08.721058 [info ] [MainThread]: 
[0m11:37:08.721196 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m11:37:08.722781 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.89115644, "process_in_blocks": "0", "process_kernel_time": 0.214264, "process_mem_max_rss": "181600256", "process_out_blocks": "0", "process_user_time": 1.341649}
[0m11:37:08.723138 [debug] [MainThread]: Command `dbt run` failed at 11:37:08.723087 after 0.89 seconds
[0m11:37:08.723346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108796cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b54380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ba5b80>]}
[0m11:37:08.723565 [debug] [MainThread]: Flushing usage events
[0m11:37:08.853567 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:37:42.139363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108410980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1b5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1b5820>]}


============================== 11:37:42.160529 | 3eb53ade-cb73-4a92-86af-5efef1c738ac ==============================
[0m11:37:42.160529 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:37:42.160879 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'cache_selected_only': 'False', 'target_path': 'None', 'invocation_command': 'dbt run --select path:models/marts/product', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'introspect': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'log_format': 'default', 'printer_width': '80', 'use_experimental_parser': 'False', 'write_json': 'True', 'quiet': 'False', 'static_parser': 'True', 'empty': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'partial_parse': 'True', 'version_check': 'True', 'warn_error': 'None'}
[0m11:37:42.330133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2b53a0>]}
[0m11:37:42.358356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b16b860>]}
[0m11:37:42.359660 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:37:42.410954 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:37:42.489999 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:37:42.490562 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_user_engagement_score.sql
[0m11:37:42.717179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be71d30>]}
[0m11:37:42.765689 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:37:42.767079 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:37:42.781283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be35070>]}
[0m11:37:42.781556 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:37:42.781731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbef7d0>]}
[0m11:37:42.783924 [info ] [MainThread]: 
[0m11:37:42.784109 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:37:42.784248 [info ] [MainThread]: 
[0m11:37:42.784480 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:37:42.786638 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:37:42.850424 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:37:42.850671 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:37:42.850822 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:42.868957 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:37:42.869909 [debug] [ThreadPool]: On list_analytics: Close
[0m11:37:42.870264 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:37:42.870512 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:37:42.873751 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:42.873926 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:37:42.874070 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:42.874724 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:37:42.875396 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:42.875548 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:37:42.875784 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:42.875912 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:42.876042 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:37:42.876288 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:42.876671 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:37:42.876804 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:37:42.876922 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:37:42.877116 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:42.877251 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:37:42.878230 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:37:42.880828 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:37:42.880984 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:37:42.881165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:42.881418 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:37:42.881549 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:37:42.881682 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:37:42.891535 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:37:42.892487 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:37:42.893461 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:37:42.893626 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:37:42.894932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbee870>]}
[0m11:37:42.895176 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:42.895319 [debug] [MainThread]: On master: BEGIN
[0m11:37:42.895446 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:37:42.895698 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:42.895834 [debug] [MainThread]: On master: COMMIT
[0m11:37:42.895958 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:42.896076 [debug] [MainThread]: On master: COMMIT
[0m11:37:42.896265 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:42.896390 [debug] [MainThread]: On master: Close
[0m11:37:42.897677 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_activation_funnel
[0m11:37:42.897858 [debug] [Thread-2 (]: Began running node model.saas_analytics.rpt_feature_adoption
[0m11:37:42.898233 [debug] [Thread-3 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:37:42.898086 [info ] [Thread-1 (]: 1 of 3 START sql table model main.rpt_activation_funnel ........................ [RUN]
[0m11:37:42.898512 [info ] [Thread-2 (]: 2 of 3 START sql table model main.rpt_feature_adoption ......................... [RUN]
[0m11:37:42.898736 [info ] [Thread-3 (]: 3 of 3 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:37:42.898935 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_activation_funnel)
[0m11:37:42.899150 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_feature_adoption'
[0m11:37:42.899427 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.saas_analytics.rpt_user_engagement_score'
[0m11:37:42.899593 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_activation_funnel
[0m11:37:42.899760 [debug] [Thread-2 (]: Began compiling node model.saas_analytics.rpt_feature_adoption
[0m11:37:42.899918 [debug] [Thread-3 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:37:42.903860 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.905419 [debug] [Thread-2 (]: Writing injected SQL for node "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.907651 [debug] [Thread-3 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.908360 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_activation_funnel
[0m11:37:42.908588 [debug] [Thread-3 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:37:42.914583 [debug] [Thread-2 (]: Began executing node model.saas_analytics.rpt_feature_adoption
[0m11:37:42.929030 [debug] [Thread-2 (]: Writing runtime sql for node "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.929456 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.929668 [debug] [Thread-3 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.930130 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.930296 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: BEGIN
[0m11:37:42.930450 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:37:42.930745 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.930943 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.931089 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: BEGIN
[0m11:37:42.931263 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:37:42.931402 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:37:42.931544 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:37:42.931689 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.931832 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:37:42.932141 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

  
    
    

    create  table
      "analytics"."main"."rpt_activation_funnel__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define activation milestones
user_milestones as (
    select 
        user_id,

        --step 1: signed up (everyone should have this)
        true as completed_signup,

        --step 2: completed onboarding
        max(case when event_name = 'onboarding_completed' then 1 else 0 end) = 1 as completed_onboarding,
        min(case when event_name = 'onboarding_completed' then event_at end) as onboarding_completed_at,

        --step 3: used feature A
        max(case when event_name = 'feature_a_used' then 1 else 0 end) = 1 as used_feature_a,
        min(case when event_name = 'feature_a_used' then event_at end) as feature_a_first_used_at,

        --step 4: used feature B
        max(case when event_name = 'feature_b_used' then 1 else 0 end) = 1 as used_feature_b,
        min(case when event_name = 'feature_b_used' then event_at end) as feature_b_first_used_at,

        --step 5: upgraded to paid
        max(case when event_name = 'upgrade' then 1 else 0 end) = 1 as upgraded,
        min(case when event_name = 'upgrade' then event_at end) as upgrade_at

    from events 
    group by user_id   
),

--combine with user data
user_activation as (
    select 
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.is_currently_subscribed,

        --milestone completion
        um.completed_signup,
        coalesce(um.completed_onboarding, false) as completed_onboarding,
        coalesce(um.used_feature_a, false) as used_feature_a,
        coalesce(um.used_feature_b, false) as used_feature_b,
        coalesce(um.upgraded, false) as upgraded,

        --milestone timestamps
        um.onboarding_completed_at,
        um.feature_a_first_used_at,
        um.feature_b_first_used_at,
        um.upgrade_at,

        --time to milestone
        datediff('day', u.signed_up_at, um.onboarding_completed_at) as days_to_onboarding,
        datediff('day', u.signed_up_at, um.feature_a_first_used_at) as days_to_feature_a,
        datediff('day', u.signed_up_at, um.feature_b_first_used_at) as days_to_feature_b,
        datediff('day', u.signed_up_at, um.upgrade_at) as days_to_upgrade,

        --activation level (how many steps completed?)
        case 
            when coalesce(um.upgraded, false) then 'Full Activated (Paid)'
            when coalesce(um.used_feature_a, false) or coalesce(um.used_feature_b, false) then 'Feature User'
            when coalesce(um.completed_onboarding, false) then 'Onboarded'
            else 'Signed Up Only'
        end as activation_level,

        --activation score (0-5 based on steps completed)
        (case when um.completed_signup then 1 else 0 end) +
        (case when coalesce(um.completed_onboarding, false) then 1 else 0 end) + 
        (case when coalesce(um.used_feature_a, false) then 1 else 0 end) +
        (case when coalesce(um.used_feature_b, false) then 1 else 0 end) +
        (case when coalesce(um.upgraded, false) then 1 else 0 end) as activation_score
    from users u 
    left join user_milestones um 
        on u.user_id = um.user_id  
)

select * from user_activation
    );
  
  
[0m11:37:42.932492 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:42.932926 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.933150 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

  
    
    

    create  table
      "analytics"."main"."rpt_feature_adoption__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--define key feature events
feature_events as (
    select 
        user_id,
        event_name as feature,
        min(event_at) as first_used_at,
        max(event_at) as last_used_at,
        count(*) as total_uses,
        count(distinct event_at) as days_used
    from events 
    where event_name in ('feature_a_used', 'feature_b_used', 'onboarding_completed')
    group by user_id, event_name 
),

--join to users to get cohort info
feature_adoption as (
    select
        u.user_id,
        u.signed_up_at,
        date_trunc('month', u.signed_up_at) as cohort_month,
        u.acquisition_channel,
        u.user_lifecycle_stage,
        u.is_currently_subscribed,
        fe.feature,
        fe.first_used_at,
        fe.last_used_at,
        fe.total_uses,
        fe.days_used,

        --time to adoption
        datediff('day', u.signed_up_at, fe.first_used_at) as days_to_first_use,
        datediff('hour', u.signed_up_at, fe.first_used_at) as hours_to_first_use,

        --adoption timeframe buckets
        case
            when datediff('day', u.signed_up_at, fe.first_used_at) = 0 then 'Day 0 - Signup Day'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 1 then 'Day 1'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 3 then 'Days 2-3'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 7 then 'Days 4-7'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 14 then 'Days 8-14'
            when datediff('day', u.signed_up_at, fe.first_used_at) <= 30 then 'Days 15-30'
            else '30+ Days'
        end as adoption_timeframe,

        --engagement level
        case
            when fe.days_used >= 10 then 'Power User'
            when fe.days_used >= 5 then 'Regular User'
            when fe.days_used >= 2 then 'Occassional User'
            else 'One-Time User'
        end as engagement_level,

        --recency
        datediff('day', fe.last_used_at, current_date) as days_since_last_use

    from users u 
    inner join feature_events fe 
        on u.user_id = fe.user_id 
)

select * from feature_adoption
    );
  
  
[0m11:37:42.933389 [debug] [Thread-3 (]: SQL status: OK in 0.002 seconds
[0m11:37:42.933810 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.934132 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct user_activity_date) as total_active_days,
        min(user_activity_date) as first_active_date,
        max(user_activity_date) as last_active_date,
        datediff('day', min(user_activity_date), max(user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when user_activity_date >= current_date - interval '7 days'
            then user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when user_activity_date >= current_date - interval '30 days'
            then user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity 
    group by user_id 
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= current_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    where e.event_at is not null 
    group by e.user_id 
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:37:42.960814 [debug] [Thread-1 (]: SQL status: OK in 0.028 seconds
[0m11:37:42.963983 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.964292 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_activation_funnel'
  
[0m11:37:42.964509 [debug] [Thread-3 (]: SQL status: OK in 0.030 seconds
[0m11:37:42.964677 [debug] [Thread-2 (]: SQL status: OK in 0.031 seconds
[0m11:37:42.967790 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.968364 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.968555 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:37:42.968747 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */
alter table "analytics"."main"."rpt_user_engagement_score__dbt_tmp" rename to "rpt_user_engagement_score"
[0m11:37:42.968954 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:37:42.969584 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.970026 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_activation_funnel'
  
[0m11:37:42.970259 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:37:42.970862 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.971034 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:37:42.971200 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_feature_adoption'
  
[0m11:37:42.977403 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: COMMIT
[0m11:37:42.977583 [debug] [Thread-1 (]: SQL status: OK in 0.007 seconds
[0m11:37:42.977839 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.979516 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.979692 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: COMMIT
[0m11:37:42.979852 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:37:42.980082 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */
alter table "analytics"."main"."rpt_activation_funnel" rename to "rpt_activation_funnel__dbt_backup"
[0m11:37:42.981795 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.982065 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption" rename to "rpt_feature_adoption__dbt_backup"
[0m11:37:42.982358 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:37:42.983687 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.983863 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */
alter table "analytics"."main"."rpt_activation_funnel__dbt_tmp" rename to "rpt_activation_funnel"
[0m11:37:42.984108 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:37:42.984279 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m11:37:42.985575 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.988794 [debug] [Thread-3 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:37:42.988992 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m11:37:42.989153 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */
alter table "analytics"."main"."rpt_feature_adoption__dbt_tmp" rename to "rpt_feature_adoption"
[0m11:37:42.989324 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

      drop table if exists "analytics"."main"."rpt_user_engagement_score__dbt_backup" cascade
    
[0m11:37:42.991241 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: COMMIT
[0m11:37:42.991771 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:42.992006 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:42.992180 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:37:42.992354 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: COMMIT
[0m11:37:42.993304 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:37:42.996787 [debug] [Thread-3 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:37:42.997053 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:42.997274 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: COMMIT
[0m11:37:42.998410 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4d2c00>]}
[0m11:37:42.998767 [info ] [Thread-3 (]: 3 of 3 OK created sql table model main.rpt_user_engagement_score ............... [[32mOK[0m in 0.10s]
[0m11:37:42.999089 [debug] [Thread-3 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:37:42.999275 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:37:43.000693 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_activation_funnel"
[0m11:37:43.000883 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_activation_funnel"} */

      drop table if exists "analytics"."main"."rpt_activation_funnel__dbt_backup" cascade
    
[0m11:37:43.001509 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:37:43.002628 [debug] [Thread-2 (]: Using duckdb connection "model.saas_analytics.rpt_feature_adoption"
[0m11:37:43.002794 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_feature_adoption"} */

      drop table if exists "analytics"."main"."rpt_feature_adoption__dbt_backup" cascade
    
[0m11:37:43.002945 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:37:43.003641 [debug] [Thread-1 (]: On model.saas_analytics.rpt_activation_funnel: Close
[0m11:37:43.003924 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c42db50>]}
[0m11:37:43.004100 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:37:43.005002 [debug] [Thread-2 (]: On model.saas_analytics.rpt_feature_adoption: Close
[0m11:37:43.004381 [info ] [Thread-1 (]: 1 of 3 OK created sql table model main.rpt_activation_funnel ................... [[32mOK[0m in 0.10s]
[0m11:37:43.005325 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_activation_funnel
[0m11:37:43.005629 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3eb53ade-cb73-4a92-86af-5efef1c738ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4acb30>]}
[0m11:37:43.005966 [info ] [Thread-2 (]: 2 of 3 OK created sql table model main.rpt_feature_adoption .................... [[32mOK[0m in 0.11s]
[0m11:37:43.006239 [debug] [Thread-2 (]: Finished running node model.saas_analytics.rpt_feature_adoption
[0m11:37:43.006879 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:43.007098 [debug] [MainThread]: On master: BEGIN
[0m11:37:43.007228 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:37:43.007608 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:43.007761 [debug] [MainThread]: On master: COMMIT
[0m11:37:43.007894 [debug] [MainThread]: Using duckdb connection "master"
[0m11:37:43.008015 [debug] [MainThread]: On master: COMMIT
[0m11:37:43.008216 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:43.008350 [debug] [MainThread]: On master: Close
[0m11:37:43.008525 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:37:43.008640 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_activation_funnel' was properly closed.
[0m11:37:43.008750 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_feature_adoption' was properly closed.
[0m11:37:43.008860 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:37:43.009027 [info ] [MainThread]: 
[0m11:37:43.009177 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m11:37:43.009577 [debug] [MainThread]: Command end result
[0m11:37:43.040399 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:37:43.041605 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:37:43.044631 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:37:43.044844 [info ] [MainThread]: 
[0m11:37:43.045037 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:37:43.045179 [info ] [MainThread]: 
[0m11:37:43.045336 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m11:37:43.046773 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.9462629, "process_in_blocks": "0", "process_kernel_time": 0.214128, "process_mem_max_rss": "198197248", "process_out_blocks": "0", "process_user_time": 1.385641}
[0m11:37:43.047033 [debug] [MainThread]: Command `dbt run` succeeded at 11:37:43.046990 after 0.95 seconds
[0m11:37:43.047227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1b5850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b1b58b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10707f1d0>]}
[0m11:37:43.047542 [debug] [MainThread]: Flushing usage events
[0m11:37:43.158892 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:38:05.151306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10630fbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a21c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a219d0>]}


============================== 11:38:05.155792 | 06cfbc55-0a92-4d76-bb20-ef5edd10183a ==============================
[0m11:38:05.155792 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:38:05.156125 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'printer_width': '80', 'cache_selected_only': 'False', 'fail_fast': 'False', 'indirect_selection': 'eager', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'invocation_command': 'dbt test --select path:models/marts/product', 'debug': 'False', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs', 'write_json': 'True', 'no_print': 'None', 'partial_parse': 'True', 'quiet': 'False', 'warn_error': 'None', 'log_format': 'default'}
[0m11:38:05.334233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b24a40>]}
[0m11:38:05.363034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ee08f0>]}
[0m11:38:05.364370 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:38:05.425389 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:38:05.506225 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:38:05.506487 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:38:05.506631 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:38:05.530221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cc8a70>]}
[0m11:38:05.579779 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:38:05.581317 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:38:05.635909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fbbe60>]}
[0m11:38:05.636233 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:38:05.636425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f334d0>]}
[0m11:38:05.638813 [info ] [MainThread]: 
[0m11:38:05.639002 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:38:05.639149 [info ] [MainThread]: 
[0m11:38:05.639392 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:38:05.642194 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics_main'
[0m11:38:05.672665 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:38:05.672900 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:38:05.673043 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:05.685107 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m11:38:05.685356 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:38:05.685515 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:38:05.698541 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m11:38:05.699735 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:38:05.700694 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:38:05.700852 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:38:05.702398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '06cfbc55-0a92-4d76-bb20-ef5edd10183a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070863f0>]}
[0m11:38:05.702642 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:05.702776 [debug] [MainThread]: On master: BEGIN
[0m11:38:05.702900 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:38:05.703214 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:05.703347 [debug] [MainThread]: On master: COMMIT
[0m11:38:05.703473 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:05.703592 [debug] [MainThread]: On master: COMMIT
[0m11:38:05.703800 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:05.703929 [debug] [MainThread]: On master: Close
[0m11:38:05.705449 [debug] [Thread-1 (]: Began running node test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62
[0m11:38:05.705651 [debug] [Thread-2 (]: Began running node test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858
[0m11:38:05.705972 [debug] [Thread-3 (]: Began running node test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b
[0m11:38:05.705834 [info ] [Thread-1 (]: 1 of 6 START test not_null_rpt_activation_funnel_user_id ....................... [RUN]
[0m11:38:05.706177 [debug] [Thread-4 (]: Began running node test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56
[0m11:38:05.706343 [info ] [Thread-2 (]: 2 of 6 START test not_null_rpt_feature_adoption_feature ........................ [RUN]
[0m11:38:05.706691 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62)
[0m11:38:05.706483 [info ] [Thread-3 (]: 3 of 6 START test not_null_rpt_feature_adoption_user_id ........................ [RUN]
[0m11:38:05.706899 [info ] [Thread-4 (]: 4 of 6 START test not_null_rpt_user_engagement_score_user_id ................... [RUN]
[0m11:38:05.707118 [debug] [Thread-2 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858'
[0m11:38:05.707282 [debug] [Thread-1 (]: Began compiling node test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62
[0m11:38:05.707472 [debug] [Thread-3 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b'
[0m11:38:05.707668 [debug] [Thread-4 (]: Acquiring new duckdb connection 'test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56'
[0m11:38:05.707818 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858
[0m11:38:05.716739 [debug] [Thread-1 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62"
[0m11:38:05.717003 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b
[0m11:38:05.717222 [debug] [Thread-4 (]: Began compiling node test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56
[0m11:38:05.719549 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858"
[0m11:38:05.721603 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b"
[0m11:38:05.723487 [debug] [Thread-4 (]: Writing injected SQL for node "test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56"
[0m11:38:05.723964 [debug] [Thread-1 (]: Began executing node test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62
[0m11:38:05.730128 [debug] [Thread-3 (]: Began executing node test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b
[0m11:38:05.732447 [debug] [Thread-1 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62"
[0m11:38:05.733648 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b"
[0m11:38:05.733832 [debug] [Thread-2 (]: Began executing node test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858
[0m11:38:05.734056 [debug] [Thread-4 (]: Began executing node test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56
[0m11:38:05.735288 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858"
[0m11:38:05.736447 [debug] [Thread-4 (]: Writing runtime sql for node "test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56"
[0m11:38:05.736858 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b"
[0m11:38:05.737053 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b: BEGIN
[0m11:38:05.737251 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62"
[0m11:38:05.737446 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858"
[0m11:38:05.737602 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:38:05.737782 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56"
[0m11:38:05.737935 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62: BEGIN
[0m11:38:05.738085 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858: BEGIN
[0m11:38:05.738359 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56: BEGIN
[0m11:38:05.738514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:38:05.738672 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.738810 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:38:05.738960 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:38:05.739169 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b"
[0m11:38:05.739392 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.739624 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."rpt_feature_adoption"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m11:38:05.739806 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.739975 [debug] [Thread-1 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62"
[0m11:38:05.740125 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.740343 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858"
[0m11:38:05.740509 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."rpt_activation_funnel"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m11:38:05.740694 [debug] [Thread-4 (]: Using duckdb connection "test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56"
[0m11:38:05.740870 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select feature
from "analytics"."main"."rpt_feature_adoption"
where feature is null



  
  
      
    ) dbt_internal_test
[0m11:38:05.741137 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "analytics"."main"."rpt_user_engagement_score"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m11:38:05.741551 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.741752 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:38:05.741914 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:38:05.742083 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.744108 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b: ROLLBACK
[0m11:38:05.744786 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858: ROLLBACK
[0m11:38:05.745417 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56: ROLLBACK
[0m11:38:05.746154 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62: ROLLBACK
[0m11:38:05.746631 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b'
[0m11:38:05.747050 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858'
[0m11:38:05.747452 [debug] [Thread-4 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56'
[0m11:38:05.747875 [debug] [Thread-1 (]: Failed to rollback 'test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62'
[0m11:38:05.748037 [debug] [Thread-3 (]: On test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b: Close
[0m11:38:05.748197 [debug] [Thread-2 (]: On test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858: Close
[0m11:38:05.748358 [debug] [Thread-4 (]: On test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56: Close
[0m11:38:05.748501 [debug] [Thread-1 (]: On test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62: Close
[0m11:38:05.748806 [info ] [Thread-3 (]: 3 of 6 PASS not_null_rpt_feature_adoption_user_id .............................. [[32mPASS[0m in 0.04s]
[0m11:38:05.749085 [info ] [Thread-2 (]: 2 of 6 PASS not_null_rpt_feature_adoption_feature .............................. [[32mPASS[0m in 0.04s]
[0m11:38:05.749354 [info ] [Thread-4 (]: 4 of 6 PASS not_null_rpt_user_engagement_score_user_id ......................... [[32mPASS[0m in 0.04s]
[0m11:38:05.749808 [debug] [Thread-3 (]: Finished running node test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b
[0m11:38:05.749595 [info ] [Thread-1 (]: 1 of 6 PASS not_null_rpt_activation_funnel_user_id ............................. [[32mPASS[0m in 0.04s]
[0m11:38:05.750066 [debug] [Thread-2 (]: Finished running node test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858
[0m11:38:05.750274 [debug] [Thread-4 (]: Finished running node test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56
[0m11:38:05.750447 [debug] [Thread-3 (]: Began running node test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd
[0m11:38:05.750657 [debug] [Thread-1 (]: Finished running node test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62
[0m11:38:05.750859 [debug] [Thread-2 (]: Began running node test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c
[0m11:38:05.751084 [info ] [Thread-3 (]: 5 of 6 START test unique_rpt_activation_funnel_user_id ......................... [RUN]
[0m11:38:05.751497 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_rpt_feature_adoption_user_id.68c65a6d0b, now test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd)
[0m11:38:05.751315 [info ] [Thread-2 (]: 6 of 6 START test unique_rpt_user_engagement_score_user_id ..................... [RUN]
[0m11:38:05.751717 [debug] [Thread-3 (]: Began compiling node test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd
[0m11:38:05.751900 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.saas_analytics.not_null_rpt_feature_adoption_feature.54230c4858, now test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c)
[0m11:38:05.755118 [debug] [Thread-3 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd"
[0m11:38:05.755352 [debug] [Thread-2 (]: Began compiling node test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c
[0m11:38:05.758775 [debug] [Thread-2 (]: Writing injected SQL for node "test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c"
[0m11:38:05.759335 [debug] [Thread-3 (]: Began executing node test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd
[0m11:38:05.760563 [debug] [Thread-3 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd"
[0m11:38:05.760809 [debug] [Thread-2 (]: Began executing node test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c
[0m11:38:05.761945 [debug] [Thread-2 (]: Writing runtime sql for node "test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c"
[0m11:38:05.762216 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd"
[0m11:38:05.762399 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd: BEGIN
[0m11:38:05.762547 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:38:05.762714 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c"
[0m11:38:05.762990 [debug] [Thread-2 (]: On test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c: BEGIN
[0m11:38:05.763149 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:38:05.763308 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.763513 [debug] [Thread-3 (]: Using duckdb connection "test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd"
[0m11:38:05.763677 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_activation_funnel"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:38:05.763843 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:05.764067 [debug] [Thread-2 (]: Using duckdb connection "test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c"
[0m11:38:05.764225 [debug] [Thread-2 (]: On test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    user_id as unique_field,
    count(*) as n_records

from "analytics"."main"."rpt_user_engagement_score"
where user_id is not null
group by user_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m11:38:05.768798 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m11:38:05.769057 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:38:05.769855 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd: ROLLBACK
[0m11:38:05.770514 [debug] [Thread-2 (]: On test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c: ROLLBACK
[0m11:38:05.770969 [debug] [Thread-3 (]: Failed to rollback 'test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd'
[0m11:38:05.771131 [debug] [Thread-3 (]: On test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd: Close
[0m11:38:05.771525 [debug] [Thread-2 (]: Failed to rollback 'test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c'
[0m11:38:05.771707 [debug] [Thread-2 (]: On test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c: Close
[0m11:38:05.771979 [info ] [Thread-3 (]: 5 of 6 PASS unique_rpt_activation_funnel_user_id ............................... [[32mPASS[0m in 0.02s]
[0m11:38:05.772264 [debug] [Thread-3 (]: Finished running node test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd
[0m11:38:05.772525 [info ] [Thread-2 (]: 6 of 6 PASS unique_rpt_user_engagement_score_user_id ........................... [[32mPASS[0m in 0.02s]
[0m11:38:05.772827 [debug] [Thread-2 (]: Finished running node test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c
[0m11:38:05.773446 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:05.773595 [debug] [MainThread]: On master: BEGIN
[0m11:38:05.773719 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:38:05.774037 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:05.774171 [debug] [MainThread]: On master: COMMIT
[0m11:38:05.774299 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:05.774415 [debug] [MainThread]: On master: COMMIT
[0m11:38:05.774612 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:05.774738 [debug] [MainThread]: On master: Close
[0m11:38:05.774899 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:38:05.775015 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_activation_funnel_user_id.ea19ffbd62' was properly closed.
[0m11:38:05.775123 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_user_engagement_score_user_id.12e8ea942c' was properly closed.
[0m11:38:05.775231 [debug] [MainThread]: Connection 'test.saas_analytics.unique_rpt_activation_funnel_user_id.b9607320dd' was properly closed.
[0m11:38:05.775336 [debug] [MainThread]: Connection 'test.saas_analytics.not_null_rpt_user_engagement_score_user_id.d306418e56' was properly closed.
[0m11:38:05.775469 [info ] [MainThread]: 
[0m11:38:05.775607 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m11:38:05.776148 [debug] [MainThread]: Command end result
[0m11:38:05.798755 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:38:05.799918 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:38:05.803355 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:38:05.803535 [info ] [MainThread]: 
[0m11:38:05.803714 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:38:05.803857 [info ] [MainThread]: 
[0m11:38:05.804003 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m11:38:05.805360 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.6903624, "process_in_blocks": "0", "process_kernel_time": 0.189453, "process_mem_max_rss": "155484160", "process_out_blocks": "0", "process_user_time": 1.151151}
[0m11:38:05.805567 [debug] [MainThread]: Command `dbt test` succeeded at 11:38:05.805528 after 0.69 seconds
[0m11:38:05.805735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a21c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107236270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107236240>]}
[0m11:38:05.805893 [debug] [MainThread]: Flushing usage events
[0m11:38:05.915667 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:55:15.407952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447c200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b59a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b59820>]}


============================== 11:55:15.439887 | 31eb091c-bb17-41f8-b776-4d24f82f7caf ==============================
[0m11:55:15.439887 [info ] [MainThread]: Running with dbt=1.11.4
[0m11:55:15.440307 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run --select rpt_user_engagement_score', 'quiet': 'False', 'write_json': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'empty': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/hazeldonaldson/.dbt', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'log_format': 'default', 'printer_width': '80', 'partial_parse': 'True', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'log_path': '/Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/logs'}
[0m11:55:15.623611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104633ec0>]}
[0m11:55:15.653222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043d3680>]}
[0m11:55:15.654602 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:55:15.706423 [debug] [MainThread]: checksum: 092b177ba87eb6bb3c55954c38aa57900eebac336561eb9ee32218428aa81d43, vars: {}, profile: , target: , version: 1.11.4
[0m11:55:15.787789 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:55:15.788304 [debug] [MainThread]: Partial parsing: updated file: saas_analytics://models/marts/product/rpt_user_engagement_score.sql
[0m11:55:16.021180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105414230>]}
[0m11:55:16.070974 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:55:16.073100 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:55:16.088875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105294620>]}
[0m11:55:16.089152 [info ] [MainThread]: Found 20 models, 65 data tests, 3 sources, 472 macros
[0m11:55:16.089333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053ada60>]}
[0m11:55:16.090469 [info ] [MainThread]: 
[0m11:55:16.090647 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:55:16.090785 [info ] [MainThread]: 
[0m11:55:16.091018 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:55:16.091435 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_analytics'
[0m11:55:16.120344 [debug] [ThreadPool]: Using duckdb connection "list_analytics"
[0m11:55:16.120586 [debug] [ThreadPool]: On list_analytics: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"analytics"'
    
  
  
[0m11:55:16.120738 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:55:16.141584 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m11:55:16.142539 [debug] [ThreadPool]: On list_analytics: Close
[0m11:55:16.142901 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_analytics, now create_analytics_main)
[0m11:55:16.143159 [debug] [ThreadPool]: Creating schema "database: "analytics"
schema: "main"
"
[0m11:55:16.146528 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:55:16.146707 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
        select type from duckdb_databases()
        where lower(database_name)='analytics'
        and type='sqlite'
    
  
[0m11:55:16.146849 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:55:16.147840 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:55:16.148903 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:55:16.149069 [debug] [ThreadPool]: On create_analytics_main: BEGIN
[0m11:55:16.149338 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:55:16.149468 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:55:16.149598 [debug] [ThreadPool]: On create_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "create_analytics_main"} */

    
    
        create schema if not exists "analytics"."main"
    
[0m11:55:16.150155 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:55:16.150802 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:55:16.150995 [debug] [ThreadPool]: Using duckdb connection "create_analytics_main"
[0m11:55:16.151146 [debug] [ThreadPool]: On create_analytics_main: COMMIT
[0m11:55:16.151422 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:55:16.151571 [debug] [ThreadPool]: On create_analytics_main: Close
[0m11:55:16.154568 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_analytics_main, now list_analytics_main)
[0m11:55:16.157461 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:55:16.157631 [debug] [ThreadPool]: On list_analytics_main: BEGIN
[0m11:55:16.157758 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:55:16.158055 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:55:16.158185 [debug] [ThreadPool]: Using duckdb connection "list_analytics_main"
[0m11:55:16.158316 [debug] [ThreadPool]: On list_analytics_main: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "connection_name": "list_analytics_main"} */
select
      'analytics' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main'
    and lower(table_catalog) = 'analytics'
  
[0m11:55:16.170031 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m11:55:16.171054 [debug] [ThreadPool]: On list_analytics_main: ROLLBACK
[0m11:55:16.171971 [debug] [ThreadPool]: Failed to rollback 'list_analytics_main'
[0m11:55:16.172125 [debug] [ThreadPool]: On list_analytics_main: Close
[0m11:55:16.173680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057bd040>]}
[0m11:55:16.173911 [debug] [MainThread]: Using duckdb connection "master"
[0m11:55:16.174047 [debug] [MainThread]: On master: BEGIN
[0m11:55:16.174174 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:55:16.174426 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:55:16.174560 [debug] [MainThread]: On master: COMMIT
[0m11:55:16.174688 [debug] [MainThread]: Using duckdb connection "master"
[0m11:55:16.174804 [debug] [MainThread]: On master: COMMIT
[0m11:55:16.175011 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:55:16.175136 [debug] [MainThread]: On master: Close
[0m11:55:16.176573 [debug] [Thread-1 (]: Began running node model.saas_analytics.rpt_user_engagement_score
[0m11:55:16.176839 [info ] [Thread-1 (]: 1 of 1 START sql table model main.rpt_user_engagement_score .................... [RUN]
[0m11:55:16.177058 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_analytics_main, now model.saas_analytics.rpt_user_engagement_score)
[0m11:55:16.177229 [debug] [Thread-1 (]: Began compiling node model.saas_analytics.rpt_user_engagement_score
[0m11:55:16.181617 [debug] [Thread-1 (]: Writing injected SQL for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.182605 [debug] [Thread-1 (]: Began executing node model.saas_analytics.rpt_user_engagement_score
[0m11:55:16.198250 [debug] [Thread-1 (]: Writing runtime sql for node "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.199229 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.199403 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: BEGIN
[0m11:55:16.199551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:16.199917 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:55:16.200072 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.200364 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

  
    
    

    create  table
      "analytics"."main"."rpt_user_engagement_score__dbt_tmp"
  
    as (
      

with users as (
    select * from "analytics"."main"."dim_users"
),

activity as (
    select * from "analytics"."main"."fct_user_activity_by_date"
),

events as (
    select * from "analytics"."main"."fct_events"
),

--get the latest date in the data to use as "today"
data_date_range as (
    select
        max(user_activity_date) as latest_date
    from activity
),

--calculate activity metrics per user
user_activity_metrics as (
    select 
        user_id,

        --overall activity
        count(distinct a.user_activity_date) as total_active_days,
        min(a.user_activity_date) as first_active_date,
        max(a.user_activity_date) as last_active_date,
        datediff('day', min(a.user_activity_date), max(a.user_activity_date)) + 1 as activity_span_days,

        --recent activity (last 30 days)
        count(distinct case
            when a.user_activity_date >= d.latest_date - interval '30 days'
            then a.user_activity_date
        end) as active_days_l30d,

        count(distinct case
            when a.user_activity_date >= d.latest_date - interval '7 days'
            then a.user_activity_date
        end) as active_days_l7d,

        --weekly activity rate
        count(distinct case
            when a.user_activity_date >= d.latest_date - interval '30 days'
            then a.user_activity_date
            end)::float / 30 * 7 as avg_days_per_week_l30d
    from activity a
    cross join data_date_range d 
    group by a.user_id, d.latest_date
),

--calculate event diversity (breadth of engagement)
user_event_diversity as (
    select 
        e.user_id,
        count(distinct e.event_name) as unique_events_all_time,
        count(*) as total_events_all_time,

        --last 30 days
        count(distinct case
            when e.event_at >= d.latest_date - interval '30 days'
            then e.event_name
        end) as unique_events_l30d,

        count(distinct case
            when e.event_at >= d.latest_date - interval '30 days'
            then 1
        end) as total_events_l30d
    from events e 
    cross join data_date_range d 
    where e.event_at is not null 
    group by e.user_id, d.latest_date
),

--combine metrics and calculate engagement score
user_engagement as (
    select 
        u.user_id,
        u.signed_up_at,
        u.acquisition_channel,
        u.is_currently_subscribed,
        u.user_lifecycle_stage,

        --activity metrics
        coalesce(am.total_active_days, 0) as total_active_days,
        coalesce(am.active_days_l30d, 0) as active_days_l30d,
        coalesce(am.active_days_l7d, 0) as active_days_l7d,
        coalesce(am.avg_days_per_week_l30d, 0) as avg_days_per_week_l30d,
        am.first_active_date,
        am.last_active_date,
        am.activity_span_days,

        --event metrics
        coalesce(ed.unique_events_all_time, 0) as unique_events_all_time,
        coalesce(ed.total_events_all_time, 0) as total_events_all_time,
        coalesce(ed.unique_events_l30d, 0) as unique_events_l30d,
        coalesce(ed.total_events_l30d, 0) as total_events_l30d,

        --DAU/WAU/MAU classification
        case 
            when coalesce(am.active_days_l7d, 0) >= 1 then 'WAU'
            when coalesce(am.active_days_l30d, 0) >= 1 then 'MAU'
            else 'Dormant'
        end as activity_status,

        --engagement score (0-100)
        --formula: (recency * 30) + (frequency * 40) + (breadth * 30)
        least(100, (
            --recency: active in the last 7 days = 30 pts, last 30 days = 15 pts
            case 
                when coalesce(am.active_days_l7d, 0) >= 1 then 30
                when coalesce(am.active_days_l30d, 0) >= 1 then 15
                else 0
            end +
            --frequency: days active in the last 30 days (max 40 pts)
            least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
            --breadth: unique events in the last 30 days (max 30 pts)
            least(30, coalesce(ed.unique_events_l30d, 0) * 6)
        )) as engagement_score,

        --engagement tier
        case 
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15 
                    else 0
                end + 
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 70 then 'High Engagement'
            when least(100, (
                case
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) >= 40 then 'Medium Engagement'
            when least(100, (
                case 
                    when coalesce(am.active_days_l7d, 0) >= 1 then 30
                    when coalesce(am.active_days_l30d, 0) >= 1 then 15
                    else 0
                end +
                least(40, coalesce(am.active_days_l30d, 0) * 1.33) +
                least(30, coalesce(ed.unique_events_l30d, 0) * 6)
            )) > 0 then 'Low Engagement'
            else 'Dormant'
        end as engagement_tier,

        --recency
        case
            when am.last_active_date is null then null 
            else datediff('day', am.last_active_date, current_date)
        end as days_since_last_active
    from users u 
    left join user_activity_metrics am 
        on u.user_id = am.user_id 
    left join user_event_diversity ed 
        on u.user_id = ed.user_id 
)

select * from user_engagement
    );
  
  
[0m11:55:16.250972 [debug] [Thread-1 (]: SQL status: OK in 0.050 seconds
[0m11:55:16.253913 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.254131 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_user_engagement_score'
  
[0m11:55:16.254660 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:55:16.255275 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.255442 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main'
      AND table_name = 'rpt_user_engagement_score'
  
[0m11:55:16.255870 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:55:16.259050 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.259249 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */
alter table "analytics"."main"."rpt_user_engagement_score" rename to "rpt_user_engagement_score__dbt_backup"
[0m11:55:16.260156 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:55:16.262394 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.262585 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */
alter table "analytics"."main"."rpt_user_engagement_score__dbt_tmp" rename to "rpt_user_engagement_score"
[0m11:55:16.262898 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:55:16.270168 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: COMMIT
[0m11:55:16.270384 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.270551 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: COMMIT
[0m11:55:16.273725 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:55:16.276476 [debug] [Thread-1 (]: Using duckdb connection "model.saas_analytics.rpt_user_engagement_score"
[0m11:55:16.276680 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: /* {"app": "dbt", "dbt_version": "1.11.4", "profile_name": "saas_analytics", "target_name": "dev", "node_id": "model.saas_analytics.rpt_user_engagement_score"} */

      drop table if exists "analytics"."main"."rpt_user_engagement_score__dbt_backup" cascade
    
[0m11:55:16.277460 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:55:16.278620 [debug] [Thread-1 (]: On model.saas_analytics.rpt_user_engagement_score: Close
[0m11:55:16.279579 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eb091c-bb17-41f8-b776-4d24f82f7caf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103dd33b0>]}
[0m11:55:16.279884 [info ] [Thread-1 (]: 1 of 1 OK created sql table model main.rpt_user_engagement_score ............... [[32mOK[0m in 0.10s]
[0m11:55:16.280152 [debug] [Thread-1 (]: Finished running node model.saas_analytics.rpt_user_engagement_score
[0m11:55:16.280736 [debug] [MainThread]: Using duckdb connection "master"
[0m11:55:16.280882 [debug] [MainThread]: On master: BEGIN
[0m11:55:16.281003 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:55:16.281283 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:55:16.281437 [debug] [MainThread]: On master: COMMIT
[0m11:55:16.281569 [debug] [MainThread]: Using duckdb connection "master"
[0m11:55:16.281684 [debug] [MainThread]: On master: COMMIT
[0m11:55:16.281874 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:55:16.281996 [debug] [MainThread]: On master: Close
[0m11:55:16.282162 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:55:16.282280 [debug] [MainThread]: Connection 'model.saas_analytics.rpt_user_engagement_score' was properly closed.
[0m11:55:16.282404 [info ] [MainThread]: 
[0m11:55:16.282543 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m11:55:16.282829 [debug] [MainThread]: Command end result
[0m11:55:16.307317 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/manifest.json
[0m11:55:16.308474 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/semantic_manifest.json
[0m11:55:16.311659 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/hazeldonaldson/Documents/saas-analytics-platform/dbt/target/run_results.json
[0m11:55:16.311837 [info ] [MainThread]: 
[0m11:55:16.312032 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:55:16.312171 [info ] [MainThread]: 
[0m11:55:16.312322 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:55:16.313715 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.94575536, "process_in_blocks": "0", "process_kernel_time": 0.238083, "process_mem_max_rss": "221364224", "process_out_blocks": "0", "process_user_time": 1.360974}
[0m11:55:16.313939 [debug] [MainThread]: Command `dbt run` succeeded at 11:55:16.313898 after 0.95 seconds
[0m11:55:16.314108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b598b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b59a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e016d0>]}
[0m11:55:16.314273 [debug] [MainThread]: Flushing usage events
[0m11:55:16.435402 [debug] [MainThread]: An error was encountered while trying to flush usage events
